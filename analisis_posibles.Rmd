# ¿QUÉ ANÁLISIS PODEMOS HACER DE LOS RESULTADOS? {#COMP5}

<center>![](images/comp5.png){width=60%}</center>


Una vez que hemos terminado de aplicar el examen y capturado la información (ya sea en el pilotaje o en la aplicación final), es importante que analicemos si los reactivos que redactamos realmente se comportaron como esperábamos que lo hicieran y si los resultados generales verdaderamente pueden interpretarse y utilizarse de las maneras que teníamos previstas. Para ello, hay seis grandes áreas de análisis que nos permitirán determinarlo:

:::nota
**Nota:**
Mucha de la información aquí presentada se basa en las recomendacioens dentro del libro de Chadha (2009) y del de Lane et al. (2016); si al lector le interesa profundizar en alguno de estos puntos, recomendamos estas dos fuentes como principales puntos de referencia
:::

* [**Desempeño de reactivos:**](#desmpeño) Nos dirá si realmente los reactivos funcionaron en cuanto a dificultad y discriminación, y si los distractores funcionaron como se esperaba que lo hicieran.

* [**Dimensionalidad:**](#dimensionalidad) Nos mostrará si todos los reactivos de un dominio o subdominio específico realmente están midiendo lo mismo, o si, por el contrario, algún reactivo que esperábamos midiera un dominio, en realidad se agrupa mejor con reactivos que miden otra cosa. Esta es una importante [evidencia de validez](#evidencia-validez). 

* [**Confiabilidad:**](#confiabilidad) Nos dirá si el examen es consistente a través del tiempo o de las versiones diferentes del mismo.

* **Imparcialidad** Nos permitirá determinar si nuestros reactivos benefician a algún grupo en específico, independientemente de su nivel de habilidad.

* [**Evidencias de validez:**](#evidencia-validez) En esta sección se agrupan varios análisis que nos permitirán identificar si el examen realmente mide lo que pretende medir y si en verdad se puede interpretar de la manera que pretendemos hacerlo. 

* **Calificación:** Nos ayudará a determinar cómo establecer puntos de corte para decidir quién aprueba y quién no, dependiendo de los usos e interpretaciones previstos

:::aera
**Estándar 5.0:** Los puntajes de la prueba deben derivarse de una manera que respalde las interpretaciones de los puntajes de la prueba para los usos propuestos de las pruebas. Los desarrolladores y usuarios de la prueba deben documentar evidencia de imparcialidad, confiabilidad y validez de los puntajes de la prueba para su uso propuesto.
:::


## Desempeño de los reactivos

### Teoría Clásica de los Tests

Para realizar el análisis psicométrico de los reactivos, desde la [Teoría Clásica de los Tests (TCT)](#TCT-G), son tres los puntos principales que necesitamos evaluar: la dificultad, la discriminación y la [correlación](#correlacion) con el total del instrumento.

#### Índice de dificultad

Cuando hablamos de dificultad del reactivo en la TCT, nos referimos a la proporción de alumnos que contestó correctamente el reactivo con respecto al total de examinados. Supongamos que, en un examen de matemáticas, incluimos dos reactivos de cálculo integral; uno de ellos, es contestado correctamente por el 80% de los estudiantes que tomaron el examen, mientras que el otro, solamente fue contestado correctamente por el 35% de los participantes. Con solo esta información, es fácil asumir que el primer reactivo es mucho más sencillo que el segundo, dado que una gran proporción de los examinados lo contestó correctamente. Puede parecer un poco contraintuitivo, pero al ser la proporción de respuestas correctas, un número alto en el índice de dificultad, nos indica que el reactivo es sencillo, mientras que un número pequeño nos indica que el reactivo es difícil. El índice de dificultad, en lugar de presentarse en porcentaje como el ejemplo, suele presentarse como un valor decimal, dado que se calcula dividiendo el número de personas que contestaron correctamente el reactivo entre el número total de participantes. En el primer reactivo del ejemplo, su índice de dificultad sería de .8, y el segundo, de .35. 

Idealmente, lo que buscamos en nuestros reactivos es que no sean tan sencillos como para que todos los sustentantes los contesten correctamente, pero que, a la vez, no sean tan complejos que ninguno pueda hacerlo. Aunque bajo esa lógica parecería recomendable buscar que todos los ítems tengan un índice de dificultad de .5, la realidad es que lo adecuado es balancear el examen para tener reactivos relativamente fáciles y reactivos relativamente difíciles. Algunos autores suelen utilizar como parámetros un umbral de .2 a .8; es decir, buscan mantener únicamente los reactivos cuyo índice de dificultad sea mayor a .2, pero que sea menor a .8. Sin embargo, no podemos tomar este criterio como una regla de decisión que aplica en todos los casos. Si tenemos un reactivo que salga del límite o se encuentre muy cerca de este, es importante que tomemos en consideración el resto de los indicadores aquí descritos para decidir si lo eliminamos o no.

#### Proporción de elección de opciones de respuesta 

Una vez que ya revisamos la proporción de sustentantes que contestó correctamente el reactivo, también es importante que desglosemos las proporciones correspondientes a quienes lo contestaron de manera incorrecta. Dado que los reactivos suelen tener cuatro opciones de respuesta, siendo una correcta y las otras tres distractoras, es importante identificar qué proporción de sustentantes contestó cada uno de los distractores. Idealmente, esperaríamos que, por ejemplo, si el 70% de los sustentantes contestó correctamente el reactivo, el 30% restante se distribuya en aproximadamente 10% por cada distractor.

Aunque esta proporción equitativa no siempre se logra, sí es importante que revisemos este dato, ya que, por ejemplo, si observamos que ese 30% de respuestas incorrectas, se reparte en 15% para un distractor, 15% para otro y 0% para el último, esto nos está indicando que el último distractor no es seleccionado por nadie, y por tanto no cumple su función como distractor, quizá porque es muy obvio para los sustentantes que esa no es la respuesta correcta, o quizá por alguna cuestión de formato o redacción. Esto se vuelve más importante porque quizá este distractor que no funciona puede estar afectando el índice de dificultad, y corregirlo, podría hacer que el reactivo se desempeñe mejor. Es por ello que es importante detectar este tipo de inconsistencias para poder decidir si, en lugar de eliminar el reactivo, lo enviamos a revisión para quizá cambiar esa opción de respuesta por una más distractora. 

#### Índice de discriminación

En el contexto de análisis de reactivos, hablamos de discriminación para referirnos a la capacidad que tiene el reactivo para distinguir correctamente el grupo de sustentantes con desempeño alto (sustentantes con el mayor puntaje total en el examen) con respecto a los sustentantes de bajo desempeño (sustentantes con el puntaje global en el examen más bajo). En otras palabras, un reactivo discrimina bien, cuando son más los sustentantes con alto desempeño los que lo contestan bien, y más los que tienen bajo desempeño quienes lo contestan de manera incorrecta. Si, por el contrario, tenemos un reactivo que solo el 40% de los sustentantes contestó correctamente, pero de ese 40%, la mayoría fueron sustentantes con un desempeño bajo, el reactivo no está funcionando de manera adecuada. También se puede dar el caso donde lo respondan de manera correcta sustentantes de ambos grupos por igual, por lo que el reactivo realmente no aporta información importante. 

El índice de discriminación se obtiene al restar el número de sustentantes del grupo bajo que respondieron correctamente, al número de sustentantes del grupo alto que respondieron correctamente, lo cual luego se divide entre el número total de participantes. Si obtenemos un valor negativo de esta operación, significa que fue mayor la proporción de sustentantes en el grupo bajo que contestó correctamente el reactivo. Si obtenemos 0 o valores muy cercanos a 0, significa que la proporción de ambos grupos es prácticamente la misma. Con esto en cuenta, el criterio que se suele utilizar es conservar reactivos con un índice de discriminación mayor a .2, o en casos más estrictos, mayores a .3.  Nuevamente, no podemos tomar solo este índice como criterio para eliminar el reactivo, sino que se debe considerar en conjunto con los demás.

#### Correlación entre el reactivo y el total de la prueba

Otro punto importante es qué tanto el reactivo contribuye al puntaje total de la prueba. Para determinar esto, utilizamos un método estadístico llamado [correlación biserial puntal](#reactivo-total). En esencia, una correlación nos dice qué tanto dos valores están relacionados y de qué manera; en este caso, el valor de la correlación nos dirá si el contestar de manera correcta un reactivo está relacionado con obtener puntajes altos y si contestarlo de manera incorrecta se relaciona con la obtención de puntajes bajos. En otras palabras, nos dirá si contestar de manera correcta o incorrecta cierto reactivo nos ayuda a determinar el nivel de habilidad de la persona (entendido como el puntaje total de la prueba). 

La correlación se obtiene de comparar el valor (correcto o incorrecto) que obtuvo cada sustentante en determinado reactivo con su puntaje total en la prueba. El valor que se obtiene estará dentro de un rango de -1 a 1. En este rango, valores cercanos a -1 indican una correlación negativa, es decir, contestar de manera correcta se relaciona más con obtener puntajes bajos, mientras que valores más cercanos a 1 indican una correlación positiva, siendo que contestar correctamente se relaciona con obtener puntajes altos. Por otro lado, valores cercanos a 0, ya sean positivos o negativos, nos indican que no hay una relación entre contestar de manera correcta o incorrecta el reactivo y el puntaje total de la prueba. 

Lo que esperamos con los reactivos que diseñamos es que todos los reactivos contribuyan a determinar el nivel de habilidad de los sustentantes, por ende, lo que esperamos son correlaciones de carácter positivo. No necesitamos correlaciones que se encuentren muy cercanas a 1, de hecho eso es poco probable. Sin embargo, solemos usar como criterio el conservar reactivos con correlaciones mayores a .2. Nuevamente, este dato solo se puede usar como criterio cuando se toma en cuenta con el resto de información obtenida con respecto al reactivo.

#### Correlación de los distractores.

La correlación que hicimos hace un momento se hace al tomar el valor de contestar correctamente como 1, y contestar incorrectamente como 0. Bajo esa premisa, podemos hacer la misma correlación para cada una de las opciones de respuesta, es decir, si un sustentante eligió uno de los distractores, lo tomamos como un 1, y si no lo eligió, como un 0. Esto nos permite realizar la correlación con el puntaje total también con los distractores. La ventaja que tiene este análisis es que nos permite detectar si los distractores realmente están funcionando como esperamos que funcionen. Lo que esperaríamos es obtener correlaciones negativas en todos los distractores; es decir, contestar un reactivo con uno de los distractores se relaciona con obtener puntajes más bajos en la prueba. En este caso no hay un criterio establecido para qué tan grande deba ser el nivel de la correlación. Sin embargo, en caso de obtener correlaciones positivas en alguno de los distractores, nos estaría diciendo que elegir ese distractor se relaciona con obtener puntajes más altos, por lo que podría ser que ese distractor en realidad sea una respuesta correcta o parcialmente correcta. 

### Teoría de Respuesta al Ítem

Otra metodología que también se suele utilizar para analizar las propiedades de los reactivos es la [Teoría de Respuesta al Ítem](#TRI_G). Este modelo teórico, a diferencia del anterior, se centra en la relación entre el nivel de habilidad de los sustentantes y su probabilidad de responder de manera correcta o incorrecta cierto reactivo. Esta teoría parte del supuesto de que la probabilidad de responder de manera correcta un reactivo está dada en función del nivel de habilidad de los sustentantes y por 1 o más parámetros referentes al reactivo de manera específica. 

Tomando en cuenta estos parámetros, se obtiene la probabilidad de que un sustentante conteste de manera correcta un reactivo en función de su nivel de habilidad. Pero de manera más interesante para este contexto, nos da específicamente los valores de estos parámetros, que si observamos con detenimiento podremos identificar que los primeros dos se refieren al mismo concepto que vimos en la Teoría Clásica de los Test. 

#### Modelo de un parámetro

El [modelo de un parámetro](#1PL) únicamente toma en cuenta el nivel de habilidad de los sustentantes y la dificultad del reactivo, y asume que la discriminación de todos los reactivos es igual. El valor del índice de dificultad, en teoría puede ir de -infinito a +infinito. El índice de dificultad puede variar de valores negativos cuando es fácil o valores positivos cuando es difícil. Un buen criterio de decisión es conservar los reactivos cuyo valor en el parámetro de dificultad sea de -2.5 a 2.5.

#### Modelo de dos parámetros

El [modelo de dos parámetros](#2PL), además de considerar el nivel de habilidad y la dificultad de los reactivos, incluye la variación en el nivel de discriminación de los reactivos. El parámetro de discriminación puede tomar valores comenzando en 0 y hasta un máximo de 2. Suele considerarse que reactivos con valores iguales o mayores a 0.45 son reactivos con un buen nivel de discriminación. Valores más pequeños indicarían que el reactivo no discrimina bien entre los grupos con mayor o menor nivel de habilidad.

### Eliminación de reactivos

Ya que hemos obtenido toda la información anterior, es importante que la analicemos reactivo por reactivo para que podamos decidir qué reactivos conservaremos para la versión final y para la calificación. La decisión de eliminar o no un reactivo dependerá de múltiples factores. En primer lugar, es importante ver qué tanto cumple con los criterios esperados, si en un principio el reactivo tiene muy malos indicadores (por ejemplo, índice de dificultad menor a .1, índice de discriminación menor a .2 o correlación biserial negativa), entonces lo mejor será eliminarlo directamente, ya que un reactivo con un desempeño muy pobre es difícilmente rescatable. Sin embargo, si se excede en alguno de los indicadores o en varios por poco, es importante considerar el funcionamiento de los distractores, para ver si es factible enviar el reactivo a revisión para que sea corregido para futuras aplicaciones. 

Otro aspecto que es importante tomar en cuenta es la cantidad de reactivos que se tiene para cada indicador. Si fuéramos muy estrictos y elimináramos cada reactivo que no cumple con alguno de los criterios, probablemente terminaríamos con un examen muy pequeño, y el riesgo de eso es que terminemos por eliminar todos los reactivos de alguna dimensión o subdimensión, logrando que tengamos un examen con una gran [subrepresentación del constructo](#subrep). Cuando diseñamos exámenes, por lo general se genera más de un reactivo para cada indicador, pero puede darse el caso que todos los reactivos de un indicador tengan alguna deficiencia, por lo que hay que considerar cuál o cuáles de ellos pueden ser más susceptibles a correcciones que ayuden a mejorar su desempeño, y con ello conservar el indicador.

## Dimensionalidad

Un aspecto muy importante a evaluar para obtener evidencias de validez es la dimensionalidad. Este aspecto se refiere a qué tanto los reactivos se agrupan de la manera que esperaríamos se agrupen. Supongamos que elaboramos un examen de matemáticas, donde incluimos varios reactivos de álgebra y varios de geometría. Si un estudiante sabe realizar operaciones algebraicas, pero tiene dificultades con geometría, esperaríamos que conteste de manera correcta la mayoría de los reactivos de álgebra y que no pueda contestar correctamente muchos de geometría. En otras palabras, nosotros suponemos que los reactivos que clasificamos como referentes a álgebra están midiendo toda la habilidad en álgebra de los estudiantes y los que elaboramos de geometría, que estén todos midiendo la habilidad en geometría. 

Tomando esto en cuenta, lo que esperaríamos es que nuestro examen tenga una estructura interna definida por lo que planteamos en un inicio al elaborar la tabla de especificaciones, si suponemos que todos los reactivos dentro de una dimensión o dominio están midiendo un mismo constructo, esperaríamos que estos reactivos se agrupen de esta manera. Habrá otras ocasiones en que el examen en su conjunto presente una estructura unidimensional, es decir, todos los reactivos se agrupen y midan un solo constructo. Por ejemplo, un examen de comprensión lectora es muy probable que presente una estructura unidimensional. 

Para poder determinar la estructura interna de un instrumento, son dos las principales técnicas utilizadas: análisis factorial exploratorio [AFE](#AFE) y análisis factorial confirmatorio [AFC](#AFC).

### Análisis Factorial Exploratorio

En el primer caso, mediante un software estadístico, se establecen algunos criterios para que el software haga una serie de análisis que permitan identificar la cantidad de factores (término utilizado para referirnos a las dimensiones del examen) necesarios y los reactivos que se agrupan en cada uno de estos factores. Dado que es un enfoque exploratorio, nosotros solamente proporcionamos los reactivos y algunos criterios como el método de estimación, la cantidad de factores o los métodos de rotación (se explicará más a detalle en la sección práctica), y es el software el que determina la estructura que mejor se ajusta a los datos. 

:::nota
**Nota:**
Dado que este procedimiento es más complejo en cuanto a los análisis y los criterios de decisión, recomendamos al lector revisarlo directamente en el siguiente capítulo de Análisis Estadísticos en R para que lo puedan revisar en un ejemplo concreto. Este comentario aplica tanto para AFE como para AFC y DIF que se verá más adelante.
:::

### Análisis Factorial Confirmatorio

En el caso del AFC, nosotros proporcionamos el modelo a contrastar. Es decir, nosotros especificamos qué reactivos pertenecen a qué dimensión o factor, y el software estadístico se encarga de identificar qué tan bien se ajusta ese modelo a los datos que tenemos. En otras palabras, el AFC nos dice en qué medida los reactivos realmente se comportan de esa manera. Por lo general, cuando hicimos una buena construcción de la tabla de especificaciones y de los reactivos, ya tenemos una idea de cómo esperamos que se agrupen los factores, por lo que es posible usar directamente el AFC en lugar de comenzar con el AFE.

## Confiabilidad

Cuando hablamos de confiabilidad nos referimos a “la consistencia de las calificaciones obtenidas por las mismas personas en ocasiones diferentes o con diferentes conjuntos de reactivos equivalentes” (Anastasi, 1966). En otras palabras, para que un instrumento sea confiable, nosotros esperaríamos que, si lo aplicamos a una persona en un momento dado, y se lo volvemos a aplicar tiempo después (asumiendo que su nivel de rasgo no cambie), la persona obtenga resultados iguales o muy similares en ambas aplicaciones. O, si le administramos dos instrumentos que miden lo mismo con reactivos equivalentes (reactivos que evalúen lo mismo con el mismo nivel de dificultad y discriminación), la persona obtendría también resultados muy similares en ambos instrumentos.

En el primer caso, la manera más simple de obtener la confiabilidad del instrumento sería al obtener la correlación entre los puntajes de la primera y la segunda aplicación ([Test-Retest](#test-retest)), y en el segundo caso, la correlación entre los puntajes de ambas versiones del instrumento. Sin embargo, estos dos procedimientos no siempre son viables. En el caso de exámenes, al ser preguntas de conocimientos, es más sencillo que los estudiantes se familiaricen con el contenido y les sea más sencillo en una segunda aplicación del mismo examen, por lo que el nivel de confiabilidad podría estar sesgado, además de que no siempre es posible reunir a las mismas personas para dos aplicaciones distintas. De igual forma, diseñar dos versiones paralelas del mismo examen, es costo y toma mucho tiempo, por lo que no en todos los casos es algo viable o deseable. 

Por ello, una tercera alternativa es evaluar la confiabilidad con respecto a la consistencia interna del instrumento, es decir, qué tanto los reactivos del instrumento se relacionan entre sí, y que tan homogéneos son. La prueba para medir la confiabilidad por consistencia interna más utilizada es el [Alpha de Cronbach](#alpha). Este coeficiente se ubica en un rango de 0 a 1, siendo los valores más cercanos a 1 indicadores de una consistencia interna alta. Sin embargo, este coeficiente tiene algunas limitaciones, dado que depende mucho de la cantidad de reactivos y del tamaño de la muestra (entre más reactivos tenga el examen y más personas se tengan en la muestra, más alto será el valor de Alpha). Además, se basa en algunos supuestos que no siempre es posible que se cumplan en el contexto de los exámenes. Por ello, una alternativa es utilizar el coeficiente [Omega de McDonald](#omega), el cual reduce los sesgos por tamaño de muestra y cantidad de reactivos, y toma en cuenta la dimensionalidad del instrumento (si el instrumento tiene varias dimensiones, las analiza en conjunto y otorga un valor específico para cada dimensión y uno global).

:::aera
**Estándar 2.3:**
"Para cada puntaje total, subpuntaje o combinación de puntajes que deba interpretarse, deben reportarse estimaciones de índices relevantes de confiabilidad/ precisión."
:::

##	Imparcialidad

Un análisis que puede resultar de suma importancia de acuerdo con nuestros usos e interpretaciones previstos es el análisis de la imparcialidad. Cuando tenemos grupos claramente diferenciados a quienes se les aplicará nuestro examen, (e.g., hombres y mujeres, personas extranjeras y nativas, personas provenientes de sistemas de bachillerato diferentes, etc.), uno de los objetivos que debemos asegurar para nuestro examen, es que sea capaz de determinar el nivel de habilidad que tiene la persona en el constructo que evalúa nuestro examen independientemente de sus características particulares o grupo de pertenencia; en otras palabras, queremos que nuestro examen determine si alguien es aprobado o reprobado por su nivel de habilidad únicamente, y no que una persona apruebe porque le da cierta ventaja pertenecer a un grupo específico. Cuando los reactivos se comportan de esta manera, decimos que son reactivos sesgados, en los que algo de su redacción, planteamiento u opciones de respuesta, está favoreciendo alguna característica de uno de los grupos, independientemente de su nivel de habilidad.

Para detectar la presencia de este sesgo, la herramienta más utilizada es el análisis del [Funcionamiento Diferencial del Reactivo](#DIF-G) (DIF por sus siglas en inglés). Existen distintos métodos para estimar la presencia de DIF en los reactivos, algunos provenientes de la TRI y otros provenientes de la TCT. En general, estos métodos buscan analizar si existen diferencias en la probabilidad de los participantes de responder correctamente un reactivo, cuando ambos participantes tienen el mismo nivel de habilidad. 

:::nota
**Nota:**
Para revisar con más detalle la interpretación de estos análisis, por favor dirigirse al capítulo anexo de análisis en R en la sección de análisis DIF
:::

Una vez que hemos identificado si existen reactivos que presentan DIF, es importante que hagamos un análisis cualitativo para interpretar aquello que puede estar generando este DIF en el reactivo. La presencia de DIF no significa necesariamente que el reactivo esté mal; en realidad, la presencia de DIF puede también indicar diferencias reales entre los grupos evaluados. Por ejemplo, que un grupo proveniente de cierta universidad salga más alto que los de otras universidades, puede ser porque el examen esté sesgado y evalúe cosas que solo se vieron en ese programa, pero también puede que sea debido a que en general, en esa universidad se ofrece una preparación de mayor calidad que en otras escuelas, y el examen está siendo capaz de medir esas diferencias. En todo caso, es importante indagar más, tanto a nivel cuantitativo con los siguientes análisis como a nivel cualitativo con la revisión de expertos y la presentación de evidencias que respalden una u otra afirmación.

Dependiendo de estos resultados, será importante tomar la decisión de si todos los reactivos se utilizan para la calificación, o si se decide omitir ciertos reactivos de la calificación final dado que presentan un DIF muy significativo.


:::aera
**Estándar 4.10:**
"Cuando un desarrollador de pruebas evalúa las propiedades psicométricas de los ítems, el modelo utilizado para ese fin (p. ej., teoría clásica de los tests, teoría de respuesta al ítem u otro modelo) debe documentarse. La muestra utilizada para estimar las propiedades de los ítems debe describirse y debe ser de un tamaño y diversidad adecuados para el procedimiento. El proceso por el cual se criban los ítems y los datos utilizados para cribado, como dificultad del ítem, discriminación de ítems, o funcionamiento diferencial de los ítems (DIF) para grupos importantes de individuos examinados también deben documentarse. Cuando se utilizan métodos basados en modelos (p. ej., TRI) para estimar los parámetros de los ítems en el desarrollo de pruebas, el modelo de respuesta al ítem, los procedimientos de estimación y la evidencia de ajuste del modelo deben documentarse."
:::


## Evidencias de validez

Recordando que lo que se vuelve válido no es el instrumento en sí, sino las interpretaciones y usos que se le dan a los resultados del mismo, el tema de recopilar evidencias de validez para estos usos e interpretaciones se vuelve sumamente amplio y complejo. 

Además de las evidencias que ya hemos ido generando, algunos de los principales métodos para obtención de evidencia de validez que vienen de la elaboración de análisis estadísticos son los siguientes:

### Correlación con otras pruebas: Supongamos que nosotros definimos nuestro examen como un instrumento de medición que permite dar cuenta del constructo de comprensión lectora a partir de cierta teoría, y esta teoría, afirma que la comprensión lectora tiene una relación significativa con la memoria de trabajo que mencionamos en otro ejemplo. Si queremos generar mayor evidencia de la validez del uso de este examen como una medida de comprensión lectora bajo esta teoría, podríamos aplicar esta misma prueba junto con nuestro examen a los participantes y medir qué tanto se relacionan estadísticamente estas dos pruebas. 

De igual forma, si según nuestra teoría, existe una relación fuerte entre el nivel de comprensión lectora y el número de libros que la persona reporta leer al año en promedio, podríamos solicitar este dato a nuestros participantes y observar si esta relación se presenta como esperaríamos. 

En términos generales, este aspecto tiene que ser planeado desde un inicio, definiendo qué pruebas o qué datos puedo solicitar a los sustentantes que me permitan dar cuenta de la validez de esta interpretación bajo cierta teoría. La decisión de qué aspectos incluir estará influida por el tiempo que dispondremos para la aplicación, por la longitud de nuestros instrumentos (no es conveniente aplicar muchos instrumentos seguidos, ya que los participantes podrían cansarse y reducir su rendimiento), por la solidez de la teoría que afirma la relación entre los instrumentos y por los requerimientos específicos de la institución en relación con los usos e interpretaciones previstos. 

### Regresiones lineales y logísticas

Si bien las correlaciones pueden ser una fuente importante de evidencias de validez, en ocasiones necesitaremos más que demostrar la relación entre dos variables, ver si esa relación tiene algún nivel de causalidad. Por lo general, muchos exámenes se diseñan para ser usados como criterios para la toma de decisiones, y esto implica que tomamos el resultado del examen como un predictor de algo que acontecerá en el futuro. Por ejemplo, un examen de admisión es utilizado como criterio para decidir si alguien es admitido o no en una universidad, bajo la premisa de que, si obtiene un puntaje alto en el examen, es más probable que tenga un desempeño alto en el programa al que será admitido. Para poder presentar evidencia de que esta interpretación del resultado del examen como predictor de desempeño es válida, necesitamos, no solamente literatura que señale dicha relación causal, sino también evidencia empírica de nuestro caso en particular. Este ejemplo puede resultar sumamente complejo para generar evidencias, pero hay otros casos en los que lo que intentamos predecir es más sencillo de medir a corto plazo. Por ejemplo, un examen de egreso del bachillerato como predictor del desempeño de ingreso a licenciatura, o un examen que predice los puntajes en otro examen específico. 

Cuando hablamos en términos de predicción, la herramienta más utilizada es el análisis de [regresión lineal](#regresion) o de regresión logística. El primero nos permite predecir, a partir de algún criterio como el puntaje de nuestro examen, el resultado en otra variable con varias opciones de resultado posibles; por ejemplo, predecir las calificaciones de un alumno a partir de su puntaje de ingreso, o predecir su puntaje en otro examen. Por su parte, la regresión logística, nos permite predecir la probabilidad de que se dé una condición específica o no. Por ejemplo, podemos intentar predecir, a partir de un examen de egreso del bachillerato, la probabilidad de que los alumnos sean admitidos o no a la educación superior. 

## Calificación

Una vez que hemos terminado todos los análisis anteriores y contamos con una versión final del instrumento que presenta buenos índices de calidad de los reactivos, una adecuada dimensionalidad, confiabilidad y otras evidencias de validez de las interpretaciones, es momento de tomar los reactivos que terminamos conservando y utilizarlos para reportar las calificaciones de los sustentantes. 

Si hemos llevado el proceso de manera sistemática hasta este momento, siguiendo todos los pasos aquí especificados, y si realizamos los análisis con la clave de respuestas de manera correcta, este procedimiento resultará sumamente sencillo, dado que solo queda generar la suma de los aciertos de cada estudiante, proceso que ya realizamos en el capítulo de análisis en R. Sin embargo, hay algunos pasos adicionales correspondientes a la interpretación y al reporte de estos resultados.

###	Métodos para establecer puntos de corte (standard setting)

Si uno de los usos establecidos para la prueba involucra la definición de grupos específicos a partir de los puntajes de la prueba; por ejemplo, clasificar a los sustentantes en desempeño insatisfactorio, satisfactorio y sobresaliente o determinar una calificación aprobatoria para la admisión a la universidad, es necesario llevar a cabo un procedimiento más para definir de la manera más objetiva y empírica posible los [puntos de corte](#corte) que vamos a utilizar para cada caso. Existen muchos y muy diversos métodos para establecer puntos de corte, y cada uno de ellos utiliza procedimientos muy diversos. Para fines prácticos, en este manual describiremos únicamente uno de los métodos más utilizados: el método de Angoff utilizado para situaciones en las que requerimos calcular únicamente un punto de corte (e.g., aprobado-no aprobado) basados en un criterio específico. Si al lector le interesa obtener más información sobre distintos métodos, o desea obtener más de un punto de corte o puntos referentes a normas, recomendamos revisar el capítulo 10 del libro de Lane et al. (2016).

#### Método de Angoff

Para poder determinar el punto de corte de nuestro examen con este método, necesitamos de la ayuda de un grupo de expertos para tomar la decisión. Este grupo puede ser el mismo que nos ayudó con la generación de reactivos, aunque si es posible conseguir otro grupo, también es algo recomendable. 

El método de Angoff consiste en presentar a este grupo de expertos cada uno de los reactivos que se utilizarán para puntaje, y solicitarle a cada uno, primero, que tengan en mente cómo sería un examinado con un nivel de competencia mínimamente necesario para aprobar el examen (o si la decisión es que el examinado entre o no entre a la universidad, por ejemplo, se buscaría tener en mente a una persona con el nivel mínimo de competencia necesario para desempeñarse adecuadamente en la universidad). Una vez que se considera este perfil de acuerdo con cada uno de los jueces, se les solicita que determinen cual consideran que es la probabilidad de que un sustentante con el nivel de competencia mínimamente requerido conteste correctamente dicho reactivo, ubicando la probabilidad normalmente en una escala de 1 a 100. 

Ya que se han generado las probabilidades de cada reactivo por cada juez, suele hacerse una segunda ronda, permitiendo la discusión y diálogo entre los jueces y otorgar retroalimentación, para así reducir la variabilidad entre sus puntajes, buscando generar resultados más consistentes. 

:::nota
**Nota:**
En ocasiones, se suele presentar a los jueces los resultados psicométricos de los reactivos para ayudarles en su toma de decisión, en particular, se les puede mostrar los índices de dificultad y de discriminación de cada reactivo.
:::

Una vez que se han dado las dos rondas, se capturan los resultados finales de cada uno de los jueces y se elabora una tabla en la que se registre el puntaje que cada juez le dio a cada reactivo, para después, promediar estos puntajes por reactivo y por juez, y al sumar todas estas probabilidades promedio y dividirlas entre el total de jueces y reactivos, se obtiene el punto de corte. Dado que se les solicita probabilidades de 0 a 100, lo que obtenemos como puntaje se puede interpretar como un porcentaje de respuestas correctas.

:::aera
**Estándar 5.21:** "Cuando las interpretaciones de puntajes propuestas involucran uno o más puntajes de corte, deben documentarse claramente la justificación y los procedimientos utilizados para establecer puntajes de corte."
:::

:::evidencia
**Evidencia documental:**
Durante esta fase, es importante registrar todos los procedimientos llevados a cabo, especialmente la metodología utilizada para llegar a ciertos resultados. Por ejemplo, qué criterios se utilizaron para desarrollar el AFE, de qué manera se estimó el índice de discriminación (mediante TCT o TRI), qué método se utilizó para estimar la confiabilidad, etc. De igual forma, todos los resultados deben ser capturados y resguardados para utilizarlos posteriormente en el informe final.
:::

:::seguridad
**Seguridad:**
Es importante que diseñemos un protocolo que nos permita resguardar y limitar el acceso a las bases de datos utilizadas para realizar el análisis,
:::


:::quiz

[**¿Para qué nos sirven los análisis de las propiedades de los reactivos como el análisis de dificultad, discriminación o de los distractores?**]


:::



:::quiz

[**¿Si tenemos un reactivo que presenta un índice de dificultad de 0.20 y que presenta un DIF moderado, debemos eliminarlo? Justifica tu respuesta**]

:::



:::quiz

[**¿Para qué necesitamos hacer los análisis DIF?**]

:::