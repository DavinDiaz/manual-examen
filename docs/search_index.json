[["index.html", "ASPECTOS ESENCIALES EN LA CONSTRUCCIÓN DE EXÁMENES DE OPCIÓN MÚLTIPLE ¿Para qué necesitamos revisar un manual técnico? ¿Cúales son las fases del procedimiento? ¿Cómo leer este manual?", " ASPECTOS ESENCIALES EN LA CONSTRUCCIÓN DE EXÁMENES DE OPCIÓN MÚLTIPLE Davin Eduardo Díaz García Elizabeth Vega Gatica 2021-06-14 ¿Para qué necesitamos revisar un manual técnico? El presente manual está dirigido a comisiones académicas de licenciatura y posgrado, no expertas en evaluación, que se encargan de la elaboración de exámenes de opción múltiple. Busca ser una guía en el proceso de diseño, desarrollo, aplicación, análisis e interpretación de exámenes de alto impacto que pretendan ser usados como herramientas que permitan la toma de decisiones en distintos ámbitos a nivel licenciatura o posgrado. Mediante una serie de cuestionamientos y preguntas guía, además de información sintetizada, buscamos que los lectores puedan diseñar, construir y validar los exámenes de acuerdo con sus necesidades y contextos específicos. Cuando queremos medir aprendizajes, conocimientos, habilidades o desempeños de nuestros alumnos, debemos buscar herramientas que registrarlos u observarlos, por lo que buscamos desarrollar un instrumento que permita realizar afirmaciones sobre el desempeño de los individuos. Pero, desarrollar un instrumento no es tarea fácil, requiere de una serie de pasos y condiciones que ayuden a garantizar la pertinencia, justicia, confiabilidad y la validez de las interpretaciones que se quieran emitir y que tienen un impacto significativo en la vida de los alumnos, por lo que es fundamental desarrollar el instrumento basándonos en las buenas prácticas de evaluación, es decir, en los Estándares para Pruebas Psicológicas y Educativas del American Educational Research Association, American Psychological Association y National Council on Measurement in Education (AERA, APA, and NCME 2018), las cuales nos ayudarán a evitar perjudicar con los resultados de las pruebas a los alumnos; es decir, el objetivo es lograr la construcción de instrumentos confiables, válidos y justos, es por eso que a continuación se presentan una serie de componentes que nos guiarán en el diseño y desarrollo de instrumentos de medición adecuados basados en los estándares y en evidencias científicas. ¿Cúales son las fases del procedimiento? Fases del procedimiento: ¿Cómo leer este manual? Como el lector se podrá dar cuenta, el presente manual está constituido por una serie de fases que permitirán el diseño y desarrollo de exámenes de alto impacto; además cada una de las fases cuenta con 5 apartados adicionales, que contienen información de suma importancia, los apartados se verían de la siguiente manera: Evidencias documentales En este apartado se señalan los productos resultantes de cada fase que servirán para dar evidencias de los procesos o análisis realizados, lo cual permitirá apoyar la validez de las interpretaciones y usos de los resultados. Seguridad Este es un aspecto transversal en todas las fases de construcción del examen, por lo que en cada fase se hacen explicitas las recomendaciones para el resguardo de la información obtenida. Estándares Dado que el presente manual está basado en los Estándares para Pruebas Psicológicas y Educativas de la AERA, APA, and NCME (2018), se hacen explícitos algunos de estos estándares que de manera general abordan cada una de las fases. Preguntas Al finalizar cada fase se plantean uno o más cuestionamientos que permiten al lector reflexionar sobre lo aprendido en dicha fase, e identificar si no se entendió de manera clara algún aspecto del contenido, es decir, sirven como un método de autoevaluación del aprendizaje alcanzado. Las respuestas propuestas a estas preguntas se incluirán en una sección distinta, por si el lector desea consultarlas, pero sí recomendamos que intente responderlas por sí mismo para reforzar el aprendizaje. Notas También se realizan recomendaciones sobre bibliografía de apoyo para profundizar en ciertos aspectos del proceso de evaluación y se hace uso de notas que permiten aclarar determinados puntos especificados en el contenido de las fases. De igual manera, al ser un manual interactivo, el lector podrá encontrar durante su lectura algunos vínculos que le permiten trasladarse a otro apartado del manual que sea necesario leer según las necesidades y propósitos del examen que se va a desarrollar, así como vínculos que permiten al lector ir directamente al glosario ubicado al final del manual. Finalmente, es preciso señalar que, aunque las fases tienen un orden lógico determinado, una recomendación que permitirá al lector darle un uso efectivo y práctico a la información proporcionada por el manual, es realizar una lectura general de todo el manual antes de comenzar a planear, diseñar y desarrollar el instrumento, lo cual permitirá la comprensión de la relación existente entre las diversas fases y por ende facilitará la realización de la primera fase del manual, la cual requiere el planteamiento de los aspectos generales de todo el proceso de diseño y desarrollo del examen, por lo que se necesita entender el proceso de construcción en su totalidad. Considerando lo anterior, el presente manual usa como referencias principales tres fuentes bibliográficas: AERA, APA, and NCME (2018), INEE (2019) y Lane, Raymond, and Haladyna (2016), por lo que las citas que se encuentren dentro del contenido de las fases son fuentes específicas de elementos más especializados, planteadas de acuerdo con los requerimientos de las fases del manual. Ahora si estamos listos para comenzar la construcción de nuestro examen. REFERENCIAS "],["COMP1.html", "COMPONENTE 1 ¿QUÉ, PARA QUÉ Y CÓMO VAMOS A EVALUAR? 1.1 ¿Qué voy a medir? 1.2 ¿Para qué voy a medir? 1.3 ¿A quién voy a evaluar? 1.4 ¿Cuáles serán las interpretaciones para los usos previstos de la prueba? 1.5 ¿Cuál será el tipo de instrumento y el formato de los reactivos? 1.6 ¿Qué procedimiento seguiremos para saber si nuestro instrumento es válido, confiable y que nuestros reactivos se comportan de manera adecuada? 1.7 ¿Cuántos reactivos tendrá nuestra prueba y como estará organizada? 1.8 ¿Cómo calificaremos la prueba? 1.9 Otras características importantes a considerar 1.10 A modo de repaso", " COMPONENTE 1 ¿QUÉ, PARA QUÉ Y CÓMO VAMOS A EVALUAR? En este componente debemos considerar un panorama general de los aspectos esenciales del desarrollo de una prueba, los cuales nos permitirán responder las cuestiones básicas ¿qué voy a evaluar? ¿para qué voy a evaluar? y, ¿cómo voy a evaluar?, todo esto nos servirá como un marco de referencia para que los componentes posteriores que conforman este manual trabajen en conjunto en la tarea de respaldar las afirmaciones que hagamos de los resultados de los alumnos observados en la prueba, buscando siempre que exista una alineación y coherencia entre las distintas fases del proceso de desarrollo de la prueba, con respecto a los usos e interpretaciones que pretendemos hacer de los resultados del examen. Los pasos que conforman este componente son los siguientes según el INEE (2019) y Lane, Raymond, and Haladyna (2016): 1.1 ¿Qué voy a medir? Debemos definir que habilidad o conocimiento queremos evaluar, aclarando desde qué modelo teórico o educativo será abordado, por ejemplo, si nuestra prueba medirá comprensión lectora, ese sería nuestro constructo y podríamos especificar que lo abordaremos basándonos en el marco educativo de la prueba de competencia lectora de PISA 2018 (Educación 2018). 1.2 ¿Para qué voy a medir? Tenemos que definir el propósito de la evaluación, puesto que es el que guiará la forma en la que se desarrollarán los componentes posteriores, por lo que siempre hay que tenerlo presente. Por ejemplo, nuestro examen de comprensión lectora puede tener como propósito realizar una evaluación diagnóstica de la habilidad de comprensión lectora de los alumnos de sexto semestre de la carrera de Psicología a fin de contar con elementos que permitan al docente ajustar los contenidos que se abordarán durante el semestre. 1.3 ¿A quién voy a evaluar? Tenemos que especificar claramente las características de la población objetivo y el contexto, con el fin de enfatizar que las inferencias o interpretaciones que se hagan de los resultados de la prueba son válidas para esa población y contexto en específico. Siguiendo con el ejemplo anterior nuestra población serían los estudiantes de sexto semestre de la licenciatura en Psicología del turno matutino y vespertino de la Facultad de Psicología de la UNAM, facultad que se encuentra en la Ciudad de México. Este aspecto es importante porque las inferencias que se realicen solo serán válidas a esta población, si se quisiera aplicar este instrumento en estudiantes de sexto semestre de otro contexto tendríamos que hacer de nuevo una revisión exhaustiva de todo el instrumento y de sus parámetros psicométricos. 1.4 ¿Cuáles serán las interpretaciones para los usos previstos de la prueba? Tenemos que especificar si las interpretaciones que haremos de los resultados de la prueba serán con referencia a normas o a criterios establecidos, en ambas lo que hacemos es comparar la evaluación con un referente, en el caso de una evaluación referente a normas o normativo el referente de comparación son el conjunto de sustentantes que respondieron la prueba, mientras que en una evaluación referente a criterios o criterial, se interpretan los resultados a partir de un estándar o criterio, por lo que hay que señalar de manera precisa cómo se hará la interpretación de los resultados. Un ejemplo de un examen que sus resultados se interpretan a partir de la norma sería si nuestro examen de comprensión lectora nos sirve para ordenar a los alumnos de menor a mayor puntaje y así a los primeros diez lugares les damos una beca para un curso sobre redacción de cuentos y ensayos; mientras que un examen criterial sería aquel en el que nuestras interpretaciones de los resultados se basan en criterios de desempeño como básico, competente y sobresaliente y a partir de esos resultados establecemos que los que tengan un desempeño básico en comprensión lectora irán a un curso de regularización. Como podemos notar las interpretaciones de los resultados no pueden separarse de sus usos, es decir, de las decisiones que se tomarán a partir de ellos, por lo que en este paso es fundamental precisar cuáles serán los usos válidos y no válidos de los resultados del instrumento. En el ejemplo anterior podemos especificar que el examen tendrá un uso para el otorgamiento de becas y que los usos inválidos serían usar los resultados para aprobar o reprobar alumnos en un curso. En el otro ejemplo los usos previstos serían el de asignar a los estudiantes a un curso de regularización y como usos inválidos podemos establecer usar los resultados para asignar calificaciones finales de un curso. Tenemos que ser muy claros en los usos válidos y no válidos para evitar que se hagan malos usos de la prueba que corrompan la percepción que se tiene de la evaluación y por ende dejen de ser válidas nuestras interpretaciones de la prueba. 1.5 ¿Cuál será el tipo de instrumento y el formato de los reactivos? Con base en los pasos anteriores tenemos que decidir qué tipo de instrumento desarrollaremos en función del tiempo y recursos con los que contamos, si va a ser una prueba objetiva o un cuestionario, si las repuestas que se solicitarán serán de selección o libres y también debemos especificar si la administración del instrumento está planeada para ser en lápiz y papel o en computadora. Por ejemplo, para una prueba de comprensión lectora podemos hacer uso de una prueba objetiva con reactivos de selección múltiple que contarán con cuatro opciones de respuesta donde solo una será la repuesta correcta y la aplicaremos en formato de lápiz y papel. Este manual se enfocará en el proceso referente a pruebas objetivas con reactivos de selección múltiple. Sin embargo, es importante que el lector tenga la posibilidad de utilizar otras opciones, las cuales puede consultar en libros como el de Lane, Raymond, and Haladyna (2016). 1.6 ¿Qué procedimiento seguiremos para saber si nuestro instrumento es válido, confiable y que nuestros reactivos se comportan de manera adecuada? Aquí se hace referencia al modelo de medición de la prueba, es decir, debemos hacer explicita la manera en que obtendremos los parámetros de los reactivos en términos de su dificultad (cantidad de sustentantes que responden correctamente el reactivo) y su discriminación (si el reactivo diferencia entre los sustentantes que saben y los que no saben), la confiabilidad de la medición (precisión del instrumento) y la validez de la evaluación (inferencias precisas de los resultados de la evaluación). También es importante determinar desde este momento si llevaremos a cabo análisis de sesgos para asegurar la justicia en la evaluación, es decir, que los reactivos no actúen de manera diferente en las subpoblaciones en términos de sexo, religión, etnia, nacionalidad, idioma, etc. En este paso también debemos establecer los criterios de calidad de los reactivos e instrumentos, en otras palabras, debemos determinar las propiedades estadísticas deseadas para determinar que un reactivo es adecuado o no y también si el instrumento está funcionando o no de acuerdo con nuestro propósito. Por ejemplo, podemos decidir que la validez del contenido de la prueba la determinaremos por medio de un grupo de expertos que revisarán los reactivos, la confiabilidad la determinaremos a través de la consistencia interna del instrumento usando el coeficiente Alpha de Cronbach y con el método de pruebas paralelas, dado que desarrollaremos dos versiones del examen; con respecto a las propiedades estadísticas podemos determinar que la confiabilidad será mayor a 0.8 con un nivel de significancia de 0.05. 1.7 ¿Cuántos reactivos tendrá nuestra prueba y como estará organizada? El determinar la longitud del instrumento depende del propósito de la evaluación, de los recursos y tiempo disponible y de las interpretaciones y usos previstos de los resultados, también tenemos que señalar si habrá varias versiones de una misma prueba, y cómo se organizará el instrumento en sus diferentes dimensiones. Por ejemplo, la prueba de comprensión lectora aplicada a alumnos de sexto semestre contará con 60 reactivos, 20 de la dimensión de localización de información, 20 de comprensión y 20 de evaluación y reflexión que en conjunto serán resueltos en un tiempo de 2 horas. 1.8 ¿Cómo calificaremos la prueba? En este apartado tenemos que retomar lo que se establecimos en el paso 1.4 sobre si nuestras interpretaciones de los resultados estarán basadas en la norma o en criterios de desempeño, pero tenemos que ser específicos en la forma en que calificaremos la prueba, señalando si los reactivos solo tienen una respuesta correcta y las demás son los distractores o si hay reactivos con respuestas parcialmente correctas; de igual manera se especifica si todos los reactivos tienen el mismo peso en la calificación o si una dimensión del instrumento tiene un peso mayor. Por ejemplo, nuestro examen de comprensión lectora que cuenta con 60 reactivos de respuesta correcta única se calificará otorgando un punto a cada reactivo y todas las dimensiones tienen el mismo valor para la calificación final del examen. 1.9 Otras características importantes a considerar Definir los grupos de expertos que participaran en el desarrollo de cada componente, por ejemplo, en la elaboración de las especificaciones, otros en el desarrollo de reactivos y otros en la revisión de los reactivos elaborados, especificando los procedimientos de selección y sus cualificaciones. También es importante definir desde este momento si se realizaran adaptaciones al instrumento para los sustentantes con discapacidades o que hablen otro idioma. Como nos podemos dar cuenta en la totalidad de este componente definimos de manera clara los aspectos básicos de nuestra prueba y determinamos de manera general los procedimientos que seguiremos en los componentes posteriores de manera más detallada. Evidencia documental: En este componente podemos obtener como evidencias la ficha técnica de la prueba, la información académica y profesional de los expertos que nos apoyarán en los componentes posteriores y el protocolo de seguridad de la información. Seguridad: Hay que tener en cuenta el resguardo de la información que se construya en este componente, por lo que es necesario establecer protocolos de seguridad y hacerlos explícitos a los grupos de expertos que nos apoyarán en las siguientes fases de desarrollo. Estándar 4.0 Las pruebas y programas de evaluación deben diseñarse y desarrollarse de una manera que respalde la validez de las interpretaciones de los puntajes de la prueba para sus usos previstos. Los desarrolladores y editores de pruebas deben documentar las medidas tomadas durante el proceso y desarrollo de la prueba para proporcionar evidencia de imparcialidad, confiabilidad y validez para los usos previstos para individuos en la población prevista de individuos examinados (AERA, APA, and NCME 2018) Estándar 1.8 La composición de cualquier muestra de examinandos de la cual se obtiene evidencia de validación debe describirse con tanto detalle como sea práctico y aceptable, incluidas características sociodemográficas y de desarrollo relevantes. (AERA, APA, and NCME 2018) 1.10 A modo de repaso ¿Por qué es necesario establecer los usos inválidos de la prueba? ¿Qué función tiene el propósito de la evaluación? REFERENCIAS "],["COMP2.html", "COMPONENTE 2 ¿CÓMO DELIMITAMOS LO QUE EVALUAREMOS EN NUESTRO EXAMEN? 2.1 Definiendo nuestor marco conceptual 2.2 Pasando de la definición del constructo a la construcción de reactivos 2.3 A modo de repaso", " COMPONENTE 2 ¿CÓMO DELIMITAMOS LO QUE EVALUAREMOS EN NUESTRO EXAMEN? En este componente revisaremos todo el procedimiento necesario para delimitar el constructo que evaluaremos, desde el marco conceptual hasta la generación de nuestra tabla de especificaciones. Todo el procedimiento aquí definido se basa primordialmente en la propuesta del INEE (2019) 2.1 Definiendo nuestor marco conceptual Una vez que ya sabemos lo que queremos medir, y tenemos un plan general de cómo queremos lograrlo, es importante que definamos el marco conceptual a partir del cual vamos a generar nuestro instrumento. Al hablar de marco conceptual nos estamos refiriendo a la base teórica que da sustento a los reactivos que vamos a redactar en la siguiente sección. En el ejemplo que ya hemos utilizado, si lo que queremos es medir la comprensión lectora de nuestros alumnos, primero tenemos que definir a qué nos referimos con comprensión lectora, o qué significa que una persona tenga comprensión lectora. En ese sentido, en lugar de tratar de generar una definición propia, es importante que, para darle mayor sustento a nuestro instrumento, comencemos a revisar las definiciones y teorías que ya existen y que otros autores han dado con respecto al constructo, lo que nos permitirá darnos cuenta que existen diversas posturas con respecto a lo que significa evaluar el nivel de comprensión lectora; dentro de estas distintas posturas, habrán algunas que nos resulten más afines a los propósitos de nuestro examen, o que simplemente abarquen el constructo de mejor manera de acuerdo con lo que planteamos inicialmente. Es por ello que es de suma importancia que hagamos una revisión extensiva de la literatura que nos permita ubicar el marco conceptual y la definición del constructo que tengan mayor concordancia con los propósitos que planteamos en la primera etapa. Nota: Es importante considerar que cuando hagamos la búsqueda para definir específicamente el constructo que queremos medir, no nos encontraremos con definiciones aisladas y puntuales, sino con marcos conceptuales extensos que den sustento a las definiciones propuestas, y que habrá varios autores y trabajos encaminados dentro de un mismo marco conceptual. Habrá constructos como el de comprensión lectora que tendrán una amplia gama de modelos teóricos de donde elegir, pero habrá otros constructos menos estudiados dentro de los que quizá tendremos que generar nuestras propias definiciones a partir de lo que ya han estudiado otros autores al respecto. Estándar 1.12: Si la razón fundamental para la interpretación de los puntajes para un uso dado depende de premisas sobre los procesos psicológicos u operaciones cognitivas de los examinandos, debe proporcionarse la evidencia teórica o empírica que respalde esas premisas. Cuando enunciados sobre los procesos empleados por observadores o calificadores sean parte del argumento de validez, debe proporcionarse información similar. (AERA, APA, and NCME 2018) 2.2 Pasando de la definición del constructo a la construcción de reactivos 2.2.1 Operacionalizando Ahora que ya tenemos el propósito del instrumento y el constructo que va a evaluar (definido dentro de un marco conceptual), la siguiente pregunta es ¿cómo paso de la definición del constructo a generar los reactivos? La mejor manera de llevar a cabo este procedimiento es mediante la operacionalización, una palabra muy técnica que en esencia significa traducir la definición de nuestro constructo en términos medibles y observables. En otras palabras, se trata de descomponer el constructo en aquellas dimensiones o aspectos que nos puedan dar cuenta de éste. Si seguimos con el ejemplo del examen de comprensión lectora, la pregunta quizá sería: ¿qué características tiene o qué competencias cuantificables puede demostrar una persona con buena comprensión lectora? Supongamos que nos basamos en el modelo conceptual que ofrece PISA (Educación 2018). para responder a estas preguntas. Dentro de la definición de comprensión lectora que PISA ofrece, destacan tres componentes principales: localizar, comprender y reflexionar. En este ejemplo, estos tres componentes se convertirán en las dimensiones principales de nuestro instrumento, que a su vez se desglosarán en subcomponentes específicos. Nota: Estas distintas partes del instrumento suelen tener nombres diversos según se revisen dentro de la literatura, por lo que es posible encontrarles como áreas, dominios, componentes, dimensiones, etc. En exámenes de conocimientos, es importante que delimitemos el contenido que se va a incluir. Por lo general, tanto nuestra definición del marco conceptual como nuestro proceso de operacionalización estarán en función ya sea del plan de estudios o del programa de la materia que se pretende evaluar. En estos casos, nuestra función será definir qué contenidos de dicho plan de estudio o programa serán los que incluiremos en nuestro examen, lo cual puede depender en gran medida del propósito del examen y de sus usos e interpretaciones previstos. Habrá ocasiones, como el ejemplo anterior, en que el marco conceptual ya incluya un proceso de operacionalización; es decir, que ya dentro de la revisión de la literatura nos encontremos con una definición que divide el constructo en distintos componentes y subcomponentes. En otras ocasiones, el constructo que queremos medir quizá ya esté definido por el plan de estudios o programa, como ya se mencionó. Por ejemplo, puede que busquemos generar un examen para evaluar los conocimientos al egreso de alumnos de preparatoria con respecto a su nivel de inglés. Dado que lo que deseamos evaluar son los conocimientos adquiridos durante su formación en este tema, la ruta más evidente será referirnos al plan de estudios de la preparatoria y revisar los contenidos específicos de las materias referentes a inglés, donde ya encontraremos desglosados los componentes y objetivos de aprendizaje que se tuvieron durante su impartición. Estándar 1.11: Cuando la razón fundamental para la interpretación de los puntajes de la prueba para un uso dado se basa en parte en lo apropiado del contenido de la prueba, los procedimientos seguidos en la especificación y generación del contenido de la prueba deben describirse y justificarse con referencia a la población que se prevé evaluar y al constructo que la prueba tiene por objeto medir o el dominio que tiene por objeto representar. Si la definición del contenido muestreado incorpora criterios como la importancia, frecuencia o criticidad, estos criterios también deben explicarse y justificarse con claridad. (AERA, APA, and NCME 2018) 2.2.2 Elaborando nuestra tabla de especificaciones Ya que hemos definido en términos operacionales nuestro constructo, es importante que capturemos esa información en lo que llamaremos nuestra tabla de especificaciones. Esta herramienta será nuestra guía fundamental que nos permita generar los reactivos directamente a partir de la definición que hemos generado. Comenzaremos por mostrar un ejemplo de tabla de especificaciones que nos permitirá desglosar sus componentes y comprender su estructura. Cabe hacer la aclaración, de que no existe una sola forma de generar tablas de especificaciones, y que los componentes de la misma dependerán mucho del propósito de nuestro instrumento y de las características del constructo. Ejemplo de tabla de especificaciones: Dentro de la primera columna, será importante que definamos los componentes o dimensiones principales de nuestro constructo; es decir, las áreas que se desglosan directamente de la definición del constructo. En el ejemplo de comprensión lectora, estas tres áreas tomadas de la definición de PISA (Educación 2018) serían localizar, comprender y reflexionar. Después, en la segunda columna añadiremos los subcomponentes en caso de que el constructo evaluado lo permita. Supongamos que queremos hacer un examen de historia de México. Los componentes serían quizá grandes fases o etapas de la historia, como la revolución mexicana, y los subcomponentes podrían ser acontecimientos importantes dentro de estas etapas históricas, como el plan de San Luis, la decena trágica o el plan de agua prieta. Como estamos hablando de exámenes de conocimientos o habilidades, es importante que definamos también los aprendizajes esperados o los aspectos a evaluar. En otras palabras, en el ejemplo del examen de historia de México, debemos definir qué del plan de San Luis esperamos que el alumno sepa específicamente. Para generar estos aprendizajes esperados, es importante que los definamos en términos de niveles de aprendizaje. Para ello, el ejemplo que se utilizará aquí se basa en la taxonomía de Bloom revisada por Anderson and Bloom (2001), pero cabe aclarar que no es la única taxonomía existente para los niveles de aprendizaje. Una de las mayores ventajas de utilizar este tipo de taxonomías es que nos ayudan a acercarnos más en el camino de generar reactivos, ya que nos permiten aterrizar los dominios y subdominios en acciones concretas. La taxonomía de Bloom nos muestra distintos niveles de habilidades cognitivas que van de lo más sencillo a lo más complejo, y dichas habilidades son definidas en términos de verbos específicos para cada una de ellas, lo que permite llevar el conocimiento a la acción. Niveles de aprendizaje de Bloom: En un examen específico, dependiendo del nivel educativo, y sobre todo de los contenidos particulares del constructo que se pretende medir, podremos requerir usar todos los niveles presentados en la taxonomía o solamente algunos. Por ejemplo, en el examen de historia de México, habrá una importante cantidad de reactivos que impliquen únicamente recordar, que permitan demostrar que el alumno ubica específicamente los acontecimientos en los momentos en que ocurrieron. Sin embargo, habrá otros reactivos que impliquen no solamente recordar, sino también comprender cómo esos acontecimientos se relacionan, o qué situaciones fueron las causantes de ciertos eventos históricos. Podríamos incluir también reactivos que impliquen analizar y evaluar, pero difícilmente podremos incluir reactivos de aplicar, ya que se trata de un constructo que no tiene una forma aparente de aplicación como si lo tienen, por ejemplo, los constructos relacionados con matemáticas; en un examen de álgebra, es mucho más probable que nos encontremos con reactivos que le pidan al sustentante realizar operaciones matemáticas para resolver un problema en específico, lo cual pertenecería al nivel de aplicación dentro de la taxonomía de Bloom. En realidad, veremos que el mismo proceso que ya realizamos hasta aquí nos ayudará a definir qué niveles de aprendizaje son más útiles para lo que pretendemos evaluar. Con lo que llevamos hasta el momento de la tabla de especificaciones, podemos alinear los dominios y subdominios y aprendizajes esperados con la taxonomía de Bloom, de tal manera que podamos redactar los aprendizajes esperados en concordancia con el nivel de habilidad cognitiva que esperamos del sustentante para ese aspecto en específico. Para un mismo domino o incluso para un mismo subdominio, podemos tener varios aprendizajes esperados en distintos niveles de la taxonomía de Bloom. Por ejemplo, para el subdominio del plan de San Luis dentro del dominio de la revolución mexicana, podemos ubicar como aprendizajes esperados que el sustentante recuerde qué era el plan de San Luis o cuáles eran sus principales puntos. A su vez, también podemos tener como aprendizaje esperado que comprenda qué implicaciones tuvo para el desarrollo de la revolución. La pregunta importante que nos permitirá delimitar qué niveles de la taxonomía esperamos para cada dominio y subdominio es: ¿qué es lo más importante que se espera que el alumno sepa de este tema? Supongamos que queremos hacer un examen para el ingreso a un posgrado en investigación, y dentro de los dominios incluimos la estadística, y dentro de los subdominios incluimos medidas de tendencia central. De este tema, ¿qué nos interesa más que el alumno sepa?, ¿nos interesa que sepa el concepto de qué es la media?, ¿nos interesa que demuestre que sabe obtener la media a través de la fórmula?, o, posiblemente ¿nos interesa que el alumno sepa interpretar el significado de esta? Quizá la respuesta sea que nos interesan los tres aspectos, pero dado que dos son procedimientos secuenciados (para interpretarla debo saber qué es), podríamos quedarnos solamente con los niveles de aplicación y análisis. Nota: Es muy importante que, aunque la taxonomía de Bloom ya nos ofrece ciertos verbos que podemos utilizar en cada nivel de aprendizaje, redactemos cada uno de la manera más concreta posible, evitando que queden más de un aprendizaje o verbo por aprendizaje esperado. En algunas tablas de especificación, se puede dedicar una columna específica para poner de manifiesto el nivel de la taxonomía de Bloom para cada aprendizaje esperado. Ya que hemos delimitado los dominios, subdominios y aprendizajes esperados, el siguiente paso es determinar cuántos reactivos requeriremos por cada aprendizaje esperado, y en general, por cada dominio y subdominio. Esta decisión dependerá de múltiples factores. En parte ya estará delimitada desde el plan general que realizamos en el primer componente, pero también, dependerá de la importancia de cada uno de los dominios, subdominios y aprendizajes esperados. Supongamos que, en el examen de estadística para el ingreso al posgrado, incluimos los subdominios de medidas de tendencia central, medidas de dispersión y comparación de medias (prueba t, ANOVA, etc.). Dado que el posgrado al que se va a ingresar es de cohorte experimental, es muy importante que los alumnos sepan utilizar pruebas para comparar las medias, y, dado que es un tema más amplio y complejo que los otros dos, es razonable decidir incluir más reactivos de este subdominio que de los otros. El peso que tendrá cada dominio, subdominio y aprendizaje esperado podrá estar dado por los requerimientos de la institución solicitante del examen, por el plan de estudios que se está tomando como base, por la revisión de la literatura que indica mayor peso de un dominio sobre otro, o por la consulta con expertos que nos indiquen cuáles son de mayor importancia. Finalmente, hay algunos otros aspectos que podemos incluir dentro de nuestra tabla de especificaciones para guiar de manera más sencilla la construcción de reactivos. Podemos incluir qué tipo de reactivo se espera para cada aprendizaje esperado, si es que utilizaremos distintos tipos de reactivos (más detalles en la siguiente sección). También es posible incluir más detalles de lo que esperamos específicamente de cada subdominio y aprendizaje esperado. Este punto será útil si recibiremos ayuda para la redacción de los reactivos. Uno de los propósitos principales de la tabla de especificaciones es dejar todo tan claro y específico que sea posible entregarla a distintos redactores de reactivos, y que ellos sean capaces de generar los reactivos correspondientes únicamente con la tabla como guía. Por ello, si hay algo más que sea de importancia para la generación de reactivos, es importante añadirlo a la tabla. Por último, podemos poner fuentes de información específicas para el diseño de cada reactivo, lo cual servirá para dar sustento teórico al instrumento, pero a la vez, servirá para que los redactores puedan consultar dichas fuentes de información para generar cada reactivo. Evidencia documental: En este componente, nuestra principal evidencia documental será la propia tabla de especificaciones que generamos. Sin embargo, también será de suma importancia que generemos un registro de las fuentes consultadas para generar el marco conceptual y el proceso de operacionalización, construyendo un texto en el que se documente todo el procedimiento de revisión teórica y la información obtenida Seguridad: La tabla de especificaciones debe resguardarse y delimitar específicamente quién tiene acceso a ella. Parte de la información aquí obtenida puede hacerse pública, tanto el marco conceptual como parte del proceso de operacionalización, pero es importante resguardar todo aquello que pueda atentar contra la validez de la prueba; es decir, si se filtra información hacia los examinados de los contenidos específicos del examen, esto puede invalidar la interpretación de que su puntaje se debe a su nivel de conocimientos, dado que en este caso estaría influido por su conocimiento de la estructura y contenidos del examen. 2.3 A modo de repaso ¿Para qué necesitamos un marco conceptual del constructo a evaluar? ¿Cuál es la utilidad que tiene generar una tabla de especificaciones? REFERENCIAS "],["COMP3.html", "COMPONENTE 3 ¿CÓMO ELABORAMOS NUESTRO EXAMEN? 3.1 Redactando nuestros reactivos 3.2 Validando nuestros reactivos 3.3 Piloteando y analizando psicométricamente nuestros reactivos 3.4 Creando nuestro banco de reactivos 3.5 Ensamblando nuestro examen 3.6 A modo de repaso", " COMPONENTE 3 ¿CÓMO ELABORAMOS NUESTRO EXAMEN? Una vez que hemos elaborado nuestra tabla de especificaciones estamos listos para comenzar la tarea de construcción de reactivos, de manera que a partir de ellos podamos realizar interpretaciones y usar los resultados del examen de manera confiable, válida y justa. Según el INEE (2019) y Lane, Raymond, and Haladyna (2016) las etapas de elaboración son 3.1 Redactando nuestros reactivos Como vimos en la sección anterior, la tabla de especificaciones será la guía que nos permita construir reactivos acordes a lo que queremos medir, lo cual se logra gracias a su desglose en área, subárea, aprendizaje esperado, nivel de evaluación, cantidad y tipo de reactivos. Nota: La elaboración de reactivos debe ser realizada por profesionales expertos en el campo de conocimiento al que pertenece el examen, dado que ellos manejan los temas y logran plasmar en las opciones de respuesta elementos que suelen ser los que los alumnos confunden regularmente con la respuesta correcta, por lo que estos expertos no debieron participar en la elaboración de la tabla de especificaciones. Es necesario brindarles como insumo la tabla de especificaciones y capacitarlos en los lineamientos técnicos que deben seguir para la construcción de los reactivos. Antes de ir directo con la construcción, debemos conocer qué es un reactivo, los elementos que lo conforman y los tipos de reactivos de opción múltiple que podemos utilizar. Un reactivo es un planteamiento que solicita la realización de una tarea, está conformado por la base donde se especifica el planteamiento preciso del problema a resolver, también tiene opciones de respuesta donde una es la respuesta correcta y los otros son opciones incorrectas o distractores. Los diferentes tipos de reactivos de opción múltiple tienen ventajas y desventajas, pero la elección de uno u otro depende principalmente de su adecuación al aprendizaje esperado y al nivel de evaluación que nos señale la tabla de especificaciones. Según Rodriguez and Albano (2017) y CENEVAL (2019) los reactivos se pueden clasificar en: 3.1.1 Reactivos de cuestionamiento directo: Su base se construye en forma de pregunta la cual solicita una respuesta directa que se encuentra entre las opciones de respuesta, también pueden ser formuladas de manera que sea una oración que deba ser completada en su estructura final, es decir, que las opciones de respuesta cierren la oración que está en la base del reactivo. 3.1.2 Reactivos de completamiento: Su base es un enunciado donde se establece la información relevante omitiendo uno o dos conceptos o palabras importantes. Las opciones de respuesta están conformadas por los conceptos o palabras más plausibles que deben completar el enunciado, pero solo una opción debe ser la correcta. A diferencia de los reactivos de cuestionamiento directo, el concepto o palabra faltante no puede ir al final o al principio de la oración, y como máximo estos reactivos deben tener tres espacios para completar. 3.1.3 Reactivos de elección de elementos o complejos: Solicitan al alumno elegir dos o más elementos según un criterio establecido en el enunciado o pregunta base del reactivo. En las opciones de respuesta se ponen combinaciones de los elementos y el alumno debe seleccionar la opción que tenga la combinación de elementos que sea la correcta. 3.1.4 Reactivos de jerarquización u ordenamiento: El enunciado base del reactivo establece un criterio con el cual solicita al alumno ordenar un conjunto de elementos, por lo que las opciones de respuesta muestran el conjunto de elementos en distinto orden y el alumno debe seleccionar la opción que los ordena de manera correcta según el criterio señalado. 3.1.5 Reactivo verdadero o falso: La base del reactivo muestra una oración o enunciado declarativo, por lo que se requiere que el alumno señale si el enunciado es verdadero o falso. 3.1.6 Reactivos de relación de columnas o matching: La base del reactivo está constituida por dos conjuntos de elementos que deben ser relacionados de acuerdo con un criterio especificado. Las opciones de respuesta mostrarán diferentes relaciones entre los elementos y los alumnos deberán elegir la que contenga la combinación correcta. 3.1.7 Conjuntos de reactivos basados en contexto o multireactivos: Esta clasificación hace referencia a la agrupación de reactivos que buscan responder a un mismo estímulo, por ejemplo, hacer uso de un cuento y en función de él desarrollar el contenido de reactivos que midan comprensión lectora, por lo que los diferentes reactivos que parten de un mismo estímulo pueden estar basados en cualquiera de las clasificaciones anteriormente explicadas, es decir, pueden tener diferentes formatos. Nota: Usar el formato conjunto de reactivos basados en contexto requiere la utilización de un escenario o estímulo, por lo que son reactivos valiosos por su utilidad para evaluar niveles cognitivos superiores como aplicación, análisis y evaluación. Su construcción requiere que se seleccione o construya un texto o escenario que sea adecuado y significativo para los alumnos de acuerdo con su nivel educativo y contexto. Según la OECD (2018) estos recursos pueden abordar situaciones personales, públicas, educativas u ocupacionales y los textos que elijamos pueden ser de tipo descriptivo, narrativo, explicativo o argumentativo, los cuales pueden tener diferentes formatos de diseño y transición dependiendo del contenido y del medio por el cual se va a aplicar el examen. Una vez que ya identificamos los tipos de reactivos o formatos que se pueden desarrollar y ya seleccionamos a los expertos que nos apoyarán en su construcción, es viable empezar con la capacitación de los expertos guiándonos de la tabla de especificaciones. Imaginemos que ya se ha seleccionado el texto del cual se basará el contenido de los reactivos y que se tiene la siguiente información en la tabla de especificaciones: En este ejemplo tenemos que desarrollar un reactivo de cuestionamiento directo con cuatro opciones de respuesta que nos permita evaluar la habilidad que el alumno tiene para definir términos o frases a partir del contenido de fuentes escritas incorporando sinónimos o antónimos para determinar significado. El reactivo debe requerir un nivel cognitivo de comprensión, por lo que el reactivo, basándose en la información del texto o estímulo seleccionado, podría quedar de la siguiente manera: Parafraseando al Dr. Gary Small, autor de El cerebro digital, «la actual eclosión de la tecnología digital no solo está cambiando nuestra forma de vivir y comunicarnos, sino que está alterando, rápida y profundamente nuestro cerebro»; En el contexto de la lectura, ¿a qué hace referencia el término eclosión utilizado por el Dr. Gary Small? Acción de salir del cascarón o salir de lo cerrado. Explosión de un fenómeno o un movimiento social o cultural. Irrupción abrupta o el crecimiento súbito de algo. Nota: Los reactivos de ejemplo de esta sección se basan en un texto argumentativo que se puede encontrar en el siguiente enlace: Setién, M. (2017). Redes sociales y adolescencia: ¿oportunidad o peligro? Para fines prácticos, solamente incluimos pequeñas viñetas de este texto para contextualizar los reactivos de ejemplo. Es esencial que al construir los reactivos los expertos justifiquen las opciones de respuesta, tanto la correcta como los distractores, lo cual brindará la oportunidad de rectificar si las opciones son adecuadas o si hay una opción que sea parcialmente correcta. Ahora bien, recordemos que lo que buscamos en la elaboración de nuestro examen es que este sea estandarizado y que los reactivos tengan parámetros psicométricos adecuados para su implementación; es debido a esto que se han establecido ciertos lineamientos técnicos que se deben seguir para conseguir su adecuada construcción. Si bien estos lineamientos varían de acuerdo a los requerimientos de cada institución y de acuerdo al formato de los reactivos, hay algunos que son generales para los exámenes de opción múltiple según Rodriguez and Albano (2017), los cuales son: Basar cada reactivo en un aspecto del contenido o una tarea cognitiva. Imagina que se plantea un reactivo de la siguiente manera: Los primeros automóviles fueron recibidos con vítores, sin embargo, a medida que fue popularizándose su uso y empezaron a aparecer los problemas, aumentaron las voces en su contra. ¿Qué argumento intenta justificar el autor con la analogía de los primeros automóviles y por qué? En este ejemplo se está solicitando en una misma pregunta dos tareas cognitivas, primero la identificación del argumento que intenta justificar García Fernández y luego la explicación del porqué de esa justificación. Lo que nos señala este lineamiento es que se debe construir el reactivo los más claro y directo posible de manera que el alumno comprenda la única tarea cognitiva que debe realizar. Utilizar material y contexto nuevos para obtener habilidades cognitivas de orden superior. Como se mencionó anteriormente, cuando se utilizan escenarios o textos se pueden construir reactivos que no solo busquen evaluar procesos cognitivos como recordar o comprender, sino que soliciten al alumno analizar, aplicar y evaluar. Por ejemplo, si en una clase de estadística, se les enseñó a los alumnos a obtener la media, mediante un ejercicio con algunos datos, dentro del examen, se les puede solicitar obtener la media de una base de datos más grande y con datos diferentes. Mantener el contenido de los reactivos independientes entre sí. Se debe evitar que la respuesta de un alumno a un reactivo influya en su respuesta a otro, es decir, debemos buscar que cada reactivo nos brinde información sobre los conocimientos, habilidades y aptitudes de los alumnos de manera independiente, de lo contrario si el alumno contesta mal uno de los reactivos por ende el otro también será incorrecto. Probar el contenido importante. Evitar el contenido demasiado específico y general. Imagina que se plantea la base del reactivo de la siguiente manera: ¿En qué año se publicó el artículo donde el autor hace la analogía de los primeros automóviles? Este reactivo está solicitando información muy específica, en este caso busca evaluar la identificación de un elemento del artículo, por lo que hay que preguntarse si el reactivo ofrece información relevante sobre los conocimientos, habilidades y aptitudes de los alumnos o si se puede transformar en un reactivo que mida elementos clave del aprendizaje. La misma lógica aplicaría si se elaboran reactivos que solicitan información muy general. Evitar opiniones y trucos. Imagina que se plantea la base del reactivo de la siguiente manera: ¿Cuál consideras que es el propósito principal del autor del artículo con respecto al uso de redes sociales? Se debe evitar redactar el reactivo de manera que se preste a solicitar una opinión por parte del alumno, lo cual causaría que no haya solo una respuesta correcta pues esta depende de lo que crea u opine el alumno. En este caso lo mejor sería preguntar de manera directa ¿cuál es el propósito? Organizar cada reactivo verticalmente en lugar de horizontalmente. Con respecto al formato de los reactivos y sus opciones de respuesta, lo ideal es que estén organizados de manera vertical, lo cual facilita la lectura de las opciones de respuesta y evita que los alumnos se confundan en la posición de la opción que van a seleccionar, debido a que de manera horizontal las opciones pueden estar demasiado juntas. Editar y probar los reactivos Este lineamiento es importante sobre todo en exámenes donde se usan formulas o notaciones que, si están mal colocadas, pueden ocasionar que los alumnos respondan de manera incorrecta, aunque si tengan la habilidad que se requiere. Lo ideal es que los reactivos sean revisados por un asistente o corrector que verifique la correcta colocación de las notaciones y fórmulas. Mantener la complejidad del lenguaje de los reactivos en un nivel apropiado para la clase que se está probando Imagina que se plantea el reactivo de la siguiente manera: Según el autor del texto los adolescentes que suelen descollar en el uso de las redes sociales presentan más problemas de Este reactivo ejemplifica el cuidado que se debe tener en el uso de palabras que no vayan acorde al nivel educativo de los alumnos a los que va dirigida la prueba. En este caso puede ser que la palabra descollar no sea comprendida por todos los alumnos, por lo que sería adecuado cambiarla por una palabra más sencilla como exceder, de lo contrario no solo se está midiendo la comprensión lectora de los alumnos sino también el conocimiento de vocabulario, es decir, se está agregando otro constructo que va a intervenir en la respuesta. Los alumnos que no comprendan la palabra van a dar una respuesta incorrecta que no se debe a su poca habilidad de comprensión lectora sino a que no conocen el vocabulario, lo cual finalmente afecta las interpretaciones que podamos hacer de los resultados de la prueba. Minimizar la cantidad de lectura en cada reactivo. Evitar los adornos. Se debe evitar presentar escenarios, textos o enunciados de los reactivos demasiado cargados de información, debido a que se sobrecarga cognitivamente al alumno y por ende lo que conteste no tendrá que ver son su habilidad de comprensión lectora, sino con el cansancio. También el uso de adornos, en vez de apoyar la información del reactivo, lo hace más complejo de manera innecesaria, a menos que nuestro examen tenga como objetivo evaluar un curso de lectura. Otro aspecto que permite construir un reactivo que se lea de forma fluida y precisa, es mover a la base o enunciado las palabras que se repiten en cada opción de respuesta. Por ejemplo: En el contexto del artículo, cuando García menciona el concepto defenestrar se refiere a atacar a alabar a justificar En este caso para hacer más fluido el reactivo se puede mover el artículo a, que se encuentra en cada opción de respuesta, a la base del reactivo y así solo dejamos los verbos en las opciones. Enunciar la idea principal en la base de forma clara y concisa y no en las opciones. Se tiene que tener siempre presente que un buen reactivo es aquel que desde que lees el enunciado puedes pensar en la respuesta correcta, sin ver las opciones de respuesta. Por lo que se debe procurar que la idea principal se encuentre en la base y no en las opciones. Redactar la base positivamente; evite las frases negativas. El problema en el uso de frases negativas es que los alumnos tienden a pasar por alto el sentido negativo de una oración y que muchas veces estos enunciados pueden caer en la doble negación, lo cual ocasionaría que los alumnos entiendan de manera incorrecta el enunciado; por lo que se recomienda evitar su uso cuando se elabore un reactivo, pero en caso de que se tengan que utilizar, lo adecuado es poner la palabra No en negritas y subrayada. Escribir tantas opciones como sean necesarias, dado el tema y la tarea cognitiva; tres opciones suelen ser suficientes. Es importante que se comprenda el papel de los distractores, son una parte vital en el buen funcionamiento de los reactivos, por lo que su elección suele estar basada en las principales confusiones o errores típicos que tienen los alumnos en el tema que se aborda, por ejemplo, si el reactivo pregunta algún aspecto sobre la revolución mexicana y el docente ha observado que sus alumnos suelen confundir los contenidos del Plan de San Luis con los del Plan de Agua Prieta, puede usarla como opción distractora. La elección de la cantidad de opciones de respuesta también es importante, se ha encontrado que tres opciones son suficientes, pero si los reactivos requieren un equilibrio entre las opciones también pueden ser cuatro, es decir, el número de opciones depende principalmente de la naturaleza del reactivo y de que los expertos logren identificar los distractores más plausibles o atractivos que permitan identificar a los alumnos que saben y los que no. Una vez que hemos determinado la cantidad de opciones de respuesta que usaremos, es importante ser consistentes dentro del examen, y que todos los reactivos tengan el mismo número de opciones de respuesta. Asegurarse de que solo una opción sea la respuesta correcta. A menos que el formato del reactivo sea de elección de dos o más elementos, es necesario asegurarse de que el reactivo solo tenga una respuesta correcta, por lo que escribir la justificación, tanto de la opción correcta como de los distractores, es útil para percatarse cuando se debe cambiar un distractor debido a que puede ser parcialmente correcto. Colocar las opciones en orden lógico o numérico. Ordenar las opciones de respuesta siguiendo una secuencia lógica, evita que los alumnos traten de encontrar un patrón de respuesta que se deba a la ubicación de la respuesta correcta en todos los reactivos y en su lugar fomenta que los alumnos se enfoquen en el contendido. Además, hace más fluida la lectura, por lo que reduce la carga cognitiva. Variar la ubicación de la respuesta correcta de manera uniforme entre las opciones. Se debe evitar que la opción correcta se ubique en el mismo lugar en la mayoría de los reactivos, todas las opciones de respuesta deben tener la misma probabilidad de tener la respuesta correcta a lo largo de todo el examen. Mantener las opciones independientes; las opciones no deben superponerse. Si dos opciones de respuesta se superponen, ambas opciones sean correctas, por lo que deben ser independientes una a de otra. Evitar usar las opciones ninguna de las anteriores, todas las anteriores y no lo sé. Imagina que se platea el reactivo de la siguiente manera: Las recomendaciones que hace Bilbao van dirigidas a que los adolescentes generen mayor autocontrol y tengan más espacios distintos al entorno virtual. se desintoxiquen de las redes sociales para que convivan con sus familias. ejerzan mayor autocontrol en distintos espacios y momentos del día. todas las anteriores Si se incluye la opción todas las anteriores se está dando pistas sobre la respuesta correcta al alumno, es decir, si el alumno descarta la opción B entonces sabe que la D tampoco es la correcta; otra opción es que si el alumno se da cuenta de que la A y C son correctas puede escoger la opción D, lo cual evita que el reactivo sea discriminativo, pues no ayuda a diferenciar cuales son los alumnos que tienen un adecuado nivel de comprensión lectora, debido a que los alumnos con un conocimiento parcial pueden responder el reactivo correctamente. Por lo que hay que evitar hacer uso de este tipo de opciones de respuesta. Expresar las opciones de manera positiva; evitar palabras negativas como no. Como en el caso del enunciando o la base del reactivo, se debe evitar que las opciones de respuesta estén redactadas de manera negativa, dado que pueden confundir a los alumnos. Evitar dar pistas sobre la respuesta correcta. Algunas de las recomendaciones que se deben de seguir son: Las opciones de respuesta deben tener una longitud similar dado que se ha comprobado que la respuesta más larga tiende a ser la correcta Evitar incluir palabras como siempre nunca completamente porque los alumnos tienden a descartarlas rápidamente debido a que son absolutos que raramente se cumplen Evitar que los alumnos hagan asociaciones entre la base del reactivo y la repuesta correcta debido a que contienen la misma palabra Evitar hacer uso de opciones ridículas o absurdas que los alumnos descarten de inmediato Mantener las opciones similares en su contenido semántico y en su estructura Nota: Para más ejemplos de cada uno de estos lineamientos generales, consultar el capítulo 4  Writing Multiple-Choise Items de Rodriguez and Albano (2017) referido en la bibliografía de este manual. Como podemos ver, la redacción de un reactivo no es tarea sencilla, hay muchos aspectos que se deben cuidar para que no intervengan y afecten el constructo que queremos medir, de otra manera las interpretaciones que hagamos de los resultados y usos de la prueba no serán válidos y podrían traer consecuencias negativas para los sustentantes, por lo que, una vez capacitados los expertos a partir de los lineamientos y que se hayan redactado los reactivos, estos deben pasar por una revisión profunda antes de ser aplicados. 3.2 Validando nuestros reactivos Una vez redactados los reactivos, es necesario que sean revisados para constatar que estén alineados de manera adecuada con la tabla de especificaciones y que no tengan sesgos que puedan poner en riesgo su validez, por ejemplo, que no contengan palabras o términos ofensivos o que pongan en desventaja a una subpoblación por sus características personales como género, religión, idioma, etc. Este es un aspecto fundamental que la AERA, APA, and NCME (2018) llama imparcialidad, aspecto que de no cuidarse puede ocasionar varianza irrelevante al constructo, es decir, que intervengan variables que no nos permitan evaluar de manera precisa el constructo que queremos medir. Nota. Las revisiones deben llevarlas a cabo diferentes grupos de expertos, los cuales no participaron en la construcción de los reactivos, todo esto con el fin de que la revisión sea lo más objetiva posible. Debe haber expertos para: Verificar que los reactivos estén alineados a la tabla de especificaciones Revisar que se cumplan los lineamientos técnicos Hacer una revisión de estilo o editorial Verificar que los reactivos no estén sesgados hacia algún subgrupo de la población objetivo Es necesario que las revisiones de los reactivos no las realicen los mismos grupos de expertos. Primero debe hacerse una revisión técnica de los reactivos para asegurar que estos cumplen con los lineamientos técnicos correspondientes, también debe haber una revisión de estilo o editorial donde se revisen en términos de su redacción y ortografía. Otro grupo de expertos debe validar los reactivos en función de su alineación con la tabla de especificaciones, que no presenten más de una respuesta correcta y que no intervengan otras variables ajenas al constructo que se quiera medir, todo esto con la finalidad de asegurarnos que el examen realmente mide lo que queremos medir de manera justa con los sustentantes y que no los ponga en desventaja por sus características personales como sexo, religión, etnia, raza, o nivel socioeconómico, por ejemplo, si un reactivo hace uso de una palabra con la que están más familiarizados los alumnos de un nivel socioeconómico alto, se pone en desventaja a los alumnos de nivel bajo, lo cual no les permite responder como lo hubieran hecho si se usara una palabra que es entendible para todos. Otro ejemplo para entender la importancia de las revisiones es que si un reactivo usa un término que es ofensivo para un subgrupo, afectará directamente en la respuesta de los alumnos, por lo que impide medir de manera justa el constructo de interés. De acuerdo con las revisiones se realizan modificaciones y/o ajustes a los reactivos. Evidencia documental: Reactivos validados por los expertos y bitácoras que señalen el proceso de validación de los reactivos por parte de cada grupo de expertos y las modificaciones a los reactivos. 3.3 Piloteando y analizando psicométricamente nuestros reactivos Una vez que se hicieron correcciones a los reactivos de la prueba, lo ideal es que se realice una aplicación inicial a una muestra representativa de la población objetivo, con el fin de estimar sus parámetros y conocer si se comportan de manera adecuada, es decir, si son útiles para medir el constructo y cumplir con el propósito de la evaluación; además este paso nos ayuda a poner en práctica el procedimiento de administración del examen, lo cual nos permitirá mejorar la práctica de administración cuando llevemos a cabo la aplicación final del examen; recordemos que el proceso de aplicación es vital para la confiabilidad de la medición, por lo que debemos asegurarnos de llevar a cabo una aplicación con procedimientos estandarizados que permitan evitar la intrusión de errores de medición en nuestra evaluación. Para ver de manera detallada el procedimiento de aplicación de un examen ir a al (Componente 4) del presente manual. El piloteo nos permitirá determinar con cuáles reactivos ensamblaremos en el examen para su aplicación final, por lo que de ninguna manera los resultados obtenidos en este paso deben ser usados para asignar puntajes finales a los alumnos. En caso de que no se pueda realizar el pilotero con una muestra representativa de alumnos, podemos hacer uso de técnicas cualitativas que retroalimentarán el funcionamiento de los reactivos, como entrevistas cognitivas, paneles de expertos o grupos focales, las cuales son técnicas que nos permiten obtener información de los alumnos o expertos con la que podemos modificar y mejorar los reactivos, pero no nos permiten obtener sus parámetros ni analizar de manera estadística el funcionamiento del examen como un todo. Si el lector quiere profundizar en las técnicas cualitativas se recomienda consultar el artículo de Urrutia Egaña et al. (2014) referido en la bibliografía del presente manual. Nota: Si bien este es un paso importante debido a que nos permite determinar los errores que podemos cometer en la administración del examen y obtener los parámetros psicométricos de los reactivos, en la práctica real muchas veces es complicado el piloteo de los reactivos en una muestra representativa de la población objetivo debido al tiempo y recursos que se necesitan, por lo que en este manual partimos de la idea de que se pueden aplicar los reactivos de manera directa a la población objetivo una vez elaborados y validados por los expertos, saltándonos el paso del pilotaje y los análisis psicométricos en esta etapa del desarrollo de nuestro examen. Por lo que en lugar de pilotearlos podemos realizar la aplicación final del examen y una vez que los alumnos lo hayan resuelto podemos llevar a cabo los análisis estadísticos de los reactivos para determinar cuáles son los que nos servirán para calificar a los alumnos y cuáles no, así con los reactivos que hayan obtenido adecuados parámetros psicométricos de acuerdo con nuestro modelo de medición, podemos obtener la calificación final de los alumnos e interpretar los resultados. Pero es necesario aclarar que, si se tiene el tiempo y los recursos, lo adecuado sería realizar una aplicación piloto y obtener los parámetros de los reactivos antes de su aplicación final. En el caso de no llevar a cabo el piloteo y por ende la primera estimación de los parámetros de los reactivos, podemos seguir leyendo el manual en el orden que venimos siguiendo, pero en caso de que si podamos realizar el piloteo de los reactivos debemos pasar al Componente 5 ¿Qué análisis podemos hacer de los resultados?, el cual nos explicará de manera detallada como realizar los análisis estadísticos de los reactivos y del instrumento como un todo. Al terminar de leer dicha sección debemos regresar a este apartado para seguir leyendo de manera continua. Evidencia documental: En caso de realizar el piloteo se debe obtener como evidencia un documento que señale el proceso de selección de la muestra, la estrategia que se utilizó, cuántas versiones del examen se aplicaron y con cuantos reactivos; de igual manera se debe especificar si se utilizaron métodos cualitativos. Además, se debe obtener evidencia de los resultados obtenidos en el piloteo, desde la administración el instrumento, hasta los parámetros, especificando cuales reactivos funcionan de manera adecuada y cuáles no. 3.4 Creando nuestro banco de reactivos La obtención de los parámetros psicométricos de los reactivos, por medio de la teoría de respuesta al ítem, ya sea mediante el piloteo o en la aplicación final del examen, es muy útil para los evaluadores dado que facilita la construcción de un banco de ítems que requiere el uso de una plataforma o programa especial que permita almacenar los reactivos que ya están calibrados y agregar la respectiva información de cada reactivo como su contenido, identificador y sus características psicométricas. De esta manera se introduce flexibilidad a la evaluación debido a que, si se requieren evaluaciones frecuentes o varias versiones de una prueba, nos facilita la selección de los reactivos que ya tienen parámetros estadísticos adecuados, es decir, el ensamblaje del examen, y aparte funciona como guía, debido a que nos permitirá realizar un diagnóstico de necesidades identificando cuando necesitemos elaborar más reactivos sobre un aprendizaje esperado debido a que los que conforman el banco han ido modificando sus parámetros estadísticos, de manera no adecuada, en diferentes aplicaciones. 3.5 Ensamblando nuestro examen Como el propósito del presente manual es guiar a las comisiones en la construcción de exámenes de alto impacto, los reactivos que deberemos seleccionar para el ensamblaje son lo que nos proporcionen información adecuada sobre los examinados a lo largo de toda la escala de habilidad, es decir, debemos conformar el examen con reactivos con niveles de dificultad fácil, media y difícil de manera equilibrada, pero, es importante precisar que si el propósito de nuestro examen está enfocado en la identificación de los estudiantes con alto desempeño, ya sea para darles una beca o para determinar ciertos lugares en una clase de alto nivel, vamos a requerir mayor cantidad de reactivos con un nivel de dificultad alto, es decir, la elección de los reactivos a incluir en el examen depende principalmente del propósito de la evaluación. Ahora bien, en el caso de que no se haya llevado a cabo el pilotaje de los reactivos con una muestra representativa de la población, el ensamblaje del examen deberá basarse en la conformación que determina la tabla de especificaciones. Por su parte, si realizamos el piloteo y obtuvimos los parámetros estadísticos de los reactivos debemos basarnos tanto en la tabla de especificaciones como en los parámetros obtenidos para decidir cuáles reactivos serán parte del examen y en qué orden se presentarán, es decir, a partir del modelo de medición que especificamos desde el primer componente de este manual, debemos decidir qué reactivos cumplen los parámetros y cuáles no, esto es importante porque en el caso de que queramos ensamblar dos formas equivalentes del examen para obtener una versión A y B, debemos asegurarnos que ambas versiones contengan lo que se conocen como reactivos ancla, es decir, que entre el 30 y 50% de los reactivos sean los mismos en ambas versiones y que estén ubicados en el mismo lugar en ambos exámenes, lo cual permitirá equipararlos al momento de obtener la calificación final de los alumnos, mientras que el resto de los reactivos de las versiones deben tener parámetros estadísticos similares, es decir, si en una versión la primera pregunta tiene una dificultad baja con cierta media y desviación estándar, en la otra versión del examen el primer reactivo deberá ser equivalente en función de sus parámetros. Todo este proceso de ensamblaje requiere la elaboración de una tabla donde especifiquemos cómo fue el ensamblaje de la prueba o de las versiones de la prueba, en términos de cada uno de los reactivos que lo conforman, su pertenencia a la tabla de especificaciones, versión del examen en el que aparece y cuáles son sus parámetros estadísticos. Evidencia documental: Presentar un documento que especifique los criterios cuantitativos y cualitativos que se eligieron para integrar el examen o las diferentes versiones de este; también se debe presentar como evidencia la tabla donde especificamos los aspectos técnicos del ensamblaje del examen. Seguridad: En todo este componente llamado elaboración del examen debemos tener mucho cuidado con la seguridad de los productos resultantes en cada paso. En el caso de la redacción de los reactivos tenemos que procurar tenerlos bajo resguardo en todo momento. Con respecto a su validación, debemos asegurarnos de que el acceso a ellos se limite únicamente a los expertos que nos apoyarán en su revisión y que inmediatamente después volverán a ser resguardados. Finalmente, los exámenes ensamblados deben ser protegidos para evitar su acceso, de lo contrario todo el proceso de medición que se está llevando a cabo se vería afectado, lo cual haría inválida y poco confiable la evaluación. Estándar 3.2: Los desarrolladores de la prueba son responsables de desarrollar pruebas que midan el constructo previsto y de minimizar el potencial de que las pruebas se vean afectadas por características irrelevantes del constructo, como características lingüísticas, comunicativas, cognitivas, culturales, físicas y otras (AERA, APA, and NCME 2018, 71) Estándar 4.7 Los procedimientos utilizados para desarrollar, revisar y probar ítems y para seleccionar ítems del conjunto de ítems deben documentarse (p.99). Estándar 4.9 Cuando se realizan ensayos de ítems o formularios de prueba, deben documentarse los procedimientos utilizados para seleccionar la(s) muestra(s) de examinandos, así como las características resultantes de la(s) muestra(s). Las muestras deben ser tan representativas como sea posible de las poblaciones para las que está prevista la prueba (p.100). 3.6 A modo de repaso ¿Cuál es la importancia de utilizar diferentes grupos de expertos en la revisión de los reactivos? ¿Por qué es ideal realizar un piloteo de los reactivos? REFERENCIAS "],["COMP4.html", "COMPONENTE 4 ¿CÓMO ADMINISTRAMOS EL EXAMEN? 4.1 ¿Cómo producimos e imprimimos el examen? 4.2 ¿Cómo aplicamos el examen? 4.3 A modo de repaso", " COMPONENTE 4 ¿CÓMO ADMINISTRAMOS EL EXAMEN? En este componente se consideran dos aspectos fundamentales, la producción de la prueba y su aplicación. Podemos considerar que si todo el desarrollo de la prueba desde el primer componente hasta ahora se ha realizado de manera adecuada (de acuerdo con las especificaciones de este manual), esta fase de administración del instrumento es más automatizada y sencilla, pero la realidad es que esta es una fase que si no se lleva a cabo con rigor puede echar abajo todo el trabajo realizado, haciendo que las interpretaciones de los resultados de la prueba y sus usos sean poco precisos e inválidos, como se verá a continuación. Los pasos que conforman este componente son los siguientes: 4.1 ¿Cómo producimos e imprimimos el examen? De manera general debemos asegurarnos de que nuestros instrumentos, ya sean en papel o en electrónico, deben estar libres de errores, debemos hacer una visualización previa del instrumento antes de su impresión o de su carga en el programa de cómputo, checar que los reactivos no tengan errores en su organización, ortografía y redacción y que las imágenes, gráficas o tablas se vean de manera nítida. En el caso de que sea una prueba en computadora debemos asegurarnos de que una vez cargados los reactivos no haya errores en el navegador o en las características del funcionamiento del programa. Este aspecto es muy delicado dado que si algo sale mal en la producción del instrumento puede invalidar las inferencias que se realicen de los resultados de la prueba, es decir, puede afectar el desempeño de los sustentantes. Por ejemplo, imaginemos que a un grupo de sustentantes le toca responder un examen donde tienen que interpretar un texto con la ayuda de una imagen, pero la imagen no se ve adecuadamente, por lo que nosotros como evaluadores no podríamos asegurar que la respuesta dada a ese reactivo está relacionada con el desempeño verdadero del sustentante, si así lo hiciéramos estaríamos haciendo inferencias equivocadas sobre los resultados, lo cual haría inválidas nuestras interpretaciones y usos de la prueba. Evidencia documental: Evidencia de las pruebas de visualización o impresión y el protocolo de seguridad de los instrumentos. 4.2 ¿Cómo aplicamos el examen? Una vez teniendo los instrumentos impresos o cargados en el programa de cómputo de manera adecuada y segura, debemos aplicar el examen. Este paso es crucial dado que necesitamos aplicar los instrumentos en condiciones estandarizadas que impidan el paso a variables extrañas que puedan afectar el desempeño de los sustentantes, ejemplo de lo que debemos delimitar son los tiempos de aplicación, las instrucciones, reglas de convivencia de los sustentantes, ambiente etc. Por ejemplo, imaginemos que un aplicador, en lugar de dar a un grupo dos horas para resolver el examen como está establecido, da 1 hora con 30 minutos ¿consideraríamos que los resultados del examen reflejarán el desempeño que hubieran tenido los sustentantes si hubieran respondido el examen en 2 horas como los demás grupos? No, definitivamente eso afectaría la validez de las interpretaciones que se hagan de los resultados y atentaría contra la equidad en la evaluación, por lo que es necesario que elaboremos un protocolo de aplicación que explique paso por paso la manera de aplicar los instrumentos, lo cual permitirá que el aplicador o aplicadores del examen lleven a cabo un mismo procedimiento que ponga a los sustentantes en igualdad de condiciones y no dé paso a otras variables que puedan afectar el desempeño de los sustentantes. Es necesario que establezcamos un plan de logística para la aplicación del instrumento que asegure las condiciones adecuadas antes, durante y después de la aplicación de la prueba. De igual manera se recomienda que llevemos un seguimiento del proceso de aplicación por medio de una bitácora donde podamos identificar las inconsistencias o las situaciones inesperadas que surjan antes, durante y después de la aplicación, por ejemplo, si se fue la luz, si los sustentantes tuvieron que salir del aula por motivos de seguridad, si el aplicador tuvo que dejar un momento el grupo, etc. con el fin de determinar si dichas situaciones impactarán en la calificación de los estudiantes y qué modificaciones son necesarias de hacer. Del mismo modo debemos determinar las situaciones donde invalidaremos los exámenes. Evidencia documental: Protocolo de aplicación del instrumento, bitácoras de la aplicación del instrumento y el protocolo de seguridad de los instrumentos. Como podemos ver tanto la producción como la aplicación son aspectos fundamentales en la evaluación que debemos de cuidar de la manera más rigurosa posible. Seguridad: En ambos aspectos es requisito ineludible el resguardo de los instrumentos, por lo que es necesaria la elaboración de un protocolo que señale la acciones a realizar y las personas que tendrán autorización para tener acceso a los instrumentos en este componente. Estándar 4.15: Las instrucciones para la administración de la prueba deben presentarse con suficiente claridad para que sea posible que otros repliquen las condiciones de administración en las que se obtuvieron los datos sobre confiabilidad, validez y (cuando corresponda) normas. Las variaciones admisibles en los procedimientos de administración deben describirse claramente. El proceso para revisar solicitudes de variaciones adicionales en la evaluación también debe documentarse. (AERA, APA, and NCME 2018) Estándar 4.16: Las instrucciones presentadas a los examinandos deben contener suficiente detalle para que los examinandos puedan responder a una tarea de la manera prevista por el desarrollador de la prueba. Cuando corresponda, deben proporcionarse los materiales de la muestra, preguntas prácticas o de la muestra, criterios para calificación y un ítem representativo identificado con cada formato de ítem o área importante en la clasificación o dominio de la prueba a los examinandos antes de la administración de la prueba, o deben incluirse en el material de evaluación como parte de las instrucciones de administración estándar. Estándar 6.6: Se deben tomar las medidas razonables para garantizar la integridad de los puntajes de las pruebas, eliminando las oportunidades para que los examinandos logren puntajes mediante medios engañosos o fraudulentos. (AERA, APA, and NCME 2018) Estándar 10.9 Cuando se decide sobre el uso de administración de pruebas basada en tecnología, los profesionales deben tener en cuenta el propósito de la evaluación, el constructo que se mide y las capacidades del examinando. (AERA, APA, and NCME 2018) 4.3 A modo de repaso ¿Por qué es necesario establecer condiciones de estandarización durante la aplicación de la prueba? REFERENCIAS "],["COMP5.html", "COMPONENTE 5 ¿QUÉ ANÁLISIS PODEMOS HACER DE LOS RESULTADOS? 5.1 Desempeño de los reactivos 5.2 Dimensionalidad 5.3 Confiabilidad 5.4 Imparcialidad 5.5 Evidencias de validez 5.6 Calificación 5.7 A modo de repaso", " COMPONENTE 5 ¿QUÉ ANÁLISIS PODEMOS HACER DE LOS RESULTADOS? Una vez que hemos terminado de aplicar el examen y capturado la información (ya sea en el pilotaje o en la aplicación final), es importante que analicemos si los reactivos que redactamos realmente se comportaron como esperábamos que lo hicieran y si los resultados generales verdaderamente pueden interpretarse y utilizarse de las maneras que teníamos previstas. Para ello, hay seis grandes áreas de análisis que nos permitirán determinarlo: Nota: Mucha de la información aquí presentada se basa en las recomendaciones de los libros de Chadha (2009) y de Lane, Raymond, and Haladyna (2016); si al lector le interesa profundizar en alguno de estos puntos, recomendamos estas dos fuentes como principales puntos de referencia Desempeño de reactivos: Nos dirá si realmente los reactivos funcionaron en cuanto a dificultad y discriminación, y si los distractores funcionaron como se esperaba que lo hicieran. Dimensionalidad: Nos mostrará si todos los reactivos de un dominio o subdominio específico realmente están midiendo lo mismo, o si, por el contrario, algún reactivo que esperábamos midiera un dominio, en realidad se agrupa mejor con reactivos que miden otra cosa. Esta es una importante evidencia de validez. Confiabilidad: Nos dirá si el examen es consistente a través del tiempo o de las versiones diferentes del mismo. Imparcialidad Nos permitirá determinar si nuestros reactivos benefician a algún grupo en específico, independientemente de su nivel de habilidad. Evidencias de validez: En esta sección se agrupan varios análisis que nos permitirán identificar si el examen realmente mide lo que pretende medir y si en verdad se puede interpretar de la manera que pretendemos hacerlo. Calificación: Nos ayudará a determinar cómo establecer puntos de corte para decidir quién aprueba y quién no, dependiendo de los usos e interpretaciones previstos Estándar 5.0: Los puntajes de la prueba deben derivarse de una manera que respalde las interpretaciones de los puntajes de la prueba para los usos propuestos de las pruebas. Los desarrolladores y usuarios de la prueba deben documentar evidencia de imparcialidad, confiabilidad y validez de los puntajes de la prueba para su uso propuesto. (AERA, APA, and NCME 2018) 5.1 Desempeño de los reactivos 5.1.1 Teoría Clásica de los Tests Para realizar el análisis psicométrico de los reactivos, desde la Teoría Clásica de los Tests (TCT), son tres los puntos principales que necesitamos evaluar: la dificultad, la discriminación y la correlación con el total del instrumento. 5.1.1.1 Índice de dificultad Cuando hablamos de dificultad del reactivo en la TCT, nos referimos a la proporción de alumnos que contestó correctamente el reactivo con respecto al total de examinados. Supongamos que, en un examen de matemáticas, incluimos dos reactivos de cálculo integral; uno de ellos, es contestado correctamente por el 80% de los estudiantes que tomaron el examen, mientras que el otro, solamente fue contestado correctamente por el 35% de los participantes. Con solo esta información, es fácil asumir que el primer reactivo es mucho más sencillo que el segundo, dado que una gran proporción de los examinados lo contestó correctamente. Puede parecer un poco contraintuitivo, pero al ser la proporción de respuestas correctas, un número alto en el índice de dificultad, nos indica que el reactivo es sencillo, mientras que un número pequeño nos indica que el reactivo es difícil. El índice de dificultad, en lugar de presentarse en porcentaje como el ejemplo, suele presentarse como un valor decimal, dado que se calcula dividiendo el número de personas que contestaron correctamente el reactivo entre el número total de participantes. En el primer reactivo del ejemplo, su índice de dificultad sería de .8, y el segundo, de .35. Idealmente, lo que buscamos en nuestros reactivos es que no sean tan sencillos como para que todos los sustentantes los contesten correctamente, pero que, a la vez, no sean tan complejos que ninguno pueda hacerlo. Aunque bajo esa lógica parecería recomendable buscar que todos los ítems tengan un índice de dificultad de .5, la realidad es que lo adecuado es balancear el examen para tener reactivos relativamente fáciles y reactivos relativamente difíciles. Algunos autores suelen utilizar como parámetros un umbral de .2 a .8; es decir, buscan mantener únicamente los reactivos cuyo índice de dificultad sea mayor a .2, pero que sea menor a .8. Sin embargo, no podemos tomar este criterio como una regla de decisión que aplica en todos los casos. Si tenemos un reactivo que salga del límite o se encuentre muy cerca de este, es importante que tomemos en consideración el resto de los indicadores aquí descritos para decidir si lo eliminamos o no. 5.1.1.2 Proporción de elección de opciones de respuesta Una vez que ya revisamos la proporción de sustentantes que contestó correctamente el reactivo, también es importante que desglosemos las proporciones correspondientes a quienes lo contestaron de manera incorrecta. Dado que los reactivos suelen tener cuatro opciones de respuesta, siendo una correcta y las otras tres distractoras, es importante identificar qué proporción de sustentantes contestó cada uno de los distractores. Idealmente, esperaríamos que, por ejemplo, si el 70% de los sustentantes contestó correctamente el reactivo, el 30% restante se distribuya en aproximadamente 10% por cada distractor. Aunque esta proporción equitativa no siempre se logra, sí es importante que revisemos este dato, ya que, por ejemplo, si observamos que ese 30% de respuestas incorrectas, se reparte en 15% para un distractor, 15% para otro y 0% para el último, esto nos está indicando que el último distractor no es seleccionado por nadie, y por tanto no cumple su función como distractor, quizá porque es muy obvio para los sustentantes que esa no es la respuesta correcta, o quizá por alguna cuestión de formato o redacción. Esto se vuelve más importante porque quizá este distractor que no funciona puede estar afectando el índice de dificultad, y corregirlo, podría hacer que el reactivo se desempeñe mejor. Es por ello que es importante detectar este tipo de inconsistencias para poder decidir si, en lugar de eliminar el reactivo, lo enviamos a revisión para quizá cambiar esa opción de respuesta por una más distractora. 5.1.1.3 Índice de discriminación En el contexto de análisis de reactivos, hablamos de discriminación para referirnos a la capacidad que tiene el reactivo para distinguir correctamente el grupo de sustentantes con desempeño alto (sustentantes con el mayor puntaje total en el examen) con respecto a los sustentantes de bajo desempeño (sustentantes con el puntaje global en el examen más bajo). En otras palabras, un reactivo discrimina bien, cuando son más los sustentantes con alto desempeño los que lo contestan bien, y más los que tienen bajo desempeño quienes lo contestan de manera incorrecta. Si, por el contrario, tenemos un reactivo que solo el 40% de los sustentantes contestó correctamente, pero de ese 40%, la mayoría fueron sustentantes con un desempeño bajo, el reactivo no está funcionando de manera adecuada. También se puede dar el caso donde lo respondan de manera correcta sustentantes de ambos grupos por igual, por lo que el reactivo realmente no aporta información importante. El índice de discriminación se obtiene al restar el número de sustentantes del grupo bajo que respondieron correctamente, al número de sustentantes del grupo alto que respondieron correctamente, lo cual luego se divide entre el número total de participantes. Si obtenemos un valor negativo de esta operación, significa que fue mayor la proporción de sustentantes en el grupo bajo que contestó correctamente el reactivo. Si obtenemos 0 o valores muy cercanos a 0, significa que la proporción de ambos grupos es prácticamente la misma. Con esto en cuenta, el criterio que se suele utilizar es conservar reactivos con un índice de discriminación mayor a .2, o en casos más estrictos, mayores a .3. Nuevamente, no podemos tomar solo este índice como criterio para eliminar el reactivo, sino que se debe considerar en conjunto con los demás. 5.1.1.4 Correlación entre el reactivo y el total de la prueba Otro punto importante es qué tanto el reactivo contribuye al puntaje total de la prueba. Para determinar esto, utilizamos un método estadístico llamado correlación biserial puntal. En esencia, una correlación nos dice qué tanto dos valores están relacionados y de qué manera; en este caso, el valor de la correlación nos dirá si el contestar de manera correcta un reactivo está relacionado con obtener puntajes altos y si contestarlo de manera incorrecta se relaciona con la obtención de puntajes bajos. En otras palabras, nos dirá si contestar de manera correcta o incorrecta cierto reactivo nos ayuda a determinar el nivel de habilidad de la persona (entendido como el puntaje total de la prueba). La correlación se obtiene de comparar el valor (correcto o incorrecto) que obtuvo cada sustentante en determinado reactivo con su puntaje total en la prueba. El valor que se obtiene estará dentro de un rango de -1 a 1. En este rango, valores cercanos a -1 indican una correlación negativa, es decir, contestar de manera correcta se relaciona más con obtener puntajes bajos, mientras que valores más cercanos a 1 indican una correlación positiva, siendo que contestar correctamente se relaciona con obtener puntajes altos. Por otro lado, valores cercanos a 0, ya sean positivos o negativos, nos indican que no hay una relación entre contestar de manera correcta o incorrecta el reactivo y el puntaje total de la prueba. Lo que esperamos con los reactivos que diseñamos es que todos los reactivos contribuyan a determinar el nivel de habilidad de los sustentantes, por ende, lo que esperamos son correlaciones de carácter positivo. No necesitamos correlaciones que se encuentren muy cercanas a 1, de hecho eso es poco probable. Sin embargo, solemos usar como criterio el conservar reactivos con correlaciones mayores a .2. Nuevamente, este dato solo se puede usar como criterio cuando se toma en cuenta con el resto de información obtenida con respecto al reactivo. 5.1.1.5 Correlación de los distractores La correlación que hicimos hace un momento se hace al tomar el valor de contestar correctamente como 1, y contestar incorrectamente como 0. Bajo esa premisa, podemos hacer la misma correlación para cada una de las opciones de respuesta, es decir, si un sustentante eligió uno de los distractores, lo tomamos como un 1, y si no lo eligió, como un 0. Esto nos permite realizar la correlación con el puntaje total también con los distractores. La ventaja que tiene este análisis es que nos permite detectar si los distractores realmente están funcionando como esperamos que funcionen. Lo que esperaríamos es obtener correlaciones negativas en todos los distractores; es decir, contestar un reactivo con uno de los distractores se relaciona con obtener puntajes más bajos en la prueba. En este caso no hay un criterio establecido para qué tan grande deba ser el nivel de la correlación. Sin embargo, en caso de obtener correlaciones positivas en alguno de los distractores, nos estaría diciendo que elegir ese distractor se relaciona con obtener puntajes más altos, por lo que podría ser que ese distractor en realidad sea una respuesta correcta o parcialmente correcta. 5.1.2 Teoría de Respuesta al Ítem Otra metodología que también se suele utilizar para analizar las propiedades de los reactivos es la Teoría de Respuesta al Ítem. Este modelo teórico, a diferencia del anterior, se centra en la relación entre el nivel de habilidad de los sustentantes y su probabilidad de responder de manera correcta o incorrecta cierto reactivo. Esta teoría parte del supuesto de que la probabilidad de responder de manera correcta un reactivo está dada en función del nivel de habilidad de los sustentantes y por 1 o más parámetros referentes al reactivo de manera específica. Tomando en cuenta estos parámetros, se obtiene la probabilidad de que un sustentante conteste de manera correcta un reactivo en función de su nivel de habilidad. Pero de manera más interesante para este contexto, nos da específicamente los valores de estos parámetros, que si observamos con detenimiento podremos identificar que los primeros dos se refieren al mismo concepto que vimos en la Teoría Clásica de los Test. 5.1.2.1 Modelo de un parámetro El modelo de un parámetro únicamente toma en cuenta el nivel de habilidad de los sustentantes y la dificultad del reactivo, y asume que la discriminación de todos los reactivos es igual. El valor del índice de dificultad, en teoría puede ir de -infinito a +infinito. El índice de dificultad puede variar de valores negativos cuando es fácil o valores positivos cuando es difícil. Un buen criterio de decisión es conservar los reactivos cuyo valor en el parámetro de dificultad sea de -2.5 a 2.5. 5.1.2.2 Modelo de dos parámetros El modelo de dos parámetros, además de considerar el nivel de habilidad y la dificultad de los reactivos, incluye la variación en el nivel de discriminación de los reactivos. El parámetro de discriminación puede tomar valores comenzando en 0 y hasta un máximo de 2. Suele considerarse que reactivos con valores iguales o mayores a 0.45 son reactivos con un buen nivel de discriminación. Valores más pequeños indicarían que el reactivo no discrimina bien entre los grupos con mayor o menor nivel de habilidad. 5.1.3 Eliminación de reactivos Ya que hemos obtenido toda la información anterior, es importante que la analicemos reactivo por reactivo para que podamos decidir qué reactivos conservaremos para la versión final y para la calificación. La decisión de eliminar o no un reactivo dependerá de múltiples factores. En primer lugar, es importante ver qué tanto cumple con los criterios esperados, si en un principio el reactivo tiene muy malos indicadores (por ejemplo, índice de dificultad menor a .1, índice de discriminación menor a .2 o correlación biserial negativa), entonces lo mejor será eliminarlo directamente, ya que un reactivo con un desempeño muy pobre es difícilmente rescatable. Sin embargo, si se excede en alguno de los indicadores o en varios por poco, es importante considerar el funcionamiento de los distractores, para ver si es factible enviar el reactivo a revisión para que sea corregido para futuras aplicaciones. Otro aspecto que es importante tomar en cuenta es la cantidad de reactivos que se tiene para cada indicador. Si fuéramos muy estrictos y elimináramos cada reactivo que no cumple con alguno de los criterios, probablemente terminaríamos con un examen muy pequeño, y el riesgo de eso es que terminemos por eliminar todos los reactivos de alguna dimensión o subdimensión, logrando que tengamos un examen con una gran subrepresentación del constructo. Cuando diseñamos exámenes, por lo general se genera más de un reactivo para cada indicador, pero puede darse el caso que todos los reactivos de un indicador tengan alguna deficiencia, por lo que hay que considerar cuál o cuáles de ellos pueden ser más susceptibles a correcciones que ayuden a mejorar su desempeño, y con ello conservar el indicador. 5.2 Dimensionalidad Un aspecto muy importante a evaluar para obtener evidencias de validez es la dimensionalidad. Este aspecto se refiere a qué tanto los reactivos se agrupan de la manera que esperaríamos se agrupen. Supongamos que elaboramos un examen de matemáticas, donde incluimos varios reactivos de álgebra y varios de geometría. Si un estudiante sabe realizar operaciones algebraicas, pero tiene dificultades con geometría, esperaríamos que conteste de manera correcta la mayoría de los reactivos de álgebra y que no pueda contestar correctamente muchos de geometría. En otras palabras, nosotros suponemos que los reactivos que clasificamos como referentes a álgebra están midiendo toda la habilidad en álgebra de los estudiantes y los que elaboramos de geometría, que estén todos midiendo la habilidad en geometría. Tomando esto en cuenta, lo que esperaríamos es que nuestro examen tenga una estructura interna definida por lo que planteamos en un inicio al elaborar la tabla de especificaciones, si suponemos que todos los reactivos dentro de una dimensión o dominio están midiendo un mismo constructo, esperaríamos que estos reactivos se agrupen de esta manera. Habrá otras ocasiones en que el examen en su conjunto presente una estructura unidimensional, es decir, todos los reactivos se agrupen y midan un solo constructo. Por ejemplo, un examen de comprensión lectora es muy probable que presente una estructura unidimensional. Para poder determinar la estructura interna de un instrumento, son dos las principales técnicas utilizadas: análisis factorial exploratorio AFE y análisis factorial confirmatorio AFC. 5.2.1 Análisis Factorial Exploratorio En el primer caso, mediante un software estadístico, se establecen algunos criterios para que el software haga una serie de análisis que permitan identificar la cantidad de factores (término utilizado para referirnos a las dimensiones del examen) necesarios y los reactivos que se agrupan en cada uno de estos factores. Dado que es un enfoque exploratorio, nosotros solamente proporcionamos los reactivos y algunos criterios como el método de estimación, la cantidad de factores o los métodos de rotación (se explicará más a detalle en la sección práctica), y es el software el que determina la estructura que mejor se ajusta a los datos. Nota: Dado que este procedimiento es más complejo en cuanto a los análisis y los criterios de decisión, recomendamos al lector revisarlo directamente en el siguiente capítulo de Análisis Estadísticos en R para que lo puedan revisar en un ejemplo concreto. Este comentario aplica tanto para AFE como para AFC y DIF que se verá más adelante. 5.2.2 Análisis Factorial Confirmatorio En el caso del AFC, nosotros proporcionamos el modelo a contrastar. Es decir, nosotros especificamos qué reactivos pertenecen a qué dimensión o factor, y el software estadístico se encarga de identificar qué tan bien se ajusta ese modelo a los datos que tenemos. En otras palabras, el AFC nos dice en qué medida los reactivos realmente se comportan de esa manera. Por lo general, cuando hicimos una buena construcción de la tabla de especificaciones y de los reactivos, ya tenemos una idea de cómo esperamos que se agrupen los factores, por lo que es posible usar directamente el AFC en lugar de comenzar con el AFE. 5.3 Confiabilidad Cuando hablamos de confiabilidad nos referimos a la consistencia de las calificaciones obtenidas por las mismas personas en ocasiones diferentes o con diferentes conjuntos de reactivos equivalentes (Anastasi, 1966). En otras palabras, para que un instrumento sea confiable, nosotros esperaríamos que, si lo aplicamos a una persona en un momento dado, y se lo volvemos a aplicar tiempo después (asumiendo que su nivel de rasgo no cambie), la persona obtenga resultados iguales o muy similares en ambas aplicaciones. O, si le administramos dos instrumentos que miden lo mismo con reactivos equivalentes (reactivos que evalúen lo mismo con el mismo nivel de dificultad y discriminación), la persona obtendría también resultados muy similares en ambos instrumentos. En el primer caso, la manera más simple de obtener la confiabilidad del instrumento sería al obtener la correlación entre los puntajes de la primera y la segunda aplicación (Test-Retest), y en el segundo caso, la correlación entre los puntajes de ambas versiones del instrumento. Sin embargo, estos dos procedimientos no siempre son viables. En el caso de exámenes, al ser preguntas de conocimientos, es más sencillo que los estudiantes se familiaricen con el contenido y les sea más sencillo en una segunda aplicación del mismo examen, por lo que el nivel de confiabilidad podría estar sesgado, además de que no siempre es posible reunir a las mismas personas para dos aplicaciones distintas. De igual forma, diseñar dos versiones paralelas del mismo examen, es costo y toma mucho tiempo, por lo que no en todos los casos es algo viable o deseable. Por ello, una tercera alternativa es evaluar la confiabilidad con respecto a la consistencia interna del instrumento, es decir, qué tanto los reactivos del instrumento se relacionan entre sí, y que tan homogéneos son. La prueba para medir la confiabilidad por consistencia interna más utilizada es el Alpha de Cronbach. Este coeficiente se ubica en un rango de 0 a 1, siendo los valores más cercanos a 1 indicadores de una consistencia interna alta. Sin embargo, este coeficiente tiene algunas limitaciones, dado que depende mucho de la cantidad de reactivos y del tamaño de la muestra (entre más reactivos tenga el examen y más personas se tengan en la muestra, más alto será el valor de Alpha). Además, se basa en algunos supuestos que no siempre es posible que se cumplan en el contexto de los exámenes. Por ello, una alternativa es utilizar el coeficiente Omega de McDonald, el cual reduce los sesgos por tamaño de muestra y cantidad de reactivos, y toma en cuenta la dimensionalidad del instrumento (si el instrumento tiene varias dimensiones, las analiza en conjunto y otorga un valor específico para cada dimensión y uno global). Estándar 2.3: Para cada puntaje total, subpuntaje o combinación de puntajes que deba interpretarse, deben reportarse estimaciones de índices relevantes de confiabilidad/ precisión. (AERA, APA, and NCME 2018) 5.4 Imparcialidad Un análisis que puede resultar de suma importancia de acuerdo con nuestros usos e interpretaciones previstos es el análisis de la imparcialidad. Cuando tenemos grupos claramente diferenciados a quienes se les aplicará nuestro examen, (e.g., hombres y mujeres, personas extranjeras y nativas, personas provenientes de sistemas de bachillerato diferentes, etc.), uno de los objetivos que debemos asegurar para nuestro examen, es que sea capaz de determinar el nivel de habilidad que tiene la persona en el constructo que evalúa nuestro examen independientemente de sus características particulares o grupo de pertenencia; en otras palabras, queremos que nuestro examen determine si alguien es aprobado o reprobado por su nivel de habilidad únicamente, y no que una persona apruebe porque le da cierta ventaja pertenecer a un grupo específico. Cuando los reactivos se comportan de esta manera, decimos que son reactivos sesgados, en los que algo de su redacción, planteamiento u opciones de respuesta, está favoreciendo alguna característica de uno de los grupos, independientemente de su nivel de habilidad. Para detectar la presencia de este sesgo, la herramienta más utilizada es el análisis del Funcionamiento Diferencial del Reactivo (DIF por sus siglas en inglés). Existen distintos métodos para estimar la presencia de DIF en los reactivos, algunos provenientes de la TRI y otros provenientes de la TCT. En general, estos métodos buscan analizar si existen diferencias en la probabilidad de los participantes de responder correctamente un reactivo, cuando ambos participantes tienen el mismo nivel de habilidad. Nota: Para revisar con más detalle la interpretación de estos análisis, por favor dirigirse al capítulo anexo de análisis en R en la sección de análisis DIF Una vez que hemos identificado si existen reactivos que presentan DIF, es importante que hagamos un análisis cualitativo para interpretar aquello que puede estar generando este DIF en el reactivo. La presencia de DIF no significa necesariamente que el reactivo esté mal; en realidad, la presencia de DIF puede también indicar diferencias reales entre los grupos evaluados. Por ejemplo, que un grupo proveniente de cierta universidad salga más alto que los de otras universidades, puede ser porque el examen esté sesgado y evalúe cosas que solo se vieron en ese programa, pero también puede que sea debido a que en general, en esa universidad se ofrece una preparación de mayor calidad que en otras escuelas, y el examen está siendo capaz de medir esas diferencias. En todo caso, es importante indagar más, tanto a nivel cuantitativo con los siguientes análisis como a nivel cualitativo con la revisión de expertos y la presentación de evidencias que respalden una u otra afirmación. Dependiendo de estos resultados, será importante tomar la decisión de si todos los reactivos se utilizan para la calificación, o si se decide omitir ciertos reactivos de la calificación final dado que presentan un DIF muy significativo. Estándar 4.10: Cuando un desarrollador de pruebas evalúa las propiedades psicométricas de los ítems, el modelo utilizado para ese fin (p. ej., teoría clásica de los tests, teoría de respuesta al ítem u otro modelo) debe documentarse. La muestra utilizada para estimar las propiedades de los ítems debe describirse y debe ser de un tamaño y diversidad adecuados para el procedimiento. El proceso por el cual se criban los ítems y los datos utilizados para cribado, como dificultad del ítem, discriminación de ítems, o funcionamiento diferencial de los ítems (DIF) para grupos importantes de individuos examinados también deben documentarse. Cuando se utilizan métodos basados en modelos (p. ej., TRI) para estimar los parámetros de los ítems en el desarrollo de pruebas, el modelo de respuesta al ítem, los procedimientos de estimación y la evidencia de ajuste del modelo deben documentarse. (AERA, APA, and NCME 2018) 5.5 Evidencias de validez Recordando que lo que se vuelve válido no es el instrumento en sí, sino las interpretaciones y usos que se le dan a los resultados del mismo, el tema de recopilar evidencias de validez para estos usos e interpretaciones se vuelve sumamente amplio y complejo. Además de las evidencias que ya hemos ido generando, algunos de los principales métodos para obtención de evidencia de validez que vienen de la elaboración de análisis estadísticos son los siguientes: 5.5.1 Correlación con otras pruebas: Supongamos que nosotros definimos nuestro examen como un instrumento de medición que permite dar cuenta del constructo de comprensión lectora a partir de cierta teoría, y esta teoría, afirma que la comprensión lectora tiene una relación significativa con la memoria de trabajo que mencionamos en otro ejemplo. Si queremos generar mayor evidencia de la validez del uso de este examen como una medida de comprensión lectora bajo esta teoría, podríamos aplicar esta misma prueba junto con nuestro examen a los participantes y medir qué tanto se relacionan estadísticamente estas dos pruebas. De igual forma, si según nuestra teoría, existe una relación fuerte entre el nivel de comprensión lectora y el número de libros que la persona reporta leer al año en promedio, podríamos solicitar este dato a nuestros participantes y observar si esta relación se presenta como esperaríamos. En términos generales, este aspecto tiene que ser planeado desde un inicio, definiendo qué pruebas o qué datos puedo solicitar a los sustentantes que me permitan dar cuenta de la validez de esta interpretación bajo cierta teoría. La decisión de qué aspectos incluir estará influida por el tiempo que dispondremos para la aplicación, por la longitud de nuestros instrumentos (no es conveniente aplicar muchos instrumentos seguidos, ya que los participantes podrían cansarse y reducir su rendimiento), por la solidez de la teoría que afirma la relación entre los instrumentos y por los requerimientos específicos de la institución en relación con los usos e interpretaciones previstos. 5.5.2 Regresiones lineales y logísticas Si bien las correlaciones pueden ser una fuente importante de evidencias de validez, en ocasiones necesitaremos más que demostrar la relación entre dos variables, ver si esa relación tiene algún nivel de causalidad. Por lo general, muchos exámenes se diseñan para ser usados como criterios para la toma de decisiones, y esto implica que tomamos el resultado del examen como un predictor de algo que acontecerá en el futuro. Por ejemplo, un examen de admisión es utilizado como criterio para decidir si alguien es admitido o no en una universidad, bajo la premisa de que, si obtiene un puntaje alto en el examen, es más probable que tenga un desempeño alto en el programa al que será admitido. Para poder presentar evidencia de que esta interpretación del resultado del examen como predictor de desempeño es válida, necesitamos, no solamente literatura que señale dicha relación causal, sino también evidencia empírica de nuestro caso en particular. Este ejemplo puede resultar sumamente complejo para generar evidencias, pero hay otros casos en los que lo que intentamos predecir es más sencillo de medir a corto plazo. Por ejemplo, un examen de egreso del bachillerato como predictor del desempeño de ingreso a licenciatura, o un examen que predice los puntajes en otro examen específico. Cuando hablamos en términos de predicción, la herramienta más utilizada es el análisis de regresión lineal o de regresión logística. El primero nos permite predecir, a partir de algún criterio como el puntaje de nuestro examen, el resultado en otra variable con varias opciones de resultado posibles; por ejemplo, predecir las calificaciones de un alumno a partir de su puntaje de ingreso, o predecir su puntaje en otro examen. Por su parte, la regresión logística, nos permite predecir la probabilidad de que se dé una condición específica o no. Por ejemplo, podemos intentar predecir, a partir de un examen de egreso del bachillerato, la probabilidad de que los alumnos sean admitidos o no a la educación superior. 5.6 Calificación Una vez que hemos terminado todos los análisis anteriores y contamos con una versión final del instrumento que presenta buenos índices de calidad de los reactivos, una adecuada dimensionalidad, confiabilidad y otras evidencias de validez de las interpretaciones, es momento de tomar los reactivos que terminamos conservando y utilizarlos para reportar las calificaciones de los sustentantes. Si hemos llevado el proceso de manera sistemática hasta este momento, siguiendo todos los pasos aquí especificados, y si realizamos los análisis con la clave de respuestas de manera correcta, este procedimiento resultará sumamente sencillo, dado que solo queda generar la suma de los aciertos de cada estudiante, proceso que ya realizamos en el capítulo de análisis en R. Sin embargo, hay algunos pasos adicionales correspondientes a la interpretación y al reporte de estos resultados. 5.6.1 Métodos para establecer puntos de corte (standard setting) Si uno de los usos establecidos para la prueba involucra la definición de grupos específicos a partir de los puntajes de la prueba; por ejemplo, clasificar a los sustentantes en desempeño insatisfactorio, satisfactorio y sobresaliente o determinar una calificación aprobatoria para la admisión a la universidad, es necesario llevar a cabo un procedimiento más para definir de la manera más objetiva y empírica posible los puntos de corte que vamos a utilizar para cada caso. Existen muchos y muy diversos métodos para establecer puntos de corte, y cada uno de ellos utiliza procedimientos muy diversos. Para fines prácticos, en este manual describiremos únicamente uno de los métodos más utilizados: el método de Angoff utilizado para situaciones en las que requerimos calcular únicamente un punto de corte (e.g., aprobado-no aprobado) basados en un criterio específico. Si al lector le interesa obtener más información sobre distintos métodos, o desea obtener más de un punto de corte o puntos referentes a normas, recomendamos revisar el capítulo 10 del libro de Lane et al. (2016). 5.6.1.1 Método de Angoff Para poder determinar el punto de corte de nuestro examen con este método, necesitamos de la ayuda de un grupo de expertos para tomar la decisión. Este grupo puede ser el mismo que nos ayudó con la generación de reactivos, aunque si es posible conseguir otro grupo, también es algo recomendable. El método de Angoff consiste en presentar a este grupo de expertos cada uno de los reactivos que se utilizarán para puntaje, y solicitarle a cada uno, primero, que tengan en mente cómo sería un examinado con un nivel de competencia mínimamente necesario para aprobar el examen (o si la decisión es que el examinado entre o no entre a la universidad, por ejemplo, se buscaría tener en mente a una persona con el nivel mínimo de competencia necesario para desempeñarse adecuadamente en la universidad). Una vez que se considera este perfil de acuerdo con cada uno de los jueces, se les solicita que determinen cual consideran que es la probabilidad de que un sustentante con el nivel de competencia mínimamente requerido conteste correctamente dicho reactivo, ubicando la probabilidad normalmente en una escala de 1 a 100. Ya que se han generado las probabilidades de cada reactivo por cada juez, suele hacerse una segunda ronda, permitiendo la discusión y diálogo entre los jueces y otorgar retroalimentación, para así reducir la variabilidad entre sus puntajes, buscando generar resultados más consistentes. Nota: En ocasiones, se suele presentar a los jueces los resultados psicométricos de los reactivos para ayudarles en su toma de decisión, en particular, se les puede mostrar los índices de dificultad y de discriminación de cada reactivo. Una vez que se han dado las dos rondas, se capturan los resultados finales de cada uno de los jueces y se elabora una tabla en la que se registre el puntaje que cada juez le dio a cada reactivo, para después, promediar estos puntajes por reactivo y por juez, y al sumar todas estas probabilidades promedio y dividirlas entre el total de jueces y reactivos, se obtiene el punto de corte. Dado que se les solicita probabilidades de 0 a 100, lo que obtenemos como puntaje se puede interpretar como un porcentaje de respuestas correctas. Estándar 5.21: Cuando las interpretaciones de puntajes propuestas involucran uno o más puntajes de corte, deben documentarse claramente la justificación y los procedimientos utilizados para establecer puntajes de corte. (AERA, APA, and NCME 2018) Evidencia documental: Durante esta fase, es importante registrar todos los procedimientos llevados a cabo, especialmente la metodología utilizada para llegar a ciertos resultados. Por ejemplo, qué criterios se utilizaron para desarrollar el AFE, de qué manera se estimó el índice de discriminación (mediante TCT o TRI), qué método se utilizó para estimar la confiabilidad, etc. De igual forma, todos los resultados deben ser capturados y resguardados para utilizarlos posteriormente en el informe final. Seguridad: Es importante que diseñemos un protocolo que nos permita resguardar y limitar el acceso a las bases de datos utilizadas para realizar el análisis, 5.7 A modo de repaso ¿Para qué nos sirven los análisis de las propiedades de los reactivos como el análisis de dificultad, discriminación o de los distractores? ¿Si tenemos un reactivo que presenta un índice de dificultad de 0.20 y que presenta un DIF moderado, debemos eliminarlo? Justifica tu respuesta ¿Para qué necesitamos hacer los análisis DIF? REFERENCIAS "],["COMP6.html", "COMPONENTE 6 ¿CÓMO EJECUTAMOS LOS ANÁLISIS ESTADÍSTICOS? 6.1 ¿Cómo capturamos nuestros datos? 6.2 ¿Cómo cargamos nuestros datos en R? 6.3 Desempeño de los reactivos 6.4 Análisis de los distractores 6.5 Dimensionalidad 6.6 Confiabilidad 6.7 Imparcialidad 6.8 Gráficos", " COMPONENTE 6 ¿CÓMO EJECUTAMOS LOS ANÁLISIS ESTADÍSTICOS? En esta sección presentaremos herramientas para llevar a cabo el análisis estadístico de los reactivos y de la prueba en general. Para ayudar a que cualquier persona que lea el manual pueda llevar a cabo estos análisis, utilizaremos el lenguaje de programación R. Pese a que llamarlo lenguaje de programación puede resultar intimidante, trataremos de presentar todo el contenido de esta sección de la manera más simple y didáctica posible, de tal forma que el lector solo tenga que reemplazar uno o dos valores que nosotros indicaremos, y que con eso sea suficiente para obtener los resultados deseados. Para lograr este cometido, necesitaremos dos prerrequisitos importantes, el primero de ellos es tener instalado R y RStudio; para ello, recomendamos dirigirse a este enlace con instrucciones para su instalación: El segundo requisito es tener una base de datos limpia y ordenada que permita analizar la información de la manera más simple posible. Para ello, recomendamos al lector dirigirse a la siguiente sección y seguir cada uno de los tips presentados ahí. 6.1 ¿Cómo capturamos nuestros datos? Una base de datos ordenada es aquella en la que cada fila representa a un solo sustentante y cada columna representa un atributo medido de ese sustentante. La primera fila ha de contener el nombre de dichos atributos, y las filas subsecuentes contendrán el valor de ese atributo para cada sustentante sin que existan duplicaciones de información. Cuando hablamos de una base de datos limpia, nos referimos a que cada celda de esta contenga información útil, correcta e interpretable. En particular hay algunos puntos que se necesitan para lograr esto: Nombres de las columnas: Cada columna debe tener un descriptor breve y explicativo. Es sumamente recomendable utilizar una única palabra o abreviatura para describir cada columna. Es importante que usemos descriptores claros, si decido llamara a la columna de Licenciatura de Procedencia como licproc, probablemente resulte confuso para una segunda persona que lea la base de datos; una mejor opción podría ser lic_procedencia. Nótese que se utiliza un _ para separar las palabras, esto será importante para los subsecuentes análisis. Codificación de valores: Cuando hacemos análisis estadísticos, la mayoría de los programas funcionan mejor si utilizamos números para todos nuestros valores. Por ello, es importante que todos los atributos capturados de los sustentantes sean capturados con números (o convertidos en caso de que ya tengamos la base de datos hecha). Por ejemplo, el atributo de sexo puede ser convertido asignando el número 0 para todos los sustentantes que sean hombres y el número 1 para todas las sustentantes que sean mujeres. En este caso, los números 0 y 1 no significan que un sexo sea superior a otro, simplemente son las etiquetas que se dan para distinguirlos. Nota: Es muy importante que mantengamos un registro por separado de lo que cada código significa. Esto puede hacerse en una segunda hoja de cálculo o en algún documento de texto que guardemos junto a la base de datos. Secuencia de las columnas: Es recomendable que introduzcamos cada atributo del sustentante en un orden lógico. De igual forma, se recomienda que las columnas iniciales contengan los datos de identificación o demográficos. La primera columna suele ser el identificador del sustentante, ya sea su nombre, clave única o clave asignada. Posterior a esto, se introducen los demás atributos como el sexo, edad, semestre que cursa, etc. En cuanto a los reactivos, es importante que se registren en orden: si tenemos 15 reactivos de un subdominio específico, que esos 15 reactivos se registren en 15 columnas consecutivas. 6.1.1 La base de datos que utilizaremos Para este ejemplo, se diseñó una base de datos ficticia de un examen de matemáticas que cubre tres áreas principales: Álgebra, Geometría y Cálculo. Cada área tiene 6 reactivos, siendo que el primer reactivo de Álgebra aparece como Alg1, el primero de Geometría como Geo1, y el primero de Cálculo como Calc1, y todos los demás con la misma clave y únicamente variando en el número final. Además de esto, se elaboró de forma ficticia variables descriptivas como sexo, grado escolar y edad. En un archivo de excel, siguiendo las recomendaciones anteriores, nuestra base de datos se vería así: 6.2 ¿Cómo cargamos nuestros datos en R? Lo primero que tenemos que hacer es abrir RStudio e instalar los paquetes que vamos a requerir para elaborar nuestros análisis. A continuación se presenta el código necesario a ejecutar. La primera vez será necesario ejecutar todo el código, incluidos los comandos install.packages. En las ocasiones siguientes, una vez que se han instalado los paquetes, solamente será necesario correr la segunda mitad con los comandos library # Instalar paquetes install.packages(&quot;psych&quot;) install.packages(&quot;psy&quot;) install.packages(&quot;sjPlot&quot;) install.packages(&quot;lavaan&quot;) install.packages(&quot;difR&quot;) install.packages(&quot;mirt&quot;) install.packages(&quot;pander&quot;) install.packages(&quot;kableExtra&quot;) install.packages(&quot;corrplot&quot;) install.packages(&quot;ShinyItemAnalysis&quot;) install.packages(&quot;ltm&quot;) install.packages(&quot;lme4&quot;) install.packages(&quot;tidyverse&quot;) #Cargar paquetes library(psych) library(psy) library(sjPlot) library(lavaan) library(difR) library(mirt) library(pander) library(kableExtra) library(corrplot) library(ShinyItemAnalysis) library(ltm) library(lme4) library(tidyverse) El primer paso es leer la base de datos que utilizaremos. Es recomendable convertirla a formato csv y moverla a la carpeta donde creémos el proyecto. Una vez que lo hemos hecho, podemos ejecutar la siguiente línea, reemplazando lo que está entre comillas después de ~/ por el nombre de nuestro archivo para la base de datos datos_completos &lt;- read.csv(&quot;~/examen matematicas.csv&quot;) Lo que aparece al inicio del código datos completos es el nombre que tendrá la variable dentro del ambiente de trabajo de R, este nombre es nuestra sugerencia, pero siempre puede tomar cualquier otro valor, siempre y cuando seamos consistentes al utilizarlo igual en las siguientes funciones. 6.2.1 ¿Cómo capturamos la clave de respuestas? Si nuestra base de datos viene con las repuestas directas que los sustentantes dieron al examen, es decir, aprece qué opción de respuesta seleccionó en cada reactivo (e.g. A, B o C), es necesario que capturemos en R cuál es la respuesta correcta para cada uno de los reactivos. Para ello, bastará que usemos la siguiente línea de código, sustituyendo cada uno de los valores dentro de c() que se encuentran entre comillas por el valor correspondiente para la respuesta correcta de los reactivos de nuestro examen. Se presentan las opciones en el orden en que esté cada reactivo en la base de datos; es decir, si el primer reactivo tiene como respuesta correcta la A, el primer valor dentro de c(), deberá ser A, si el segundo reactivo tiene como respuesta correcta D, el segundo valor dentro de c() será D, y así sucesivamente. Es importante que utilicemos la misma codificación que en la base de datos. Por ejemplo, si nuestra base de datos tiene a en lugar de A, tenemos que respetar ese formato para que el código funcione. En este ejemplo, dado que son 18 reactivos, podemos observar que tenemos 18 letras entre comillas y separadas por comas. Es muy importante que verifiquemos esta información, dado que cualquier error aunque sea en una sola de estas opciones, invalidará todas nuestras posibles interpretaciones de los resultados. Este paso es necesario únicamente si nuestra base de datos viene con las opciones de respuesta codificadas así, si nuestra base de datos viene directamente con unos y ceros, podemos saltar este paso. cadena_respuestas &lt;- c(&quot;A&quot;,&quot;D&quot;,&quot;C&quot;,&quot;B&quot;,&quot;A&quot;,&quot;B&quot;,&quot;C&quot;, &quot;D&quot;,&quot;C&quot;,&quot;A&quot;,&quot;B&quot;,&quot;D&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;,&quot;D&quot;, &quot;C&quot;) Nota En este caso decidimos nombrar la variable como cadena_respuestas dado el tipo de formato que es en R. Mantener los nombres que hemos sugerido, permitirá que sea más sencillo ejecutar todo el código con muy ligeras modificaciones y evitar errores, pero se recalca nuevamente que siempre es posible cambiar los nombres de las variables. 6.2.2 Seleccionar solamente los reactivos y transformarlos Ahora, vamos a seleccionar de nuestra base de datos únicamente los reactivos y los convertiremos en una variable diferente. Dentro del componente select pondremos el nombre del primer reactivo que aparezca en la base de datos, seguido de este, pondremos dos puntos : y después el nombre del último reactivo de la base de datos. Esta es la razón por la que es importante acomodar los reactivos en orden y todos seguidos reactivos_correctos &lt;- datos_completos%&gt;% select(Calc1:Geo6) Si ponemos atención, el código utiliza el valor de datos_completos. Este es el primer ejemplo en el que comenzamos a utilizar las variables que ya hemos definido antes, por esto es que es muy importante mantener las consistencias con los nombres, ya que si en esta línea cambiamos aunque sea una letra, R ya no podrá encontrar las variables que definimos previamente y nos dará un error Con la siguiente linea convertiremos los valores A, B, C y D en respuestas correctas utilizando la clave que generamos con anterioridad. Esto nos permitirá tener una base de datos que puede ser analizada con algunas de las técnicas que veremos a continuación. reactivos_correctos[] &lt;- +t(t(reactivos_correctos) == cadena_respuestas) Si ejecutamos la siguiente línea de código, podremos visualizar cómo se ven ahora las primeras observaciones de nuestra base de datos que solamente contiene reactivos y que se ha convertido en unos y ceros. head(reactivos_correctos) ## Calc1 Calc2 Calc3 Calc4 Calc5 Calc6 Alg1 Alg2 Alg3 Alg4 Alg5 Alg6 Geo1 Geo2 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 2 1 1 1 1 1 1 1 1 1 1 1 0 0 1 ## 3 0 1 1 0 1 0 0 0 0 0 0 0 0 1 ## 4 1 1 1 1 1 1 0 1 1 1 1 0 0 1 ## 5 1 1 1 1 1 1 0 0 0 0 0 1 1 1 ## 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## Geo3 Geo4 Geo5 Geo6 ## 1 1 1 1 1 ## 2 0 0 1 1 ## 3 1 1 1 1 ## 4 0 1 1 0 ## 5 1 1 1 1 ## 6 0 1 1 1 6.3 Desempeño de los reactivos Ya que tenemos esta base de datos, podemos realizar algunos análisis para ver cómo se comportan los reactivos. Si recordamos lo revisado en la sección del desempeño de los reactivos, los principales análisis que podemos hacer en este campo son: TCT Índice de Dificultad Índice de Discriminación Correlación reactivo-total Alfa si se elimina el reactivo TRI Índice de Dificultad Índice de Discriminación Para mayor practicidad, sugerimos al lector ejecutar todas las siguientes líneas de código tal como están, con lo que obtendrá como resultado final una tabla con todos estos estadísticos. La interpretación de estos estadísticos se especificó en el capítulo referente los tipos de análisis que podemos realizar. #Tabla inicial con dificultad y discriminación tabla_items&lt;-ItemAnalysis(reactivos_correctos) #Modelo de 2 Parámetros IRT del examen de redaccion modelo2PL &lt;- mirt(data=reactivos_correctos,model = 1, itemtype=&quot;2PL&quot;, SE=TRUE, verbose=FALSE) coef.2PL &lt;- coef(modelo2PL, IRTpars=TRUE, simplify=TRUE) items.2PL &lt;- as.data.frame(coef.2PL$items) #Creación de la tabla final propiedades_items&lt;-bind_cols(tabla_items[,c(1,10,12,15)], items.2PL[,1:2])%&gt;% rename(c (&quot;Dificultad&quot; = &quot;Difficulty&quot; , &quot;Discriminación&quot; = &quot;ULI&quot;, &quot;Alfa si se elimina&quot; = &quot;Alpha.drop&quot;, &quot;Correlación con Total&quot; = &quot;RIR&quot; ,&quot;Discriminación TRI&quot; = &quot;a&quot;, &quot;Dificultad TRI&quot; = &quot;b&quot;)) #Presentación de la tabla final kbl(propiedades_items)%&gt;% kable_paper(&quot;hover&quot;, full_width = F) Dificultad Discriminación Correlación con Total Alfa si se elimina Discriminación TRI Dificultad TRI Calc1 0.7913669 0.4532374 0.5148635 0.8665042 1.898230 -1.0765750 Calc2 0.8705036 0.3237410 0.5328361 0.8664821 2.273188 -1.3999271 Calc3 0.8633094 0.3237410 0.5135592 0.8669967 2.058128 -1.4142462 Calc4 0.8225420 0.4460432 0.5449856 0.8655487 2.262323 -1.1417800 Calc5 0.8465228 0.3669065 0.4742614 0.8681438 1.787357 -1.4051080 Calc6 0.7985612 0.4748201 0.5303807 0.8659432 2.037352 -1.0760163 Alg1 0.7026379 0.5827338 0.4408353 0.8696223 1.386176 -0.8283282 Alg2 0.5611511 0.6258993 0.4357119 0.8703345 1.248749 -0.2530500 Alg3 0.8201439 0.4316547 0.5439254 0.8655685 1.998167 -1.1907733 Alg4 0.8033573 0.4964029 0.5763031 0.8642516 2.259346 -1.0524779 Alg5 0.5971223 0.6330935 0.4648838 0.8689340 1.443025 -0.3721805 Alg6 0.6211031 0.6258993 0.4567892 0.8692341 1.454254 -0.4651869 Geo1 0.7865707 0.5107914 0.4941531 0.8672825 1.717702 -1.1066032 Geo2 0.6786571 0.5827338 0.5042273 0.8669888 1.604966 -0.6648394 Geo3 0.7577938 0.4460432 0.4436710 0.8692825 1.340876 -1.1154822 Geo4 0.7338129 0.5539568 0.5642772 0.8644139 1.979519 -0.8104590 Geo5 0.8465228 0.3741007 0.4759789 0.8680866 1.695224 -1.4434632 Geo6 0.8177458 0.3812950 0.4329043 0.8695228 1.373836 -1.4357878 6.4 Análisis de los distractores El lector recordará que también sugerimos incluir el análisis de los distractores. La manera más práctica de hacerlo es al visualizar la frecuencia con la que cada una de las opciones de respuestas fue seleccionada. Si deseamos ver estas frecuencias en forma de tabla y separados por grupos bajo, medio y alto (en las tablas aparecen como 1, 2 y 3 respectivamente), el siguiente comando puede ayudarnos a lograrlo. Nota En este caso, lo que nosotros ejecutaremos serán las primeras líneas hasta print(distractores). Lo siguiente es el resultado de estas líneas para nuestro ejemplo. Simplemente R lo muestra junto a las líneas para mayor practicidad. Como regla general, todo lo que veamos en los cuadros de código que comience con ## es el resultado que R nos da, y no son líneas que debamos ejecutar. distractores&lt;-datos_completos%&gt;% select(names(reactivos_correctos))%&gt;% DistractorAnalysis(key = cadena_respuestas, p.table=TRUE) print(distractores) ## $Calc1 ## score.level ## response Group1 Group2 Group3 ## A 0.569 0.845 0.993 ## B 0.268 0.109 0.000 ## C 0.124 0.008 0.007 ## D 0.039 0.039 0.000 ## ## $Calc2 ## score.level ## response Group1 Group2 Group3 ## A 0.176 0.039 0.015 ## B 0.078 0.000 0.000 ## C 0.052 0.000 0.000 ## D 0.693 0.961 0.985 ## ## $Calc3 ## score.level ## response Group1 Group2 Group3 ## A 0.105 0.000 0.007 ## B 0.039 0.000 0.007 ## C 0.680 0.953 0.985 ## D 0.176 0.047 0.000 ## ## $Calc4 ## score.level ## response Group1 Group2 Group3 ## A 0.033 0.000 0.000 ## B 0.588 0.915 1.000 ## C 0.281 0.062 0.000 ## D 0.098 0.023 0.000 ## ## $Calc5 ## score.level ## response Group1 Group2 Group3 ## A 0.654 0.930 0.985 ## B 0.209 0.016 0.000 ## C 0.105 0.031 0.000 ## D 0.033 0.023 0.015 ## ## $Calc6 ## score.level ## response Group1 Group2 Group3 ## A 0.052 0.016 0.000 ## B 0.549 0.891 0.993 ## C 0.288 0.078 0.007 ## D 0.111 0.016 0.000 ## ## $Alg1 ## score.level ## response Group1 Group2 Group3 ## A 0.163 0.031 0.000 ## B 0.033 0.031 0.000 ## C 0.405 0.752 0.993 ## D 0.399 0.186 0.007 ## ## $Alg2 ## score.level ## response Group1 Group2 Group3 ## A 0.438 0.217 0.081 ## B 0.229 0.070 0.015 ## C 0.092 0.078 0.052 ## D 0.242 0.636 0.852 ## ## $Alg3 ## score.level ## response Group1 Group2 Group3 ## A 0.111 0.008 0.000 ## B 0.020 0.000 0.000 ## C 0.588 0.915 0.993 ## D 0.281 0.078 0.007 ## ## $Alg4 ## score.level ## response Group1 Group2 Group3 ## A 0.523 0.930 1.000 ## B 0.340 0.062 0.000 ## C 0.085 0.000 0.000 ## D 0.052 0.008 0.000 ## ## $Alg5 ## score.level ## response Group1 Group2 Group3 ## A 0.059 0.070 0.007 ## B 0.275 0.628 0.933 ## C 0.425 0.264 0.052 ## D 0.242 0.039 0.007 ## ## $Alg6 ## score.level ## response Group1 Group2 Group3 ## A 0.359 0.225 0.022 ## B 0.183 0.062 0.007 ## C 0.137 0.085 0.015 ## D 0.320 0.628 0.956 ## ## $Geo1 ## score.level ## response Group1 Group2 Group3 ## A 0.085 0.016 0.000 ## B 0.039 0.000 0.000 ## C 0.529 0.868 1.000 ## D 0.346 0.116 0.000 ## ## $Geo2 ## score.level ## response Group1 Group2 Group3 ## A 0.373 0.767 0.941 ## B 0.464 0.171 0.044 ## C 0.150 0.062 0.015 ## D 0.013 0.000 0.000 ## ## $Geo3 ## score.level ## response Group1 Group2 Group3 ## A 0.020 0.000 0.000 ## B 0.516 0.837 0.956 ## C 0.353 0.147 0.037 ## D 0.111 0.016 0.007 ## ## $Geo4 ## score.level ## response Group1 Group2 Group3 ## A 0.451 0.806 0.985 ## B 0.458 0.178 0.015 ## C 0.059 0.008 0.000 ## D 0.033 0.008 0.000 ## ## $Geo5 ## score.level ## response Group1 Group2 Group3 ## A 0.248 0.039 0.015 ## B 0.065 0.008 0.000 ## C 0.046 0.008 0.000 ## D 0.641 0.946 0.985 ## ## $Geo6 ## score.level ## response Group1 Group2 Group3 ## A 0.059 0.047 0.007 ## B 0.020 0.008 0.000 ## C 0.634 0.853 0.993 ## D 0.288 0.093 0.000 También podemos presentar únicamente la tabla más estilizada de cualquiera de los reactivos con el siguiente comando, solamente tenemos que reemplazar el número entre corchetes por el número correspondiente al reactivo de nuestro interés kbl(distractores[[1]])%&gt;% kable_paper(&quot;hover&quot;, full_width = F) Group1 Group2 Group3 A 0.569 0.845 0.993 B 0.268 0.109 0.000 C 0.124 0.008 0.007 D 0.039 0.039 0.000 Si identificamos algún reactivo que nos llame la atención por el comportamiento de sus distractores, podemos analizarlo gráficamente con el siguiente comando: datos_completos%&gt;% select(names(reactivos_correctos))%&gt;% plotDistractorAnalysis(key = cadena_respuestas, item=4, item.name= &quot;Cálculo 4&quot;) Si queremos ver la correlación de los distractores con el puntaje total, lo podemos hacer con el siguiente comando reactivos_completos &lt;- datos_completos%&gt;% select(colnames(reactivos_correctos)) examen1&lt;-reactivos_completos %&gt;% mutate(row = row_number()) %&gt;% pivot_longer(cols = -row) %&gt;% mutate(val = 1) %&gt;% group_by(row) %&gt;% complete(name, value, fill = list(val = 0)) %&gt;% arrange(name, value) %&gt;% pivot_wider(names_from = c(name, value), values_from = val, names_sep = &#39;.&#39;, values_fill = 0) %&gt;% ungroup %&gt;% arrange(row) %&gt;% select(-row) examen1 &lt;- examen1%&gt;% mutate(Total = rowSums(reactivos_correctos, na.rm = TRUE)) correl1 &lt;- cor(examen1) cor&lt;- as.vector(correl1[ncol(examen1),1:ncol(examen1)-1]) cor_total &lt;- as.data.frame(matrix(cor, nrow = 4))%&gt;% setNames(colnames(reactivos_correctos))%&gt;% t() kbl((cor_total)) %&gt;% kable_classic() %&gt;% add_header_above(c(&quot;&quot;,&quot; A&quot;, &quot; B&quot;, &quot; C&quot;, &quot; D&quot;)) A B C D Calc1 -0.3369282 -0.1738727 0.5259912 -0.3199584 Calc2 -0.3147886 -0.2993869 -0.1197667 0.5285089 Calc3 -0.4107848 -0.1892840 0.6064053 -0.3973809 Calc4 0.6374808 -0.4761574 -0.3051633 -0.2286052 Calc5 -0.1080693 0.5535004 -0.3461032 -0.3230592 Calc6 -0.3293887 -0.3149080 -0.1504287 0.5453402 Alg1 0.5838510 -0.3854295 -0.3769662 -0.1521265 Alg2 -0.3663339 -0.3317779 -0.3054610 0.5885848 Alg3 -0.3339129 -0.1681985 0.5722226 -0.4035162 Alg4 -0.1701431 0.6070416 -0.4728389 -0.2877867 Alg5 0.5391035 -0.4639064 -0.2855065 -0.0415687 Alg6 -0.2346653 0.5969445 -0.3887874 -0.3450808 Geo1 -0.2952885 -0.1780219 0.5657538 -0.4212370 Geo2 0.5844247 -0.4315585 -0.2940830 -0.1433994 Geo3 -0.1153404 0.5233467 -0.3672995 -0.3333619 Geo4 0.6335290 -0.5043415 -0.2495189 -0.2543514 Geo5 -0.3925212 -0.3045590 -0.1770732 0.5406796 Geo6 -0.1819230 -0.1275161 0.5057479 -0.4336853 6.5 Dimensionalidad La siguiente sección es referente al análisis de dimensionalidad de nuestro instrumento, para llevar esto a cabo, recordemos que contamos con dos estrategias principales: Análisis Factorial Exploratorio y Análisis Factorial Confirmatorio. 6.5.1 Análisis Factorial Exploratorio Para realizar el AFE, necesitamos seguir una serie de pasos que presentaremos a continuación. Un primer paso importante es explorar cómo se relacionan los reactivos de nuestro examen. Para hacerlo, es más sencillo analizarlo de manera gráfica. Si ejecutamos el siguiente comando podremos visualizar la relación entre cada uno de los reactivos. Se trata de una matriz de correlaciones, en la que se cruzan los valores de los reactivos de manera horizontal y vertical. En el ejemplo, donde vemos el cruce del reactivo Calc1 y Calc2, se señala la relación que existe entre ambos reactivos. Esta gráfica, en lugar de presentar los valores, presenta un color específico que denota la intensidad de la relación entre ambos, lo cual se ve explicado en la barra lateral derecha. corrplot(cor(reactivos_correctos), method = &quot;circle&quot;, na.rm=T, sig.level = 0.05) ## Warning in text.default(pos.xlabel[, 1], pos.xlabel[, 2], newcolnames, srt = ## tl.srt, : &quot;na.rm&quot; is not a graphical parameter ## Warning in text.default(pos.ylabel[, 1], pos.ylabel[, 2], newrownames, col = ## tl.col, : &quot;na.rm&quot; is not a graphical parameter ## Warning in title(title, ...): &quot;na.rm&quot; is not a graphical parameter En este gráfico, ya podemos comenzar a ver qué tanto se relacionan los reactivos entre sí, y nos dará pistas para comenzar a identificar posibles factores, entendidos como los posibles grupos de reactivos que se relacionan más entre sí. El siguiente pasó a revisar será ver qué tan factible es llevar a cabo un AFE de acuerdo con la variabilidad de nuestra muestra y con las correlaciones de nuestros reactivos. En esencia, los dos valores que obtendremos al correr las siguientes líneas de código, nos dirán si nuestra muestra tiene suficiente variabilidad en los puntajes y los reactivos tienen una relación lo suficientemente significativa como para llevar a cabo un AFE. cortest.bartlett(reactivos_correctos) ## R was not square, finding R from data ## $chisq ## [1] 2457.232 ## ## $p.value ## [1] 0 ## ## $df ## [1] 153 KMO(reactivos_correctos) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = reactivos_correctos) ## Overall MSA = 0.89 ## MSA for each item = ## Calc1 Calc2 Calc3 Calc4 Calc5 Calc6 Alg1 Alg2 Alg3 Alg4 Alg5 Alg6 Geo1 ## 0.92 0.89 0.86 0.90 0.89 0.92 0.85 0.86 0.88 0.89 0.87 0.84 0.92 ## Geo2 Geo3 Geo4 Geo5 Geo6 ## 0.91 0.89 0.91 0.87 0.84 La Prueba de Bartlett es una prueba de significancia, por lo que esperamos que el valor de p p.value sea menor a 0.05. Cualquier valor mayor a esto nos indicaría que no es pertinente elaborar un AFE. El análisis de KMO, arroja valores de 0 a 1, siendo los valores más cercanos a 1 los que nos indican una mayor adecuación de nuestros datos para la ejecución de un AFE. El criterio de definición suele ser 0.8. Es decir, esperamos que nuestros reactivos presenten un KMO mayor a 0.8 para poder identificar que es viable incluirlos en el AFE. El siguiente paso es determinar el número de factores que vamos a extraer del AFE. Para ello, ejecutaremos la siguiente función que nos mostrará una aproximación de la cantidad ideal de factores a extraer. Este comando nos dirá directamente la cantidad de factores a extraer, y a su vez, nos presentará un gráfico en el que se puede observar la mejoría en la varianza explicada en relación con el aumento de factores. Es decir, hacia la derecha, cada punto representa la cantidad de factores, y se puede ver qué tanta varianza explica cada uno de los factores. Como se puede ver, cada factor que se añade explica menos cantidad de varianza, por lo que es importante determinar en qué punto la cantidad de varianza que explica la inclusión de un nuevo factor es tan insignificante que no resulta necesario incluir ese factor. El gráfico nos ayuda a determinarlo al trazar la línea roja punteada, en este ejemplo podemos ver que el factor que ya no añade suficiente varianza es el 5, y por ello es que el análisis recomienda 4 factores. fa.parallel(reactivos_correctos, fm = &quot;uls&quot;, fa = &quot;fa&quot;) ## Parallel analysis suggests that the number of factors = 4 and the number of components = NA Una segunda opción es el análisis vss, que realiza un procedimiento similar mediante otros métodos, lo cual nos sirve como una segunda opinión o método a considerar para decidir la cantidad de factores. Este análisis nos dará varios criterios referentes a la cantidad ideal de factores, por ejemplo, presenta en primer lugar el VSS complexity 1, y nos indica la cantidad de factores que ofrece un mejor resultado. Otro criterio que se suele tomar en cuenta es el Velicer MAP o el BIC. Idealmente, todos estos criterios deberían mostrar la misma cantidad de factores, pero cuando no es así, dependerá de nosotros seleccionar los criterios que consideremos más adecuados según nuestro caso particular.De igual forma, será necesario que respaldemos la decisión del número de factores con evidencia. vss(reactivos_correctos) ## ## Very Simple Structure ## Call: vss(x = reactivos_correctos) ## VSS complexity 1 achieves a maximimum of 0.74 with 1 factors ## VSS complexity 2 achieves a maximimum of 0.81 with 2 factors ## ## The Velicer MAP achieves a minimum of 0.02 with 2 factors ## BIC achieves a minimum of -350.11 with 3 factors ## Sample Size adjusted BIC achieves a minimum of -85.83 with 6 factors ## ## Statistics by number of factors ## vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex ## 1 0.74 0.00 0.023 135 801 3.4e-95 11.8 0.74 0.109 -13 415 1.0 ## 2 0.58 0.81 0.018 118 443 4.4e-39 8.6 0.81 0.081 -269 105 1.4 ## 3 0.54 0.77 0.018 102 265 1.8e-16 6.8 0.85 0.062 -350 -26 1.4 ## 4 0.52 0.72 0.022 87 186 3.4e-09 5.9 0.87 0.052 -339 -63 1.6 ## 5 0.47 0.71 0.028 73 123 2.1e-04 5.3 0.89 0.041 -317 -85 1.7 ## 6 0.46 0.70 0.035 60 86 1.6e-02 4.8 0.90 0.032 -276 -86 1.7 ## 7 0.44 0.69 0.043 48 54 2.5e-01 4.3 0.91 0.017 -235 -83 1.8 ## 8 0.43 0.65 0.055 37 36 5.0e-01 3.8 0.92 0.000 -187 -69 1.8 ## eChisq SRMR eCRMS eBIC ## 1 1083 0.092 0.098 268 ## 2 458 0.060 0.068 -254 ## 3 199 0.039 0.048 -417 ## 4 115 0.030 0.040 -409 ## 5 72 0.024 0.034 -368 ## 6 50 0.020 0.032 -312 ## 7 27 0.015 0.026 -263 ## 8 18 0.012 0.024 -206 El análisis VSS recomienda un solo factor, probablemente debido a que todos los reactivos son referentes a matemáticas. Sin embargo, el MAP recomienda dos factores, y el BIC recomienda tres. De manera interesante, ninguno de los valores del análisis VSS recomienda cuatro factores como el anterior. Debido a que nosotros estimamos que contamos con tres factores dado que así se construyó el examen, y el criterio BIC respalda esa decisión, haremos los análisis con estos tres factores. Sin embargo, cabe aclarar que las discrepancias entre estos métodos nos pueden estar indicando que la estructura de nuestro examen es ambigua. Una vez que hemos determinado la cantidad de factores, podemos pasar a ejecutar el siguiente código para obtener el AFE. El aspecto principal que tenemos que tomar en cuenta es la rotación. Este es un concepto con bases matemáticas, que en esencia se refiere a la forma en que el análisis puede determinar las cargas factoriales. Existen dos tipos principales, las ortogonales y las oblicuas. En las primeras se asume que los factores que obtengamos no se relacionan entre sí, mientras que en las segundas, esto sí es permitido. Dentro de las ortogonales la más utilizada es la varimax, mientras que entre las oblicuas la más utilizada es la oblimin. En este caso, decidimos utilizar promax, que es un método con la rigurosidad de varimax pero que permite la relación entre los factores en un segundo paso. Nota: Si al lector le interesa profundizar en esto, recomendamos consultar el artículo de Lloret-Segura et al. (2014), el cuál presenta una guía práctica para la elaboración de AFE en español. AFE &lt;- factanal(reactivos_correctos, 3, rotation=&quot;promax&quot;) print(AFE$loadings, digits=2, cutoff=.3, sort=TRUE) ## ## Loadings: ## Factor1 Factor2 Factor3 ## Calc1 0.60 ## Calc2 0.72 ## Calc3 0.58 ## Calc4 0.81 ## Calc5 0.74 ## Calc6 0.63 ## Geo1 0.56 ## Geo2 0.64 ## Geo3 0.59 ## Geo4 0.68 ## Geo5 0.52 ## Geo6 0.67 ## Alg3 0.57 ## Alg4 0.67 ## Alg5 0.73 ## Alg6 0.73 ## Alg1 0.43 ## Alg2 0.39 ## ## Factor1 Factor2 Factor3 ## SS loadings 2.87 2.35 2.22 ## Proportion Var 0.16 0.13 0.12 ## Cumulative Var 0.16 0.29 0.41 Podemos observar que en las filas se enlistan todos los reactivos, y en las columnas los 4 factores que obtuvimos en este caso. El valor que se muestra en cada cruce de ambos es la carga factorial de ese reactivo en ese factor dado. 6.5.1.1 Interpretación Lo que esperamos en este caso es que cada reactivo tenga una carga significativa en uno solo de los factores. Por lo general, si observamos que un reactivo se agrupa en dos factores, esto es un indicador de que dicho reactivo está midiendo dos cosas a la vez, lo cual no es algo que estemos buscando. Será importante también evaluar el valor que presente cada reactivo en la carga factorial (el valor que aparece en el cruce entre el reactivo y el factor). Algunos autores señalan como criterio para definir si el reactivo se agrupa en el factor correspondiente, que la carga factorial sea mayor a .4. De igual forma, si un reactivo carga en dos factores, podemos decantarnos por darle prioridad al factor en el que tenga una carga factorial más alta, de preferencia, si la diferencia entre ambas cargas factoriales es mayor a .2. Es importante destacar que habrá ocasiones en las que algunos reactivos no tengan una carga significativa en ningún factor, o que carguen de manera muy similar en dos factores distintos. En estos casos, esto ya es un indicio de que el reactivo no se está comportando como esperaríamos, sin embargo, podemos intentar utilizar otros métodos de rotación para ver cuál funciona mejor con nuestros datos y si existe mejoría con estos reactivos. Tal como hemos venido mencionando, para eliminar un reactivo, debemos tomar en cuenta toda la evidencia en su conjunto, y no solamente la correspondiente a un solo análisis, como en este caso. Una vez que hemos identificado las cargas factoriales y agrupado cada reactivo en un solo factor, es momento de identificar qué representa cada uno de estos factores con base en los reactivos que se agruparon en ellos. En el ejemplo de arriba, los reactivos se agruparon de la manera que se esperaba, todos los de Cálculo juntos, todos los de Algebra también y lo mismo para los de Geometría, por lo que podemos definir que esos tres son los factores que encontramos. Sin embargo, no siempre será así, en ocasiones, es posible que reactivos que contemplamos medían una cosa, se agrupen en un factor distinto, por lo que sería importante revisar dicho reactivo y considerar si de verdad está midiendo lo que esperábamos que midiera. En ocasiones, al observar la agrupación de los reactivos, y basados en nuestra revisión bibliográfica, podemos determinar que esos reactivos agrupados en un factor, en realidad están midiendo un constructo distinto. Pero para esto, hace falta un procedimiento sumamente riguroso, sistemático y basado en evidencias, tanto del AFE como de la literatura. 6.5.2 Análisis Factorial Confirmatorio Para llevar a cabo el análisis factorial confirmatorio, como ya se mencionó, nosotros definimos el modelo que queremos poner a prueba. Para lograrlo, utilizamos la sintaxis que se presenta en el cuadro de código siguiente. modelo &lt;- &#39; Calculo=~ Calc1 + Calc2 + Calc3 + Calc4 + Calc5 + Calc6 Algebra=~ Alg1 + Alg2 + Alg3 + Alg4 + Alg5 + Alg6 Geometria=~ Geo1 + Geo2 + Geo3 + Geo4 + Geo5 + Geo6 &#39; Lo primero que hacemos es ponerle un nombre a nuestro modelo, dado que en este ejemplo no haremos comparación de modelos, simplemente llamamos al que presentamos modelo. Si el lector tuviera que comparar distintos modelos, sería importante nombrar a cada uno de manera sencilla y que permita identificarlos fácilmente. Después de nombrarlo, utilizamos el signo de asignación en R &lt;- y abrimos comillas simples. La sintaxis dentro del espacio de comillas es muy simple. Primero definimos la variable latente, es decir, aquello que suponemos que nuestros reactivos están midiendo. En el ejemplo que estamos utilizando, todos los reactivos que comienzan con Calc, fueron diseñados pensando en que midieran Cálculo, por lo que podemos llamar a ese factor así. Una vez definido el nombre del factor, se incluye el símbolo =~ que nos indica que ese factor está compuesto por lo que sigue. Por ello, después de ese signo, se enlistan todos los reactivos que se incluyen en dicho factor, separados cada uno por un signo de +. Después del último reactivo, iniciamos en una nueva línea y repetimos el mismo proceso para cada factor, y al terminar simplemente cerramos las comillas simples. Es importante destacar que los reactivos dentro del modelo deben tener exactamente el mismo nombre que tienen como variables dentro de nuestra base de datos. Una vez definido el modelo, solamente tenemos que ejecutar las siguientes líneas de código para obtener el resumen de los resultados de dicho modelo en nuestra muestra. Solamente necesitamos poner dentro de la función afc el nombre de nuestro modelo, el nombre de la base de datos con nuestros reactivos y elegir el método de estimación de parámetros con el indicador estimator =. Para el contexto de exámenes con una sola respuesta correcta por reactivo, algunos autores recomiendan utilizar el método ULS por lo que es el que dejamos por defecto en esta línea de código, pero existen otras opciones como los métodos de Máxima Verosimilitud o los de WLS. El resultado de estas líneas de código será un resumen amplio de cómo se comporta el modelo, mostrando las cargas factoriales de cada reactivo y algunos otros estadísticos. Sin embargo, para fines prácticos de este manual, nos concentraremos en los resultados ofrecidos por la última línea de código, la cual nos muestra una serie de índices de bondad de ajuste del modelo. ajuste &lt;- cfa(modelo, data=reactivos_correctos ,std.lv= TRUE, estimator = &quot;ULS&quot;, auto.fix.first = FALSE, sample.nobs=417) #Resultados de dicho modelo summary(ajuste,fit.measures=T,standardized=T) ## lavaan 0.6-8 ended normally after 23 iterations ## ## Estimator ULS ## Optimization method NLMINB ## Number of model parameters 39 ## ## Number of observations 417 ## ## Model Test User Model: ## ## Test statistic 4.461 ## Degrees of freedom 132 ## P-value (Unknown) NA ## ## Model Test Baseline Model: ## ## Test statistic 167.698 ## Degrees of freedom 153 ## P-value NA ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 11.058 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.044 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Calculo =~ ## Calc1 0.275 0.068 4.070 0.000 0.275 0.677 ## Calc2 0.236 0.066 3.606 0.000 0.236 0.703 ## Calc3 0.231 0.065 3.540 0.000 0.231 0.672 ## Calc4 0.278 0.068 4.097 0.000 0.278 0.726 ## Calc5 0.229 0.065 3.509 0.000 0.229 0.634 ## Calc6 0.278 0.068 4.093 0.000 0.278 0.691 ## Algebra =~ ## Alg1 0.255 0.062 4.094 0.000 0.255 0.558 ## Alg2 0.272 0.063 4.311 0.000 0.272 0.548 ## Alg3 0.264 0.063 4.204 0.000 0.264 0.686 ## Alg4 0.292 0.064 4.552 0.000 0.292 0.733 ## Alg5 0.301 0.065 4.661 0.000 0.301 0.613 ## Alg6 0.292 0.064 4.551 0.000 0.292 0.601 ## Geometria =~ ## Geo1 0.249 0.063 3.948 0.000 0.249 0.608 ## Geo2 0.295 0.066 4.486 0.000 0.295 0.631 ## Geo3 0.234 0.062 3.743 0.000 0.234 0.545 ## Geo4 0.311 0.067 4.654 0.000 0.311 0.703 ## Geo5 0.208 0.061 3.395 0.001 0.208 0.578 ## Geo6 0.207 0.061 3.376 0.001 0.207 0.536 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Calculo ~~ ## Algebra 0.508 0.131 3.878 0.000 0.508 0.508 ## Geometria 0.607 0.152 4.003 0.000 0.607 0.607 ## Algebra ~~ ## Geometria 0.631 0.141 4.490 0.000 0.631 0.631 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .Calc1 0.090 0.062 1.455 0.146 0.090 0.541 ## .Calc2 0.057 0.058 0.984 0.325 0.057 0.505 ## .Calc3 0.065 0.058 1.125 0.261 0.065 0.548 ## .Calc4 0.069 0.062 1.117 0.264 0.069 0.472 ## .Calc5 0.078 0.057 1.357 0.175 0.078 0.598 ## .Calc6 0.084 0.062 1.362 0.173 0.084 0.522 ## .Alg1 0.144 0.058 2.465 0.014 0.144 0.688 ## .Alg2 0.173 0.060 2.885 0.004 0.173 0.700 ## .Alg3 0.078 0.059 1.322 0.186 0.078 0.529 ## .Alg4 0.073 0.062 1.186 0.235 0.073 0.462 ## .Alg5 0.150 0.063 2.404 0.016 0.150 0.624 ## .Alg6 0.151 0.062 2.445 0.014 0.151 0.639 ## .Geo1 0.106 0.058 1.820 0.069 0.106 0.630 ## .Geo2 0.132 0.063 2.105 0.035 0.132 0.602 ## .Geo3 0.129 0.057 2.267 0.023 0.129 0.703 ## .Geo4 0.099 0.064 1.539 0.124 0.099 0.506 ## .Geo5 0.087 0.055 1.569 0.117 0.087 0.666 ## .Geo6 0.106 0.055 1.928 0.054 0.106 0.713 ## Calculo 1.000 1.000 1.000 ## Algebra 1.000 1.000 1.000 ## Geometria 1.000 1.000 1.000 fitmeasures(object = ajuste,fit.measures = c(&quot;chisq&quot;,&quot;df&quot;,&quot;cfi&quot;,&quot;tli&quot;,&quot;rmsea&quot;, &quot;rmsea.ci.upper&quot;,&quot;rmsea.ci.lower&quot;)) ## chisq df cfi tli rmsea ## 4.461 132.000 1.000 11.058 0.000 ## rmsea.ci.upper rmsea.ci.lower ## 0.000 0.000 Los índices de bondad de ajuste más utilizados suelen ser dos: el CFI y el RMSEA. Existen variaciones de ambos e índices similares, pero para fines prácticos, en este manual nos concentraremos en esos dos. El CFI es un índice de la bondad de ajuste, que en esencia nos dice qué tan bien se ajustaron los datos al modelo propuesto; es decir, qué tanto los datos que tenemos de nuestra muestra realmente se están comportando como el modelo predice en términos de que los reactivos sí se agrupen como nosotros definimos que lo harían. EL CFI es un estadístico que toma valores de 0 a 1, siendo los valores más cercanos a 1 indicadores de un buen nivel de ajuste. Algunos autores suelen definir como criterio de decisión un valor mínimo de .90 para indicar un buen ajuste y un valor de .96 para indicar un ajuste excelente. Por su parte, el RMSEA, es una medida del error presente en el modelo, nos indica los residuales de aquello que el modelo no fue capaz de explicar de nuestros datos. Por ello, lo que esperamos es obtener un valor muy pequeño de este estadístico. En específico, algunos autores recomiendan como criterio de decisión valores menores a .06 como índice de un buen ajuste. 6.6 Confiabilidad Como se mencionó en el capítulo anterior, existen diversos tipos de confiabilidad, los cuales se obtienen con diferentes métodos. Sin embargo, la más usada usualmente, debido a su practicidad es la confiabilidad de consistencia interna, en especial el coeficiente Alfa de Cronbach. Para obtenerlo, solamente tenemos que utilizar la función cronbach() e indicar dentro de esta el nombre de la base de datos en donde se encuentran únicamente nuestros reactivos. cronbach(reactivos_correctos) ## $sample.size ## [1] 417 ## ## $number.of.items ## [1] 18 ## ## $alpha ## [1] 0.8738393 Este comando nos arroja tres valores específicos; aunque los primeros dos ya los conocemos (el tamaño de la muestra y el numero de reactivos), es importante verificar que están correctamente capturados. Una vez que lo hemos revisado, el dato que nos importa es el coeficiente Alfa que aparece en el último renglón. Recordaremos que este valor puede ir de 0 a 1, y los valores más cercanos a 1 suelen indicar una mejor confiabilidad. cronbach(reactivos_correctos) ## $sample.size ## [1] 417 ## ## $number.of.items ## [1] 18 ## ## $alpha ## [1] 0.8738393 Ahora bien, hasta aquí hemos omitido aclarar un punto de suma importancia en este análisis. Recordemos que en el apartado anterior llevamos a cabo el análisis de la dimensionalidad, en el que pudimos comprobar si nuestro examen presenta las dimensiones que presuponíamos desde un inicio. Si obtuvimos un examen multidimensional, como en el ejemplo que hemos utilizado aquí, es importante que analicemos la confiabilidad de cada una de las dimensiones. Dado que los reactivos se agrupan de esa forma, nosotros interpretamos que se agrupan así porque miden un mismo constructo en cada dimensión y por lo tanto, tendríamos que obtener puntajes de cada dimensión; es decir, si nuestro examen presenta varias dimensiones, tenemos que interpretar cada dimensión por separado, obteniendo un puntaje total para cada una con su respectivo coeficiente de confiabilidad. Para obtener este coeficiente de confiabilidad, simplemente tenemos que generar una base de datos para cada dimensión que contenga exclusivamente los reactivos correspondientes. En las siguientes líneas de código presentamos un ejemplo de ello. Al igual que cuando generamos la base de datos total, aquí solamente tenemos que poner el nombre del primer reactivo de la dimensión seguido de : y el nombre del último reactivo de la dimensión, todo esto dentro de la función select(). Ya que lo hemos hecho, simplemente ejecutamos nuevamente la línea cronbach() con el nombre de nuestra nueva base de datos. calculo &lt;- reactivos_correctos%&gt;% select(Calc1:Calc6) cronbach(calculo) ## $sample.size ## [1] 417 ## ## $number.of.items ## [1] 6 ## ## $alpha ## [1] 0.8399005 Si queremos interpretar la suma de las distintas dimensiones como un puntaje total, necesitamos aportar evidencia de que esta interpretación es válida. Ya sea por evidencia obtenida en la revisión para el marco conceptual o por estudios estadísticos, debemos presentar evidencia de que es posible utilizar la suma de cada dimensión como un puntaje significativo e interpretable. En caso de que nuestro examen presente una sola dimensión, podemos simplemente proceder con la primer línea de código referente al Alfa de Cronbach general. Como mencionamos en el componente 5, otra opción es el uso del coeficiente Omega, el cuál nos brinda el análisis de confiabilidad basado en la estructura de dimensionalidad que obtuvimos en el AFC, con los siguientes comandos podemos obtener los coeficiente de omega de cada dimensión. omega&lt;-omegaFromSem(ajuste) omega$omega.group$total ## [1] 0.84072092 0.09652765 0.13441104 6.7 Imparcialidad Para poder presentar evidencia de que el examen no da ventaja a ningún grupo en específico, podríamos comenzar con un análisis de varianza (ANOVA). Este análisis, nos permite identificar si existen diferencias en los promedios de puntaje total del examen entre los grupos que queramos evaluar. De hecho, un primer paso para llevar a cabo este análisis es visualizar gráficamente cómo se distribuyen los puntajes por grupos. Para ello, podemos utilizar las siguientes líneas de código: datos_completos &lt;- datos_completos%&gt;% mutate(across(-names(reactivos_correctos), factor)) reactivos_total &lt;- datos_completos%&gt;% mutate(Total = rowSums(reactivos_correctos)) reactivos_total%&gt;% ggplot(aes(x = Sexo, y = Total, col = Sexo)) + geom_boxplot() + scale_x_discrete(labels = c(&quot;Mujeres&quot;, &quot;Hombres&quot;)) Como podemos observar, en este ejemplo, no parecen existir diferencias significativas entre ambos grupos. La línea gruesa que vemos atraviesa de forma horizontal la gráfica es la media de cada grupo, y la caja como tal es una manera gráfica de representar cómo se distribuyen aproximadamente los puntajes de los evaluados. Pese a que en cuanto a las medias no parece haber diferencias, pareciera haber mayor proporción de puntajes bajos en los hombres, así que sería importante también hacer el análisis de varianza. Podemos hacerlo con las siguientes líneas de código: anova_genero&lt;-aov(Total ~ Sexo, data= reactivos_total) pander(anova_genero) Analysis of Variance Model   Df Sum Sq Mean Sq F value Pr(&gt;F) Sexo 1 18.78 18.78 1.052 0.3057 Residuals 415 7411 17.86 NA NA La información que más nos interesa es la última columna que aparece como Pr(&gt;F). Este valor es la significancia, y nos dirá si las posibles diferencias observadas entre ambos grupos son estadísticamente significativas. En educación y en ciencias sociales, el criterio de decisión lo solemos establecer en .05. Es decir, si el valor de esta columna es menor .05, podemos decir que las diferencias son significativas; en caso contrario, podemos decir que no existen diferencias entre ambos grupos. En este ejemplo, no parece haber diferencias significativas en el total. Sin embargo, esto no nos garantiza que el examen y los reactivos estén siendo imparciales. (ver nota). Para evaluar de manera más profunda y precisa la presencia de sesgos que atenten contra la imparcialidad, la herramienta más utilizada en este contexto es el Análisis del Funcionamiento Diferencial del Reactivo (DIF por sus siglas en inglés). Inclusive, aunque hayan existido diferencias significativas entre los grupos, eso no significa que esas diferencias sean debidas a que el examen está sesgado. De hecho, como se mencionó en el capítulo anterior, al igual que en el siguiente análisis, estas diferencias podrían ser explicadas por diferencias existentes reales entre ambos grupos. El análisis DIF nos indica la presencia de sesgos en los reactivos que favorezcan más a algún grupo con respecto a otro independientemente de su nivel de habilidad. Esto es, si dos sustentantes tienen el mismo nivel de habilidad, pero uno de ellos por pertenecer a un grupo distinto tiene mayor probabilidad de contestar de manera correcta dicho reactivo. Nota Existen muchos métodos distintos para detectar la presencia de DIF en cada reactivo, algunos que parten de la IRT y algunos que parten de otras teorías. Para fines prácticos, en este análisis incluiremos la comparativa de dos modelos de manera simplificada, pero si al lector le interesa profundizar en este aspecto, recomendamos revisar el capítulo de Penfield and Camilli (2006). Para llevar a cabo el análisis, simplemente tenemos que ejecutar la siguiente línea de código. En el campo de Data, pondremos el nombre de nuestra base de datos que solamente incluye los reactivos en formato de 0-1. En el campo de group, incluiremos la variable que define los grupos que queremos contrastar; en este caso, haremos el análisis en términos de género. Recordaremos, que esa variable estaba presente en la primera base de datos que cargamos en nuestro ejemplo, por lo que podemos referenciarla poniendo el nombre de la base de datos donde se encuentra seguido del signo $ y el nombre de la varible. El último componente que debemos específicar es el grupo focal. Para definir esto, es importante que tomemos en consideración cuál es el grupo que consideramos parte con desventaja. Esto puede estar basado en experiencias previas, en revisión de la literatura, o inclusive, en los análisis aquí realizados. Como podemos ver en el gráfico de cajas, pese a que no existen diferencias significativas, los hombres presentan una mayor distribución en puntajes bajos, así que, para fines didácticos, podemos asumir que son los hombres quienes parten con desventaja. El valor que ponemos en este caso es 1, que recordaremos es el que asignamos en un inicio a los hombres. MH&lt;- difMH(Data=reactivos_correctos, group=datos_completos$Sexo, focal.name= 1, purify=TRUE) MH ## ## Detection of Differential Item Functioning using Mantel-Haenszel method ## with continuity correction and with item purification ## ## Results based on asymptotic inference ## ## Convergence reached after 7 iterations ## ## Matching variable: test score ## ## No set of anchor items was provided ## ## No p-value adjustment for multiple comparisons ## ## Mantel-Haenszel Chi-square statistic: ## ## Stat. P-value ## Calc1 3.5944 0.0580 . ## Calc2 0.0816 0.7751 ## Calc3 1.3360 0.2477 ## Calc4 0.0065 0.9359 ## Calc5 0.0151 0.9023 ## Calc6 0.0002 0.9902 ## Alg1 4.1501 0.0416 * ## Alg2 2.7749 0.0958 . ## Alg3 0.0800 0.7772 ## Alg4 4.9948 0.0254 * ## Alg5 1.8603 0.1726 ## Alg6 2.0798 0.1493 ## Geo1 0.5980 0.4393 ## Geo2 0.0000 0.9981 ## Geo3 0.0080 0.9289 ## Geo4 0.0242 0.8763 ## Geo5 9.0235 0.0027 ** ## Geo6 4.5712 0.0325 * ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Detection threshold: 3.8415 (significance level: 0.05) ## ## Items detected as DIF items: ## ## Alg1 ## Alg4 ## Geo5 ## Geo6 ## ## ## Effect size (ETS Delta scale): ## ## Effect size code: ## &#39;A&#39;: negligible effect ## &#39;B&#39;: moderate effect ## &#39;C&#39;: large effect ## ## alphaMH deltaMH ## Calc1 1.9762 -1.6008 C ## Calc2 0.8399 0.4099 A ## Calc3 1.6222 -1.1368 B ## Calc4 1.0312 -0.0723 A ## Calc5 1.0158 -0.0369 A ## Calc6 1.0453 -0.1041 A ## Alg1 1.8083 -1.3921 B ## Alg2 1.5236 -0.9895 A ## Alg3 0.8668 0.3360 A ## Alg4 0.4496 1.8788 C ## Alg5 0.6849 0.8893 A ## Alg6 0.6789 0.9099 A ## Geo1 0.7557 0.6583 A ## Geo2 0.9655 0.0824 A ## Geo3 0.9377 0.1511 A ## Geo4 1.0887 -0.1998 A ## Geo5 0.3118 2.7385 C ## Geo6 0.5016 1.6216 C ## ## Effect size codes: 0 &#39;A&#39; 1.0 &#39;B&#39; 1.5 &#39;C&#39; ## (for absolute values of &#39;deltaMH&#39;) ## ## Output was not captured! Este primer análisis se basa en modelos no IRT. Lo que nos interesa es la sección titulada items detected as DIF items, dado que en ella se presentan específicamente qué reactivos fueron detetados con DIF significatvio. Por otr lado, también nos interesa la tabla que aparece al final, donde se presenta el nombre de cada reactivo seguido por dos datos numéricos y una clasificación de A, B y C; dicha clasificación nos indica si la presencia de DIF es insignificativa A, moderada B o grande C. Para verificar y contrastar estos resultados, podemos utilizar un modelo más basado en la TRI. Cabe resaltar, que al igual que en los análisis que ya hicimos referentes a la TRI, es importante considerar el tamaño de nuestra muestra, si contamos con una muestra pequeña, recomendamos quedarnos únicamente con el primer análisis. En este modelo TRI, los campos a especificar son los mismos, el único que se añade es si el modelo de la TRI será de 1 1PL, 2 2PL o tres parámetros 3PL. Lord&lt;- difLord(Data=reactivos_correctos, group=datos_completos$Sexo, focal.name= 1, model=&quot;2PL&quot;, purify=TRUE) Lord ## ## Detection of Differential Item Functioning using Lord&#39;s method ## with 2PL model and with item purification ## ## Engine &#39;ltm&#39; for item parameter estimation ## ## Convergence reached after 1 iteration ## ## No set of anchor items was provided ## ## No p-value adjustment for multiple comparisons ## ## Lord&#39;s chi-square statistic: ## ## Stat. P-value ## Calc1 3.6082 0.1646 ## Calc2 2.6574 0.2648 ## Calc3 1.8497 0.3966 ## Calc4 2.3062 0.3157 ## Calc5 0.8880 0.6415 ## Calc6 0.1602 0.9230 ## Alg1 6.7892 0.0336 * ## Alg2 3.9465 0.1390 ## Alg3 0.0213 0.9894 ## Alg4 3.7272 0.1551 ## Alg5 2.2627 0.3226 ## Alg6 2.4758 0.2900 ## Geo1 2.8509 0.2404 ## Geo2 0.0055 0.9972 ## Geo3 0.9319 0.6275 ## Geo4 0.2367 0.8884 ## Geo5 7.5089 0.0234 * ## Geo6 4.0901 0.1294 ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Detection threshold: 5.9915 (significance level: 0.05) ## ## Items detected as DIF items: ## ## Alg1 ## Geo5 ## ## Output was not captured! En este caso, el resultado se presenta al revés, dado que primero aparece la tabla con la significancia del efecto DIF. En este método, en lugar de presentarse con clasificaicón ABC, se presenta con asteríscos, siendo que los reactivos con presencia de DIF considerable aparecen con un asterísco junto a ellos, si aparecen más de un asterísco, quiere decir que el nivel de significancia del efecto DIF es mayor. Como podemos ver, utilizar estos dos métodos nos permite contrastar la presencia de DIF en los reactivos, y ver si es consistente en ambos modelos. En este ejemplo, podemos observar que el reactivo 1 de algebra (el reactivo 7 en el orden del total de reactivos) presenta DIF en los dos métodos, por lo que resulta importante indagar más en cuál está siendo su comportamiento. Para ello, una forma útil de hacerlo es analizarlo gráficamente, si corremos el siguiente código, podremos visualizar cómo se comporta la CCI del reactivo para los dos grupos. En la última línea de código, debemos especificar qué reactivo queremos visualizar al definir item =. En este caso, lo que escribiremos es el número del reactivo en el orden en el que aparecen en nuestra tabla anterior. coefLord&lt;- Lord$itemParInit plotDIFirt(coefLord, item = 7) ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] Como podemos ver, en este caso, parecería que el reactivo favorece más a las mujeres en los niveles bajos de habilidad. Es decir, cuando un hombre y una mujer tienen ambos un nivel bajo de habilidad en el constructo medido, la mujer tiene una probabilidad más alta de contestar correctamente el reactivo, mientras que en los niveles altos de habilidad, la probabilidad de contestar correctamente es más similar entre ambos grupos. A esto se le llama DIF no uniforme. Una vez que hemos detectado esto, el siguiente paso, como se mencionó en el capitulo anterior, es revisar cualitativamente el reactivo con jueces expertos para poder identificar, si efectivamente hay algo en el reactivo que esté favoereciendo al grupo de las mujeres, o si, por el contrario, se trata de una diferencia específica real entre ambos grupos con respecto a lo que este reactivo evalúa. 6.8 Gráficos Esta última sección corresponde al apartado referente al informe de resultados. Como se mencionó en esa sección, al presentar resultados es de suma utilidad recurrir a recursos gráficos para resumir la información. De hecho, algunos de los gráficos y tablas que ya se generaron en este capítulo pueden ser de gran utilidad para el reporte final, especialmente aquel que incluye el reporte del desempeño de los reactivos y del examen. Pese a ello, existen otros gráficos que pueden ser de utilidad. En este ejemplo, generaremos un par de gráficas esperando que al lector le resulte de utilidad para sus propios informes, si desean conocer más sobre las herramientas gráficas en R y cómo generar otro tipo de gráficas, recomendamos referirse a los siguientes enlaces El gráfico que generaremos en este ejemplo permitirá observar cómo se distribuyeron los porcentajes de respuestas en nuestra muestra, primero en general, y después dividios por sexo. datos_graficos &lt;- reactivos_total%&gt;% mutate(Sexo = case_when(Sexo == 0 ~ &quot;Hombre&quot;, Sexo == 1 ~ &quot;Mujer&quot;))%&gt;% mutate(Porcentaje = round((Total/max(Total))*100))%&gt;% mutate(rango_porcentaje = case_when(Porcentaje%in% 1:10 ~ &quot;1-10&quot;, Porcentaje %in% 11:20 ~ &quot;11-20&quot;, Porcentaje%in% 21:30 ~ &quot;21-30&quot;, Porcentaje%in% 31:40 ~ &quot;31-40&quot;, Porcentaje%in% 41:50 ~ &quot;41-50&quot;, Porcentaje%in% 51:60 ~ &quot;51-60&quot;, Porcentaje%in% 61:70 ~ &quot;61-70&quot;, Porcentaje%in% 71:80 ~ &quot;71-80&quot;, Porcentaje%in% 81:90 ~ &quot;81-90&quot;, Porcentaje%in% 91:100 ~ &quot;91-100&quot;)) Lo primero que haremos será modificar nuestra base de datos; en las primeras líneas de código cambiaremos algunas de nuestras variables. Primero, para que al distribuir por sexos aparezcan las etiquetas de Hombres y Mujeres en lugar de unos y ceros, modificaremos dicha variable. Es importante que ubiquemos qué valor corresponde a Hombres y cuál a mujeres para con ello describirlo correctamente en la línea de código, en este caso, el 0 corresponde a hombres, mientras que el 1 corresponde a mujeres, por lo que se asignan de esa manera. En las siguientes líneas de código se modifica el puntaje total. Primero se convierte en porcentaje y después se agrupa en frecuencias, ubicando juntos a todos los que tengan puntajes del 1 al 10%, del 11 al 20, y así sucesivamente. Ya que hemos generado esta nueva base de datos dedicada a los gráficos que vamos a generar, podemos ejecutar la siguiente línea de código para obtener nuestor primer gráfico datos_graficos%&gt;% ggplot(aes(x = rango_porcentaje)) + geom_bar(fill = &quot;steelblue&quot;) + labs(title = &quot;Distribución de frecuencias de puntajes totales&quot;, x = &quot;Puntaje total&quot;, y = &quot;Frecuencia&quot;) El paquete que estamos utilizando para este ejemplo se llama ggplot2, este paquete tienen un nivel de personalización muy amplio, pero su código es poco comprensible para quien no está familiarizado. En este ejemplo intentaremos concentrarnos en los aspectos fundamentales que nos ayudarán a generar los gráficos específicos que buscamos aquí, si al lector le interesa profundizar en el tema, nuevamente referimos a los enlaces presentados arriba. Nota Si al lector le interesa profundizar en el uso de este tipo de gráficas, una buena herramienta es la presentada en este enlace. En la sintacis, lo primero que definimos es la variable que vamos a presentar. En este primer ejemplo, en lugar de presentar el puntaje total crudo, pasaremos directamente a presentarlo en términos de porcentaje agrupados en categorías de 10 en 10, variable a la cual llamamos arriba rango_porcentaje. Posteriormente, definimos qué tipo de gráfico queremos. En este caso es un gráfico de barras que se define con la línea geom_bar(). Dentro de ella, podemos definir el color del que queremos las barras. Puede ser un nombre como steelblue que se traduce como azul metálico, o puede ser el código html del color que buscamos. Podemos buscar el color específico que queremos y obtener su código html en esta página. En la siguiente gráfica vemos un ejemplo de ello. datos_graficos%&gt;% ggplot(aes(x = rango_porcentaje)) + geom_bar(fill = &quot;#54CDBA&quot;) + labs(title = &quot;Distribución de frecuencias de puntajes totales&quot;, x = &quot;Puntaje total&quot;, y = &quot;Frecuencia&quot;) Después de este comando, añadimos uno más en el que definimos los títulos de la gráfica, tanto el título principal como los de cada eje de la gráfica; lo que está dentro de las comillas puede modificarse por el titulo que deseemos. En la siguiente gráfica, añadiremosla separación por sexos. datos_graficos%&gt;% ggplot() + geom_bar(aes(x = rango_porcentaje, fill = Sexo),position=position_dodge()) + labs(title = &quot;Distribución de frecuencias de puntajes totales&quot;, x = &quot;Puntaje total&quot;, y = &quot;Frecuencia&quot;) Si observamos el código, veremos que modificamos el valor de fill por la variable Sexo, esto permitirá que observemos por separado los valores de hombres y mujeres. Además de esto, agregamos la línea position=position_dodge(), que nos permite ver lado a lado los valores de cada sexo. Ggplot2 genera en automático dos colores predeterminados, pero al igual que en el caso anterior, podemos personalizar los colores que necesitemos, solo que con una línea de código distinta. datos_graficos%&gt;% ggplot() + geom_bar(aes(x = rango_porcentaje, fill = Sexo),position=position_dodge()) + labs(title = &quot;Distribución de frecuencias de puntajes totales&quot;, x = &quot;Puntaje total&quot;, y = &quot;Frecuencia&quot;) + scale_fill_manual(values = c(&quot;#54CDBA&quot;, &quot;#BA95DF&quot;)) En este caso, añadimos el comando scale_fill_manual() y dentro de él especificamos entre comillas cada uno de los valores que queremos. REFERENCIAS "],["COMP7.html", "COMPONENTE 7 ¿CÓMO REPORTAMOS LOS RESULTADOS 7.1 Examinados 7.2 Institución 7.3 A modo de repaso", " COMPONENTE 7 ¿CÓMO REPORTAMOS LOS RESULTADOS Una vez que hemos recopilado toda la información de las distintas fases anteriores, necesitamos ser capaces de traducirla y transmitirla a las personas interesadas. Por lo general, la manera de reportar estos resultados es mediante un informe final dirigido específicamente a las personas interesadas. Dado que, dependiendo del tipo de examen que hayamos elaborado, los interesados pueden ser grupos muy variados y diversos, será necesario adaptar el tipo de información que se proporciona a cada grupo de interesados. En este manual nos concentraremos en dos poblaciones objetivo principales, los examinados y la institución para la que se realiza el examen. 7.1 Examinados Para los examinados, lo que resulta de mayor interés al reportar es únicamente el puntaje individual. Sin embargo, para poder ofrecer la información de manera más completa posible con respecto a lo que este puntaje significa, será necesario incluir dentro del reporte también qué tipo de interpretación se utilizó. Si se trató de un examen referente a normas, por ejemplo, un examen de admisión en el que solamente se contaba con una cierta cantidad de lugares, es de mayor utilidad reportar qué significa su puntaje en términos comparativos con el resto de examinados; es decir, qué posición ocupó su puntaje en relación con los demás, y si esta posición es suficiente para ser admitido o no. En cambio, si se trata de un examen referente a criterios, lo que se reporta es la relación de su puntaje con respecto a los criterios establecidos. Por ejemplo, en un examen de grado, en el que se requiere una puntuación aprobatoria, basada en algún método de punto de corte, es importante reportar el puntaje obtenido y la interpretación en términos de la clasificación que dicho puntaje implica. Si el puntaje fue mayor al punto de corte, se explicita que se obtiene una calificación de aprobado. Estándar 10.11: Cuando sea apropiado o lo exija la ley, los profesionales deben compartir los puntajes e interpretaciones de las pruebas con el examinando. Esta información se debe expresar en un lenguaje que el examinando (o, si corresponde, el representante legal del examinando) pueda comprender. (AERA, APA, and NCME 2018) 7.2 Institución En el informe específico para la institución, el foco de atención estará en dos aspectos principales, los resultados referentes al examen en sí y aquellos referidos al colectivo de los sustentantes. Es decir, presentaremos tanto las principales evidencias de validez de las interpretaciones del examen como los resultados generales de los examinados. Para hacer la presentación de estos resultados, Lane, Raymond, and Haladyna (2016) en el capítulo correspondiente reporte de resultados, presentan algunas recomendaciones que aquí desglosaremos: Hacer el reporte legible, conciso y visualmente atractivo: Un reporte de resultados puede resultar ser un documento sumamente tedioso de leer para los interesados, especialmente si se sobrecarga de texto. Lo ideal es reducir al mínimo necesario la cantidad de texto y concentrarnos en la comunicación gráfica, desde las fuentes y colores utilizados hasta las herramientas gráficas de comunicación que veremos en los siguientes puntos. Mantener la presentación clara, simple y despejada: Evitar sobrecargar de texto es importante, pero también es importante compactar la información en general y no tratar de explicar todo el procedimiento o resultados que obtuvimos, hay que buscar explicar de manera simple y concreta los principales resultados No tratar de abarcar demasiado en una sola visualizacón de los datos: Una de las formas en las que el punto anterior puede presentarse es cuando usamos gráficas o tablas en las que pretendemos resumir demasiada información. Existen informes en los que se presentan tablas con muchas variables agrupadas en una sola tabla, lo cual las vuelve difíciles de leer e interpretar. Es recomendable presentar un solo resultado por gráfico o tabla. Algunas recomendaciones se presentan en el capítulo de análisis en R. Incluir texto que soporte y mejore la interpretación de gráficos y tablas: Si bien es recomendable tratar de comprimir los resultados principales en tablas y gráficas, es importante recordar que nuestra audiencia no siempre será especialista en interpretar estos datos. En otras ocasiones, las tablas o gráficos pueden presentar la información de manera general, pero a nosotros nos interesa llevar la atención de nuestro lector a un aspecto específico de lo abarcado en las gráficas. Por ello, es importante que incluyamos una pequeña explicación de cada gráfica que facilite su lectura. Idealmente, no deberíamos de superar un párrafo por tabla o gráfico, a menos que éste presente varios aspectos relevantes para nuestro informe. Minimizar el uso de tecnicismos estadísticos: Precisamente dado que nuestro público objetivo no siempre será experto en temas estadísticos, es importante reducir las explicaciones de este tipo al mínimo posible. En el reporte de resultados de la muestra, basta con presentarlos en términos de porcentaje, sin incluir análisis de significancia ni términos complejos como tamaño del efecto, intervalos de confianza u otros por el estilo. En el aspecto del desempeño psicométrico, es importante tratar de simplificar en la medida de lo posible los resultados, tratando de explicar lo que significan los estadísticos que obtuvimos en lugar de presentarlos directamente. Por ejemplo, en lugar de decir: Se obtuvo un Alpha de Cronbach de .077 con un intervalo de confianza de .10. Podemos simplemente decir que la confiabilidad del examen fue adecuada y consistente. Y de ser necesario, presentar el valor específico entre paréntesis. Incluir un glosario de términos clave: Cuando es absolutamente necesario explicar conceptos complejos, es necesario definirlos muy bien para el lector. Por ejemplo, al explicarle el desempeño de los reactivos en términos de dificultad y discriminación, es importante definir de manera muy simplificada a qué nos referimos con esto. Una herramienta útil, para evitar que el texto del informe se haga muy largo, es incluir un glosario con estos términos, de manera que, si el lector no comprende algún tecnicismo, pueda consultarlo directamente en el glosario. Usar gráficos de barras que faciliten la comparación: Los gráficos de barras son una de las maneras más sencillas y claras de transmitir información con respecto a frecuencias, lo que a su vez nos puede ayudar a visualizar de manera simple comparativas entre distintos grupos. Por ejemplo, en una misma gráfica de barras podemos presentar lado a lado los resultados de hombres y mujeres, permitiendo visualizar de manera sencilla si hay diferencias entre uno y otro. Agrupar los datos de maneras relevantes y significativas: Este punto tiene mucha relación con el referente a no presentar demasiada información en un solo gráfico o tabla. En ocasiones, por practicidad, buscamos generar una tabla muy grande que contenga toda la información, y explicar cada punto de esta de manera ordenada. Esto puede resultar muy confuso para el lector, por lo que es recomendable recordar que un informe es una manera de narrativa y de comunicación, por lo que es importante presentar una secuencia lógica en los resultados. Por ejemplo, comenzar con el desempeño de los reactivos, lo que valida su uso e interpretación, para después presentar los resultados generales de la población, y después profundizar en las diferencias entre los grupos de interés. En relación con este último punto, en la presentación de resultados, suele ser útil presentar comparativas entre grupos, por ejemplo, los puntajes agrupados por sexo, por escuela de procedencia, por nacionalidad, etc. Pero es importante que presentemos solamente aquellos que sean de relevancia para las personas a las que va dirigida el informe, y no simplemente presentar todas las comparativas posibles. Usar gráficos y tablas para resaltar los resultados principales: Como ya se mencionó, en el uso de gráficos y tablas, es importante ser selectivo con la información que presentamos, así como no es recomendable presentar toda la información compacta en un solo gráfico o tabla, tampoco es ideal presentar muchas tablas y gráficas seguidas. Por ejemplo, si presentamos 15 gráficas de barras seguidas, el lector puede perder el interés de interpretar cada una. Por ello, es importante que los gráficos que utilicemos sean concretos y significativos. Si se desea presentar resultados más extensos, una práctica común es incluir una sección de anexos, en la que se añaden el resto de las tablas y gráficos para que el lector los pueda consultar, para con ello, incluir solamente los gráficos principales dentro del texto. Evitar el uso de decimales: En el mismo sentido de buscar simplificar la lectura de nuestro informe, es recomendable evitar el uso de decimales por varias razones. Los decimales pueden volver más confusa la interpretación de los puntajes, además de que conllevan un nivel de especificidad que puede no ser necesario para el contexto del informe. Por ejemplo, supongamos que queremos elaborar una gráfica de barras con las frecuencias de cada puntaje posible. Si incluimos decimales, esta gráfica se podría volver muy larga e imposible de interpretar. Si aparece una barra con dos personas que obtuvieron 7.5, tres que obtuvieron 7.6, cuatro con 7.9, etc., sería mucho más sencillo que aparezcan agrupados en puntajes de 7 solamente. Una estrategia muy simple es presentar los resultados en términos de porcentaje de 0 a 100 y agrupar los resultados por decenas; es decir, presentar una barra para aquellos que obtuvieron de 0 a 10, de 11 a 20, y así sucesivamente. Este gráfico es mucho más interpretable que uno que presente la frecuencia de cada puntaje posible. Si se usa color hacerlo de manera orientada No siempre será conveniente el uso del color; por ejemplo, si el informe será presentado de manera impresa con múltiples copias, puede resultar más económico el uso de informes en blanco y negro. Sin embargo, si se utiliza color, es importante hacerlo de manera estratégica, y no simplemente como una herramienta para llamar la atención. Por ejemplo, en un gráfico de barras por sexo, es útil rellenar de un color las barras correspondientes a los hombres y otro distinto las de las mujeres. Con esta herramienta de color, se comunica en un gráfico compacto y simple de interpretar información que de otra forma habría sido más compleja y tediosa de presentar, así como difícil de interpretar. Cuando se usen gráficos así, es importante siempre incluir una leyenda que especifique qué significa cada color. Pilotear el reporte con miembros de la audiencia objetivo: Antes de presentar el reporte final, puede ser de utilidad presentar nuestro borrador a alguno de los miembros de la audiencia objetivo y consultar con ellos si el reporte es entendible, sencillo y si cumple con los aspectos esperados del mismo. Cuando este paso sea posible, nos ayudará a garantizar que el reporte verdaderamente responda a las expectativas de la audiencia objetivo. Crear reportes especialmente diseñados para las diferentes audiencias: Como ya se mencionó más arriba, cada grupo de interesados en el examen presentará un interés diferente en este. Es importante que nuestros reportes e informes estén orientados a responder a dichos intereses, y es por ello que se recomienda generar un reporte personalizado para cada grupo de interesados. Evidencia documental: Durante esta fase, la principal evidencia documental que generaremos serán los reportes mismos. Es importante que sigamos en la medida de lo posible todas las recomendaciones de este capítulo, dado que la evidencia que se generará aquí será aquella transmitida directamente a los interesados, a diferencia de la generada en otros componentes que es más de carácter técnico. Seguridad: Parte de la información obtenida en este componente se hará pública dentro del informe final, sin embargo, es importante mantener y resguardar las bases de datos utilizadas para obtener los resultados, así como toda la información de identificación de los estudiantes. En el informe de resultados a los examinados, es importante destacar que, para fines de seguridad, no se recomienda generar una devolución detallada de los aciertos y errores que tuvo cada examinado en cada reactivo, especialmente si estos mismos reactivos se van a utilizar en futuras aplicaciones, dado que esto podría atentar contra la validez del examen. La recomendación es únicamente presentar el puntaje total con su debida interpretación. De igual forma, en términos de confidencialidad, es importante diseñar un sistema que permita entregar los resultados de manera individualizada, y no publicar los resultados de cada individuo de manera que puedan ser identificables. Se busca que cada examinado pueda acceder o interpretar únicamente los resultados correspondientes a sí mismo. Estándar 5.1: Se deben proporcionar a los usuarios de la prueba explicaciones claras de las características, el significado y la interpretación prevista de los puntajes de escala, así como de sus limitaciones. (AERA, APA, and NCME 2018) 7.3 A modo de repaso ¿Por qué es recomendable utilizar reportes personalizados de acuerdo con los distintos interesados en los resultados? REFERENCIAS "],["COMP8.html", "COMPONENTE 8 ¿Y QUÉ HACEMOS DESPUÉS CON EL EXAMEN? 8.1 Contenido de los reactivos: 8.2 Seguridad del instrumento: 8.3 Desempeño psicométrico: 8.4 A modo de repaso", " COMPONENTE 8 ¿Y QUÉ HACEMOS DESPUÉS CON EL EXAMEN? Una vez que hemos terminado el proceso de elaboración de nuestro instrumento, lo aplicamos, verificamos sus propiedades métricas y validamos sus usos e interpretaciones, pareciera que el proceso está terminado. Sin embargo, hay una etapa más que suele dejarse de lado, y que, dependiendo del tipo de instrumento que hayamos construido es de suma importancia. Si nos regresamos a las primeras etapas y pensamos en los usos e interpretaciones que definimos para nuestro instrumento, hay que preguntarnos ¿el instrumento fue diseñado para una única aplicación aislada?, ¿o se pretende utilizar en múltiples aplicaciones a lo largo del tiempo? Si se trata más bien de la segunda opción, será importante que demos mantenimiento a nuestro instrumento. Cuando hablamos de mantenimiento nos referimos a vigilar que los usos e interpretaciones del instrumento sigan siendo válidos para futuras aplicaciones. Existen algunas áreas específicas que corren riesgo de atentar contra esta validez a lo largo del tiempo, y, que por lo tanto debemos cuidar: 8.1 Contenido de los reactivos: Existen áreas específicas que se actualizan constantemente, por ejemplo, un examen de física o ciencias naturales podrá contener conceptos que dentro de un par de años se vuelvan obsoletos, o que sean remplazados por nuevos conceptos o teorías. En casos como estos, es importante que vigilemos que nuestros reactivos contengan información actualizada. 8.2 Seguridad del instrumento: En exámenes de alto impacto, como un examen de ingreso a la universidad, es muy importante seguir generando reactivos constantemente. Supongamos que una persona hace el examen y no logra ingresar a la universidad, pero logra recordar o capturar reactivos específicos del examen; si no generamos nuevos reactivos para la siguiente edición, esa persona tendrá mayor probabilidad de ingresar, pero no porque tenga los conocimientos y habilidades, sino porque recuerda los reactivos y tuvo el tiempo de indagar las respuestas correctas. 8.3 Desempeño psicométrico: Es importante que tengamos en cuenta que la confiabilidad y las evidencias de validez que obtenemos, no son de la prueba en sí, sino de la muestra a la que se le aplicó en específico; por ello, es importante que en cada nueva aplicación vigilemos las propiedades de nuestros reactivos. Supongamos que, de acuerdo a nuestra estructura, queremos tener un cierto porcentaje de reactivos con un índice de dificultad de entre 0.6 y 0.8. Logramos esa proporción durante la primera aplicación, pero a la siguiente, nos damos cuenta de que algunos reactivos ya bajaron su índice de dificultad, o incluso ya no funcionan tan bien, sería prudente considerar revisar esos reactivos o generar nuevos para mantener la cantidad adecuada de acuerdo a lo que planeamos. Analizar estos reactivos nos permitirá encontrar qué es lo que está pasando; quizá algún reactivo se desempeñó distinto porque la información de ese reactivo ya está obsoleta, o quizá hubo un cambio en el plan de estudios del nivel de educación anterior, lo que hizo que ese punto ya no sea estudiado por los alumnos. Además de estos ejemplos, existen muchas otras posibles razones para que el desempeño de algunos reactivos pueda cambiar, por eso es importante vigilar constantemente que nuestro examen y reactivos se sigan desempeñando adecuadamente para que podamos argumentar que los usos e interpretaciones del examen siguen siendo válidos. Evidencia documental: Si nuestro examen se aplicará de manera cíclica, cada nueva aplicación requerirá de su propio informe de evaluación, en el cual se incluyan los análisis psicométricos de esa aplicación específica, así como los cambios referentes a la versión anterior. Es importante llevar un registro de los reactivos que fueron agregados y probados en cada versión, así como sus desempeños psicométricos correspondientes. Seguridad: Todo el proceso de mantenimiento debe llevarse de manera sistemática, diseñando un protocolo que permita tener orden de los nuevos reactivos y del desempeño de cada uno a lo largo del tiempo, y a la vez, que permita resguardar esta información y delimitar quienes tienen acceso a ella. Estándar 4.24: Las especificaciones de la prueba deben modificarse o revisarse cuando nuevos datos de investigación, cambios significativos en el dominio representado o condiciones recientemente recomendadas del uso de la prueba pueden reducir la validez de las interpretaciones de los puntajes de la prueba. Si bien no es necesario que una prueba que mantiene su utilidad sea retirada o revisada simplemente debido al paso del tiempo, los desarrolladores de la prueba y los editores de la prueba son responsables de supervisar condiciones cambiantes y de modificar, revisar o retirar la prueba según lo indicado. (AERA, APA, and NCME 2018) 8.4 A modo de repaso ¿Cuál es la razón principal por la que es importante darle mantenimiento a nuestro examen? REFERENCIAS "],["GLOSARIO.html", "GLOSARIO Alpha de Cronbach: Análisis Factorial Exploratorio: Análisis Factorial Confirmatorio: Aprendizaje esperado: Base de datos: Base de datos limpia: Base de datos ordenada: Calibración: Codificación: Confiabilidad: Consistencia interna: Constructo: Correlación: Correlación reactivo-total: Criterios de calidad de los reactivos: Cuestionario: Desempeño de reactivos: Dificultad: Dimensión: Dimensionalidad: Distractores: Entrevista cognitiva: Equiparación: Error de medición: Escenario o estímulo: Estandarización: Evaluación criterial: Evaluación normativa: Evidencias de validez: Examen (prueba objetiva): Funcionamiento Diferencial del Reactivo (DIF): Mantenimiento de la prueba: Marco conceptual: Modelo de 1 parámetro: Modelo de 2 parámetros: Niveles de aprendizaje: Omega de McDonald: Operacionalización: Paneles de expertos: Parámetros psicométricos: Propiedades estadísticas: Pruebas paralelas: Punto de corte: Reactivos ancla: Regresión Lineal: 5 Seguridad del instrumento: {-seguridadG} Sesgo: Subrepresentación del constructo: Tabla de especificaciones: Teoría Clásica de los Tests: Teoría de Respuesta al Ítem: Test-Retest: Validez: Varianza irrelevante al constructo:", " GLOSARIO Alpha de Cronbach: Método estadístico que permite evaluar la consistencia interna de un instrumento, es decir, qué tanto los reactivos del instrumento se relacionan entre sí, y que tan homogéneos son. Su coeficiente se ubica en un rango de 0 a 1, siendo los valores más cercanos a 1 indicadores de una consistencia interna alta. Análisis Factorial Exploratorio: Técnica estadística que permite determinar la estructura interna de un instrumento mediante una serie de pasos que permiten identificar la cantidad de factores necesarios (dimensiones del examen) y los reactivos que se agrupan en cada uno de estos factores, es decir, es el software el que determina la estructura que mejor se ajusta a los datos. Análisis Factorial Confirmatorio: Técnica estadística que permite determinar la estructura interna de un instrumento. A diferencia del análisis factorial exploratorio, desde el principio se debe especificar que reactivos pertenecen a qué dimensión o factor y es el software estadístico el que se encarga de identificar qué tan bien se ajusta ese modelo a los datos que tenemos. Aprendizaje esperado: Componente que determina lo que se espera que logren los sustentantes en un área y subárea de conocimiento específica; debe ser definido de forma que especifique una acción concreta, por lo que es necesario que se estructuren por medio de los niveles de aprendizaje, por ejemplo, la taxonomía de Bloom. Base de datos: Herramienta que permite la recopilación de una gran cantidad de información que será utilizada posteriormente. Base de datos limpia: Es aquélla en la que cada celda contiene información útil, correcta e interpretable. Base de datos ordenada: Es aquella en la que cada fila representa a un solo sustentante y cada columna representa un atributo medido de ese sustentante. La primera fila ha de contener el nombre de dichos atributos, y las filas subsecuentes contendrán el valor de ese atributo para cada sustentante sin que existan duplicaciones de información. Calibración: Proceso de estimación de los parámetros de los reactivos en términos de su dificultad, discriminación y su ajuste al modelo. Codificación: Proceso mediante el cual se capturan o convierten los atributos de los sustentantes a números, es decir, se le asignan símbolos a la información obtenida con el propósito de facilitar el análisis estadístico. Confiabilidad: Propiedad psicométrica del proceso de medición que permite determinar su consistencia, es decir, si la medición conduce a resultados similares no importando las fluctuaciones que puedan afectar la prueba. Consistencia interna: Medida estadística que señala qué tanto los reactivos del instrumento se relacionan entre sí, y que tan homogéneos son. Constructo: Construcción teórica del atributo a evaluar que explica la relación entre las variables latentes y las variables observadas. No adopta valores por lo tanto no es observable así que su medición se realiza mediante indicadores o manifestaciones externas de las variables. Correlación: Medida estadística estandarizada que indica qué tanto dos valores están relacionados y de qué manera, es decir, su fuerza y dirección. Correlación reactivo-total: Estimada mediante una correlación biserial puntual, es una medida estadística estandarizada que permite estimar si el contestar de manera correcta un reactivo está relacionado con obtener puntajes altos y si contestarlo de manera incorrecta se relaciona con la obtención de puntajes bajos. En otras palabras, nos dirá si contestar de manera correcta o incorrecta un reactivo determinado nos ayuda a determinar el nivel de habilidad de la persona (entendido como el puntaje total de la prueba). Criterios de calidad de los reactivos: Lineamientos institucionales, establecidos por expertos en evaluación, que brindan pautas con respecto a la construcción de los reactivos con el fin de asegurar que los reactivos midan lo que realmente deben medir, de manera válida y justa con los sustentantes. Cuestionario: Instrumento de medición que permite indagar variables personales, académicas, emocionales y sociales. está constituido por un conjunto de preguntas aisladas o por escalas, las cuales no tienen respuestas correctas. Desempeño de reactivos: Área de análisis que permite determinar el comportamiento de los reactivos en cuanto a dificultad y discriminación, y si los distractores funcionaron como se esperaba que lo hicieran. Dificultad: Hace referencia a la proporción de alumnos que contestó correctamente el reactivo con respecto al total de examinados. Dimensión: Componente que es parte de una variable compleja, el cual se origina de su análisis o desagregación. Dimensionalidad: Área de análisis que permite determinar si todos los reactivos de un dominio o subdominio específico realmente están midiendo lo mismo, o si, por el contrario, algún reactivo que se esperaba midiera un dominio, en realidad se agrupa mejor con reactivos que miden otra cosa. ## Discriminación: {-#discriminacion} Hace referencia a la capacidad que tiene el reactivo para distinguir correctamente el grupo de sustentantes con desempeño alto con respecto a los sustentantes de bajo desempeño. Distractores: Opciones incorrectas de un reactivo, los cuales buscan representar los principales errores que cometen los sustentantes en el tema que aborda el reactivo, con el fin de identificar de manera precisa el nivel de habilidad de los sustentantes. Entrevista cognitiva: Técnica cualitativa que permite retroalimentar el funcionamiento de los reactivos mediante la realización de cuestionamientos a los sustentantes con el fin de identificar de qué forma responden al reactivo y porqué. Equiparación: Proceso estadístico utilizado para adecuar las puntuaciones de dos o más versiones de un instrumento, con la finalidad de que los parámetros que sean estimados de los reactivos se manifiesten en la misma escala de medida y que los puntajes de un sustentante que respondió una versión del instrumento sean comparables con las que contestó otro sustentante en una versión distinta. Error de medición: Se refiere a la diferencia entre el valor verdadero de los puntajes de los sustentantes y el valor que se observa en los puntajes de la prueba, la cual es causada por diferentes fuentes de error que afectan la consistencia del proceso de medición. Escenario o estímulo: Texto o conjunto de textos que presentan situaciones específicas en las que los alumnos se deben basar para dar respuesta a uno o varios reactivos. Estandarización: Proceso mediante el cual se busca la implementación de procesos de calidad basados en normas de actuación comunes. Evaluación criterial: Comparan el desempeño del sustentante en la prueba con algún criterio u objetivo preestablecido, por lo que los puntajes resultantes son interpretados como el grado de precisión con el que el sustentante ha dominado un contenido específico. Evaluación normativa: Comparan el desempeño del sustentante en la prueba con otro grupo de estudiantes que han tomado la misma evaluación, por lo que las puntuaciones individuales de un sustentante son dependientes del desempeño de los otros sustentantes. Evidencias de validez: Grado en que el argumento, tanto teórico como práctico, establecido en la construcción del examen, soporta las interpretaciones y usos previstos de los puntajes del examen. Examen (prueba objetiva): Instrumento de medición en múltiples formatos que busca determinar el nivel de desempeño logrado de los sustentantes, es decir, evalúa aprendizajes logrados. Funcionamiento Diferencial del Reactivo (DIF): Un reactivo presenta funcionamiento diferencial si dos sustentantes tienen el mismo nivel de habilidad, pero uno de ellos por pertenecer a un grupo distinto tiene mayor probabilidad de contestar de manera correcta dicho reactivo. El DIF es una medida del sesgo de los reactivos y nos ayuda a presentar evidencias de imparcialidad en nuestras pruebas. Mantenimiento de la prueba: Cuando hablamos de mantenimiento nos referimos a vigilar que los usos e interpretaciones del instrumento sigan siendo válidos para futuras aplicaciones. Marco conceptual: Base teórica sobre la que se construye la definición del constructo, la operacionalización de este y la construcción de la tabla de especificaciones. Se refiere a la teoría o evidencia bibliográfica sobre la que se sustenta nuestro examen. Modelo de 1 parámetro: Modelo explicativo que parte de la Teoría de Respuesta al Ítem, y que utiliza únicamente el nivel de habilidad y el índice de dificultad del reactivo como criterios para calcular la probabilidad de que un participante responda correctamente. Modelo de 2 parámetros: Modelo explicativo que parte de la Teoría de Respuesta al Ítem, y que utiliza tanto el nivel de habilidad y el índice de dificultad como el nivel de discriminación del reactivo como criterios para calcular la probabilidad de que un participante responda correctamente. Niveles de aprendizaje: Se refieren a la complejidad del aprendizaje logrado, y existen distintas taxonomías que dan cuenta de ellos y los clasifican. Una de las más populares es la taxonomía de Bloom, que utiliza una serie de verbos para clasificar los distintos niveles de aprendizaje. En este contexto, estos niveles son utilizados para definir la complejidad de la tarea cognitiva que se le solicitará a los sustentantes en cada uno de los reactivos. Omega de McDonald: Es un estadístico que permite medir la confiabilidad de consistencia interna. A diferencia del Alfa de Cronbach, el coeficiente Omega permite tomar en consideración la dimensionalidad factorial del examen para obtener los coeficientes de confiabilidad por cada uno de los factores que lo conforman. Suele ser una alternativa más estable y menos sesgada para obtener la confiabilidad de nuestro examen. Operacionalización: Proceso por el que se define un concepto en términos medibles, observables y cuantificables. En otras palabras, se trata de descomponer el constructo en aquellas dimensiones o aspectos que nos puedan dar cuenta de este. Paneles de expertos: Grupo de especialistas independientes al proceso de diseño del examen, quienes cuentan con experiencia y dominio de por lo menos uno de los rubros referentes al examen, ya sea el contenido temático, el diseño de reactivos o los análisis psicométricos. Suele recurrirse a estos paneles como un apoyo para brindar evidencia de validez. Parámetros psicométricos: Son una serie de indicadores cuantitativos que dan cuenta de las propiedades del instrumento en términos estadísticos. Entre los más utilizados se encuentran la confiabilidad, la dimensionalidad y las propiedades de los reactivos. Propiedades estadísticas: Cualidades numéricas del examen en general y de cada uno de los reactivos que dan cuenta de su calidad basados en estándares y criterios previamente sustentados por la teoría psicométrica. Pruebas paralelas: Se puede afirmar que contamos con pruebas paralelas solamente cuando tenemos dos versiones de una prueba que tienen distintos reactivos, pero que, en principio, se mantienen las mismas proporciones en ambas versiones, tanto a nivel de los parámetros de dificultad y discriminación de los reactivos, como de la cantidad de reactivos referentes a cada aprendizaje esperado o indicador. Punto de corte: Puntaje específico a partir del cual se puede dividir a los sustentantes en dos categorías distintas, ya sea aprobado-no aprobado, -insuficiente-suficiente u alguna otra categoría acorde a los usos previstos de la prueba. Este punto se obtiene mediante procesos metodológicos rigurosos como el método de Angoff o el de Bookmark. Reactivos ancla: Se refiere a aquellos reactivos utilizados en pruebas paralelas que se conservan iguales en ambas versiones del examen, tanto a nivel de contenido como de posición en el examen, permitiendo con ello equipararlos al momento de obtener la calificación final de los alumnos. Regresión Lineal: Nos permite predecir, a partir de algún criterio como el puntaje de nuestro examen, el resultado en otra variable con varias opciones de resultado posibles; por ejemplo, predecir las calificaciones de un alumno a partir de su puntaje de ingreso, o predecir su puntaje en otro examen. La precisión con que logramos predecir la segunda variable a partir de la primera nos indicará no solo la relación entre ambas variables, sino que da cuenta de una posible relación de causalidad. 5 Seguridad del instrumento: {-seguridadG} Se refiere a todos los esfuerzos y protocolos utilizados para garantizar que toda información referente al instrumento que pudiera afectar su funcionamiento, confiabilidad y validez, sea resguardada y protegida, para con ello proteger también la validez de los usos e interpretaciones del examen. Sesgo: Cuando existe un sesgo dentro de nuestro examen, no podemos afirmar que los usos e interpretaciones de este sean válidos para todas las poblaciones, ya que alguna de ellas se está viendo beneficiada por alguna característica del examen o del proceso de construcción, aplicación e interpretación del mismo. Evitar la presencia de estos sesgos debe ser un objetivo fundamental en todo el proceso de desarrollo del examen para garantizar con ello la imparcialidad. Subrepresentación del constructo: Este fenómeno puede ocurrir cuando la cantidad de reactivos que tenemos en nuestro examen no da cuenta de forma representativa del objeto que pretendemos medir, dejando aspectos fundamentales del mismo sin ser evaluadas. Esto puede ocurrir porque desde un principio nuestra definición no fue lo suficientemente extensa, porque no generamos los suficientes reactivos, o porque tras el análisis psicométrico de estos, eliminamos reactivos que evalúan aspectos muy importantes sin tener ningún otro que diera cuenta de ese aspecto. Tabla de especificaciones: Herramienta que nos permite generar un registro de la estructura de nuestro examen, desde la definición del constructo que queremos medir, pasando por sus dimensiones, subdimensiones y cantidad de reactivos necesarios para evaluar cada uno, así como los niveles de aprendizaje buscados. Sirve como una guía fundamental para el proceso de redacción de los reactivos. Teoría Clásica de los Tests: Teoría de medición en psicología y educación que se basa en el principio de que el puntaje verdadero de una persona es la suma de su puntaje en un examen y del error de medición, por lo que se centra mucho en medir y tratar de controlar estos niveles de error, para con ello generar puntuaciones más cercanas al que sería el puntaje verdadero de la persona en el constructo medido. Teoría de Respuesta al Ítem: Teoría de medición en psicología y educación que pretende determinar el nivel de habilidad de cada persona mediante una serie de ecuaciones probabilísticas, a partir de las cuales, se puede determinar la probabilidad de que una persona conteste correctamente un reactivo basado en su nivel de habilidad y en parámetros como el nivel de dificultad y de discriminación de los reactivos. Pese a que ofrece valores más robustos y menos sesgados que la TCT, para utilizar esta metodología se requieren de operaciones y análisis más complejos, y de muestras más grandes, por lo que no siempre resulta idónea. Test-Retest: Método para estimar la confiabilidad de una prueba que se basa en aplicar la misma serie de reactivos al mismo grupo de personas en dos momentos distintos, para después establecer qué tan similares son los puntajes de cada persona en ambas aplicaciones; entre mayor sea esta similitud, mayor será la evidencia de confiabilidad. Validez: En el contexto del diseño de exámenes e instrumentos, la validez no es referente al examen mismo, sino a sus usos e interpretaciones. Lo que buscamos es generar evidencia de que es válido interpretar que el examen da cuenta del constructo que afirmamos que mide, o presentar evidencia de que es válido utilizar el examen como una herramienta para determinar si una persona puede graduarse o no. Varianza irrelevante al constructo: Se refiere a aquellas fuentes de variación en los puntajes de un examen que se deben a factores ajenos a lo que pretendemos medir. Idealmente, esperaríamos que las diferencias entre los puntajes de las personas se debieran solamente a diferencias en su nivel de conocimientos o habilidad, pero en ocasiones, algunas de estas diferencias se pueden deber a factores externos, como el aplicador, el contexto de aplicación u otros. Muchos de los procesos abordados en este manual buscan reducir esta varianza en la medida de lo posible. "],["respuestas-a-las-preguntas.html", "RESPUESTAS A LAS PREGUNTAS ¿Por qué es necesario establecer los usos inválidos de la prueba? ¿Qué función tiene el propósito de la evaluación? ¿Para qué necesitamos un marco conceptual del constructo a evaluar? ¿Cuál es la utilidad que tiene generar una tabla de especificaciones? ¿Cuál es la importancia de utilizar diferentes grupos de expertos en la revisión de los reactivos? ¿Por qué es ideal realizar un piloteo de los reactivos? ¿Por qué es necesario establecer condiciones de estandarización durante la aplicación de la prueba? ¿Para qué nos sirven los análisis de las propiedades de los reactivos como el análisis de dificultad, discriminación o de los distractores? ¿Si tenemos un reactivo que presenta un índice de dificultad de 0.20 y que presenta un DIF moderado, debemos eliminarlo? Justifica tu respuesta ¿Para qué necesitamos hacer los análisis DIF? ¿Por qué es recomendable utilizar reportes personalizados de acuerdo con los distintos interesados en los resultados? ¿Cuál es la razón principal por la que es importante darle mantenimiento a nuestro examen?", " RESPUESTAS A LAS PREGUNTAS ¿Por qué es necesario establecer los usos inválidos de la prueba? Para evitar que nuestro instrumento sea usado de forma indebida, y, por ende, haga inválidas las interpretaciones de la prueba. ¿Qué función tiene el propósito de la evaluación? Será nuestra guía en la toma de decisiones durante todo el proceso de diseño, desarrollo, implementación e interpretación de la prueba. ¿Para qué necesitamos un marco conceptual del constructo a evaluar? Para usarlo como una base teórica sobre la cual construir nuestro examen, generando evidencias de validez de la interpretación del examen. ¿Cuál es la utilidad que tiene generar una tabla de especificaciones? La tabla de especificaciones cumple dos propósitos principales. Nos sirve como una guía clara y concisa para el proceso de diseño de reactivos, de manera que expertos en el tema ajenos al proceso previo de diseño del examen, puedan comenzar a generar reactivos con solo guiarse con la tabla de especificaciones. Por otro lado, sirve como un registro del proceso de operacionalización, lo que nos da evidencia de la validez de las interpretaciones del examen. ¿Cuál es la importancia de utilizar diferentes grupos de expertos en la revisión de los reactivos? Hay muchos aspectos que se deben cuidar para que no intervengan y afecten el constructo que queremos medir, de otra manera las interpretaciones que hagamos de los resultados y usos de la prueba no serán válidos y podrían traer consecuencias negativas para los sustentantes, por lo que los reactivo deben pasar por una revisión profunda antes de ser aplicados. ¿Por qué es ideal realizar un piloteo de los reactivos? Porque nos permite determinar los errores que podemos cometer en la administración del examen y obtener los parámetros psicométricos de los reactivos para determinar cuáles son adecuados para medir el constructo de interés y así poder ensamblar el examen o las diferentes versiones de este. ¿Por qué es necesario establecer condiciones de estandarización durante la aplicación de la prueba? Para brindar a los sustentantes igualdad de condiciones y no permitir el paso a otras variables que puedan afectar el desempeño de los sustentantes. ¿Para qué nos sirven los análisis de las propiedades de los reactivos como el análisis de dificultad, discriminación o de los distractores? Nos indican si los reactivos se están comportando de la manera en que esperábamos que lo hicieran, generando con esto evidencias de validez de que los reactivos pueden ser utilizados para la obtención del puntaje final que a su vez puede ser interpretado como un indicador del nivel de conocimientos del sustentante. ¿Si tenemos un reactivo que presenta un índice de dificultad de 0.20 y que presenta un DIF moderado, debemos eliminarlo? Justifica tu respuesta No, siempre es importante tomar en cuenta todos los indicadores referentes al reactivo para decidir eliminarlo o no. Es importante tomar en cuenta también la discriminación, correlación con el total, confiabilidad y otros indicadores. De igual forma, la presencia de DIF no necesariamente significa algo negativo en el reactivo, es importante hacer una revisión cualitativa. ¿Para qué necesitamos hacer los análisis DIF? Para detectar qué reactivos presentan diferencias en las probabilidades de respuesta correcta en dos subgrupos, y con ello revisar dichos reactivos con la finalidad de ubicar si presentan sesgos en su redacción, estructura o contenidos, y con base en ello decidir si eliminarlos o modificarlos, garantizando así la imparcialidad de nuestro examen. ¿Por qué es recomendable utilizar reportes personalizados de acuerdo con los distintos interesados en los resultados? Personalizar los reportes permite entregar y comunicar de manera efectiva los resultados de mayor interés para cada uno de los interesados. De igual forma, permite tener un mayor control de quiénes tienen acceso a qué información, reportando a cada interesado únicamente la información necesaria según sea el caso. ¿Cuál es la razón principal por la que es importante darle mantenimiento a nuestro examen? Asegurar que los usos e interpretaciones de los puntajes sigan siendo válidos en futuras aplicaciones. "],["referencias.html", "REFERENCIAS", " REFERENCIAS "]]
